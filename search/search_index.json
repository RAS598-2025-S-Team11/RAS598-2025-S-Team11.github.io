{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Project Intelligent TurtleBot4","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#team-information","title":"Team Information","text":"<ul> <li>Project Name: Intelligent TurtleBot: Deep Learning-Based Object Detection and Voice-Guided Navigation</li> <li>Team Number: 11</li> <li>Team Members: Anushka Gangadhar Satav, Adithya Konda, Sameerjeet Singh Chhabra</li> <li>Semester: Spring 2025</li> <li>University: Arizona State University</li> <li>Class: RAS 598 Experimentation and Deployment of Robots</li> <li>Professor: Dr. Dan Aukes</li> <li>Email: anushka.satav@asu.edu, akonda5@asu.edu, schhab18@asu.edu</li> <li>Project Description: Our project, Intelligent Voice-Controlled Mobile Manipulator, transforms a TurtleBot4 and MyCobot robotic arm into an intelligent, multi-modal robot. Using ROS 2, the system integrates live camera feeds, YOLOv8-based object detection, LiDAR-based obstacle mapping, and real-time IMU tracking. Voice commands guide the robot\u2019s navigation and future manipulation tasks, creating a powerful platform for autonomous interaction. The mobile base handles navigation and perception, while the soon-to-be-mounted MyCobot arm will enable object manipulation, making it a fully integrated intelligent robotic system.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-plan","title":"Project Plan","text":"<p>Research Question:  How can a mobile robot effectively combine vision, speech, and autonomous navigation to create a responsive and interactive system in real-world environments?</p> <p>Concept: </p> <p>This project explores how TurtleBot4 can intelligently interact with its surroundings through vision-based object detection and voice-guided navigation. Using TurtleBot 4 with Create 3 and Raspberry Pi, our aim is to integrate and demonstrate the following capabilities:</p> <ul> <li>Real-time object detection using YOLOv8, recognizing and categorizing environmental objects with live camera input.</li> <li>Voice command interaction, allowing users to issue spoken instructions that influence robot behavior through a natural interface.</li> <li>Autonomous navigation with obstacle avoidance using the ROS2 navigation stack, enabling safe and efficient mobility.</li> <li>Decision-making based on perception, where voice commands and visual detections combine to guide task execution.</li> <li>Fallback mechanisms to ensure robustness and handle unexpected failures in hardware or software modules.</li> </ul> <p>This updated scope aligns directly with our project deliverables \u2014 voice-guided, vision-aware autonomous robot control \u2014 and leaves room for seamless extension into manipulation and complex task planning.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-workflow","title":"Project Workflow","text":"<p>The system operates by capturing data from multiple sensors, interpreting user commands, and performing autonomous navigation and feedback. The diagram below outlines the overall data and control flow of the TurtleBot4 system.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#init-theme-default-themevariables-fontsize-14px-primarycolor-ffffff-edgelabelbackground-ffffff-graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-flow-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-styling-groups-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-assign-node-classes-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui","title":"<pre><code>%%{ init: {\n  \"theme\": \"default\",\n  \"themeVariables\": {\n    \"fontSize\": \"14px\",\n    \"primaryColor\": \"#ffffff\",\n    \"edgeLabelBackground\": \"#ffffff\"\n  }\n}}%%\ngraph TD\n  A[\"Start:&lt;br/&gt;TurtleBot4 Powered On\"]\n  B[\"Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic\"]\n  C[\"Sensor Data&lt;br/&gt;Preprocessing\"]\n  D1[\"YOLOv8&lt;br/&gt;Object Detection\"]\n  D2[\"Voice Command&lt;br/&gt;Recognition\"]\n  E[\"Detected Object Info\"]\n  F[\"Intent or Goal Command\"]\n  G[\"Decision-Making Node\"]\n  H[\"ROS2 Navigation Stack\"]\n  I[\"Movement Commands&lt;br/&gt;via /cmd_vel\"]\n  J[\"GUI Update:&lt;br/&gt;Object and Nav Info\"]\n  K[\"Actuator Response:&lt;br/&gt;TurtleBot Moves\"]\n  L[\"User Feedback:&lt;br/&gt;GUI Visualization\"]\n  M[\"End\"]\n\n  %% Flow\n  A --&gt; B\n  B --&gt; C\n  C --&gt; D1\n  C --&gt; D2\n  D1 --&gt; E\n  D2 --&gt; F\n  E --&gt; G\n  F --&gt; G\n  G --&gt; H\n  H --&gt; I\n  I --&gt; K\n  K --&gt; M\n  G --&gt; J\n  J --&gt; L\n  L --&gt; M\n\n  %% Styling groups\n  classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000\n  classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000\n  classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000\n  classDef decision fill:#fce4ec,stroke:#ec407a,color:#000\n  classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000\n  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000\n\n  %% Assign node classes\n  class A,M startend\n  class B sensing\n  class C,D1,D2,E,F processing\n  class G decision\n  class H,I,K navctrl\n  class J,L gui\n</code></pre>","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-discussions","title":"Project Discussions","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#1-sensor-fusion-and-autonomous-decision-making","title":"1. Sensor Fusion and Autonomous Decision-Making","text":"<p>The TurtleBot4 system leverages multiple data sources \u2014 including vision, audio, and LiDAR \u2014 to perform real-time perception and intelligent control.  These data streams are handled through modular ROS2 nodes that contribute to both high-level autonomy and low-level motion control.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#2-sensor-inputs","title":"2. Sensor Inputs","text":"<ul> <li>Oak-D Camera provides real-time RGB images for visual perception.</li> <li>Microphone captures user voice commands as audio input.</li> <li>LiDAR and IR sensors are used for obstacle detection and collision avoidance.</li> </ul> <p>These sensors operate concurrently and feed their outputs into dedicated ROS2 nodes.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#3-perception-and-processing-nodes","title":"3. Perception and Processing Nodes","text":"<ul> <li><code>yolov8_processor</code>: Subscribes to image streams (<code>/rpi_13/oakd/preview/image_raw</code>) and outputs detected object data to <code>/yolov8_detections</code>.</li> <li><code>voice_input_node</code>: Processes raw audio and converts it into structured voice data (<code>/voice_input</code>).</li> <li><code>voice_command_parser</code>: Transforms audio into semantic commands (e.g., \"move forward\") and publishes them to <code>/voice_cmd</code>.</li> </ul> <p>Voice Control : - Whisper.cpp is a C/C++ implementation of OpenAI's Whisper, an automatic speech recognition (ASR) model, designed for efficient and accessible speech-to-text functionality.  - It's a lightweight alternative to the original Whisper model, making it suitable for resource-constrained environments and applications where speed and simplicity are prioritized. </p> <p>Object Detection : - YOLOv8 is a real-time object detection and segmentation model that uses a deep neural network to process images and identify objects within them. - It's known for its speed and accuracy, making it suitable for a wide range of applications, including surveillance, autonomous vehicles, and more. - YOLOv8's anchor-free architecture simplifies training and enhances the model's performance across various datasets. </p> <p>This layered perception setup allows the robot to understand both its physical environment and human intent simultaneously.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#31-high-level-autonomy","title":"3.1 High-Level Autonomy","text":"<p>The core decision-making logic resides in the <code>decision_maker_node</code>, which:</p> <ul> <li>Consumes outputs from both <code>/yolov8_detections</code> and <code>/voice_cmd</code></li> <li>Prioritizes user commands or detected objects to determine the appropriate action</li> <li>Publishes resulting commands to <code>/action_cmd</code></li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#32-low-level-motion-control","title":"3.2 Low-Level Motion Control","text":"<p>Low-level execution is handled by the following:</p> <ul> <li><code>collision_avoidance_node</code>: Merges <code>/action_cmd</code> with real-time sensor data (e.g., LiDAR) to determine safe trajectories</li> <li><code>navigation_controller</code>: Translates the motion plan into velocity commands published to <code>/cmd_vel</code></li> </ul> <p>These nodes form a closed-loop controller that ensures both goal-directed behavior and safety.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#4-ros2-and-gui","title":"4. ROS2 and GUI","text":"<p>The graphical user interface (GUI) acts as a crucial feedback layer between perception, autonomy, and user interaction. It helps visualize how the robot interprets its environment, processes inputs, and executes decisions.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#41-gui-integration-overview","title":"4.1 GUI Integration Overview","text":"<p>The GUI provides live visualization of:</p> <ul> <li>Real-time video feed with bounding boxes from YOLOv8 detections</li> <li>Voice command recognition status and interpreted actions</li> <li>Robot's velocity, heading, and navigation path</li> <li>Sensor diagnostics such as LiDAR, IMU, and hazard detection</li> </ul> <p>ROS2 topics connected to the GUI include:</p> <ul> <li><code>/yolov8_detections</code> \u2013 Object detection overlays</li> <li><code>/voice_cmd</code> \u2013 Recognized voice command topics</li> <li><code>/cmd_vel</code> \u2013 Velocity control signals</li> <li><code>/rpi_13/hazard_detection</code> \u2013 Safety monitoring</li> <li><code>/rpi_13/imu</code> \u2013 IMU motion feedback</li> </ul> <p>RQT GRAPH:</p> <ol> <li>From Dummy nodes (expected rqt_graph):</li> </ol> <p></p> <p>Final RQT-GRAPH: </p> <p>FINAL RQT GRAPH WITH GUI</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#42-gui-layout-design-concepts","title":"4.2 GUI Layout &amp; Design Concepts","text":"<p>Initial Mockup using Inkscape </p> <p>Proposed Interactive GUI for ROS2 Integration </p> <p>Test 1: GUI Working on Real-time Data - Faced issue with camera (not fixed) - IMU - LiDAR</p> <ol> <li>GUI Updating IMU Data Live:</li> </ol> <p></p> <ol> <li>GUI Updating LiDAR Data Live:</li> </ol> <p></p> <p>3. GUI Updating Camera Data Live:</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#these-gui-designs-aim-to-present-important-real-time-system-data-in-an-intuitive-and-user-friendly-format-enabling-both-operator-awareness-and-effective-debugging","title":"These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#43-live-sensor-visualization-in-gui","title":"4.3 Live Sensor Visualization in GUI","text":"<p>The GUI aggregates multiple sensor streams into a unified dashboard. Key visualized components:</p> <ul> <li> <p>Object Detection Feed:   Annotated Oak-D camera frames showing bounding boxes and labels.</p> </li> <li> <p>Voice Command Status:   Display of the latest interpreted command and its mapped action.</p> </li> <li> <p>Navigation Planner View:   Real-time trajectory paths generated by the navigation stack.</p> </li> <li> <p>Sensor Diagnostics:   Live data from LiDAR, IMU, and hazard modules.</p> </li> </ul> <p>GUI With Live Sensor Data</p> <p></p> <p>GUI WITH ACTUAL DATA:</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#44-gui-demonstration-video","title":"4.4 GUI Demonstration Video","text":"<p>Test 01: Object Detection + GUI Overlay</p> <p> Real-time object detection using YOLOv8 with GUI overlay showing detected objects and confidence scores.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#additional-demonstration-videos","title":"Additional Demonstration Videos","text":"<p>Test 02: Controlling the Cobot Robotic Arm</p> <p> </p> <p>A short demo showcasing voice-command-based control of TurtleBot4 in a dynamic environment.</p> <p>Test 03: Full Autonomy Simulation (Without GUI)</p> <p> </p> <p>Test 04: Live Demonstration Test 2 (With GUI)</p> <p> </p> <p>Test 04: Turtlebot4 Autonomy with MyCobot (Mobile Manipulator)</p> <p> </p> <p>A future test combining voice input, object detection, and autonomous navigation on real hardware.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#45-control-architecture-in-ros2","title":"4.5 Control Architecture in ROS2","text":"<p>The following flowchart summarizes the control pipeline, organized by data type and decision layer:</p> <pre><code>%%{ init: { \"theme\": \"default\" } }%%\ngraph TD\n\n  %% Subgraphs\n  subgraph SENSORS [\"Sensor Inputs\"]\n    CAM[Oak-D Camera]\n    MIC[Microphone]\n    LIDAR[IR / LiDAR Sensors]\n  end\n\n  subgraph PERCEPTION [\"Perception Nodes\"]\n    YOLO_NODE[Node: yolov8_processor]\n    VOICE_NODE[Node: voice_input_node]\n    DETECTIONS[/yolov8_detections/]\n    VOICE_RAW[/voice_input/]\n    CAM --&gt; YOLO_NODE --&gt; DETECTIONS\n    MIC --&gt; VOICE_NODE --&gt; VOICE_RAW\n  end\n\n  subgraph AUTONOMY [\"High-Level Autonomy\"]\n    VOICE_PARSER[Node: voice_command_parser]\n    VOICE_CMD[/voice_cmd/]\n    DECISION[Node: decision_maker_node]\n    ACT_CMD[/action_cmd/]\n    VOICE_RAW --&gt; VOICE_PARSER --&gt; VOICE_CMD\n    DETECTIONS --&gt; DECISION\n    VOICE_CMD --&gt; DECISION --&gt; ACT_CMD\n  end\n\n  subgraph CONTROL [\"Low-Level Control\"]\n    COLLISION[Node: collision_avoidance_node]\n    NAV[Node: navigation_controller]\n    CMD[/cmd_vel/]\n    LIDAR --&gt; COLLISION\n    ACT_CMD --&gt; COLLISION --&gt; NAV --&gt; CMD\n  end\n\n  CMD --&gt; MOVE[TurtleBot Movement]\n\n  %% Color classes\n  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef perception fill:#e6e6fa,stroke:#7e57c2,color:#000\n  classDef autonomy fill:#ffe0e6,stroke:#e91e63,color:#000\n  classDef control fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef output fill:#eeeeee,stroke:#757575,color:#000\n\n  %% Assign classes\n  class CAM,MIC,LIDAR sensors\n  class YOLO_NODE,VOICE_NODE,DETECTIONS,VOICE_RAW perception\n  class VOICE_PARSER,VOICE_CMD,DECISION,ACT_CMD autonomy\n  class COLLISION,NAV,CMD control\n  class MOVE output\n</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#interaction-mechanism","title":"Interaction Mechanism","text":"<p>Behavioral Influence &amp; Interfaces</p> <ul> <li>Voice Command API: Users instruct the robot using predefined phrases.</li> <li>ROS2 RQT GUI/Web Dashboard: For remote monitoring.</li> </ul> <p>(Include a professional-looking UI sketch showing how users will interact with the system.)</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#preparation-needs","title":"Preparation Needs","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#required-knowledge-topics-for-success","title":"Required Knowledge &amp; Topics for Success","text":"<pre><code>graph TD\n  A[Preparation Needs]\n\n  A --&gt; B1[Object Detection]\n  A --&gt; B2[ROS2 Navigation]\n  A --&gt; B3[Speech Recognition]\n  A --&gt; B4[Hardware Control]\n\n  B1 --&gt; C1[YOLOv8]\n  B1 --&gt; C2[OpenCV]\n\n  B2 --&gt; C3[SLAM]\n  B2 --&gt; C4[Collision Avoidance]\n\n  B3 --&gt; C5[Microphone Input]\n  B3 --&gt; C6[Command Parsing]\n\n  B4 --&gt; C7[PC-Robot Communication]\n  B4 --&gt; C8[Predefined Actions]\n\n  %% Color styling\n  classDef detect fill:#ffe0e6,stroke:#e91e63,color:#000\n  classDef nav fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef speech fill:#e6e6fa,stroke:#7e57c2,color:#000\n  classDef hardware fill:#f9fbe7,stroke:#c0ca33,color:#000\n  classDef root fill:#f5f5f5,stroke:#9e9e9e,color:#000\n\n  class A root\n  class B1,C1,C2 detect\n  class B2,C3,C4 nav\n  class B3,C5,C6 speech\n  class B4,C7,C8 hardware\n\n</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#final-demonstration-plan","title":"Final Demonstration Plan","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#setup-execution","title":"Setup &amp; Execution","text":"<ul> <li>Classroom Resources: Open space for navigation demo.</li> <li>Demonstration Steps: The robot will identify objects, respond to user queries, navigate obstacles, and execute spoken commands.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#handling-environmental-variability-future-scope","title":"Handling Environmental Variability (future scope)","text":"<ul> <li>Adaptive Algorithms: Adjust object detection thresholds dynamically.</li> <li>Fallback Modes: If detection fails, switch to manual control or alternative recognition models.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#testing-evaluation-plan","title":"Testing &amp; Evaluation Plan","text":"<ul> <li>Simulated Testing in Gazebo before real-world deployment.</li> <li>Comparison Metrics:</li> <li>Object recognition accuracy.</li> <li>Speech command response time.</li> <li>Navigation success rate.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#impact","title":"Impact","text":"<p>This project will:</p> <ul> <li>Advance AI-driven robotics interactions for smart environments.</li> <li>Develop speech-integrated autonomous systems.</li> <li>Provide students hands-on experience with ROS2, AI, and embedded systems.</li> <li>Potentially contribute to assistive robotics research.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#future-scope","title":"Future Scope:","text":"<p>In future iterations, the system can be extended with:</p> <ul> <li>Robotic arm integration mounted on TurtleBot4 for executing pick-and-place actions based on detected objects.</li> <li>Task-oriented planning using semantic understanding of scenes (e.g., pick red object and place it near the wall).</li> </ul> <p></p> <p></p> <p>Here is the generated image of a cobot mounted on a TurtleBot4 using CHATGPT 4o</p> <p>FINAL GUI :</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#references-subject-to-change","title":"References (Subject to change):","text":"<ol> <li>Deep Learning model options: https://yolov8.com/</li> <li>YOLOv8 example: https://rs-punia.medium.com/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956</li> <li>Speech Recognition Libraries: https://pypi.org/project/SpeechRecognition/</li> <li>Turtlebot4 Mapping Resource: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/generate_map.html</li> <li>Mapping, Localizing, Path planning packages for Turtlebot4: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/turtlebot4_navigator.html</li> </ol>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#advising-resources","title":"Advising &amp; Resources","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-advisor","title":"Project Advisor","text":"<ul> <li>Dr. Daniel Aukes </li> <li>Resource Needs: Hardware support, mentorship on TurtleBot4 Hardware integration with ROS2.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#weekly-milestones-weeks-7-16","title":"Weekly Milestones (Weeks 7-16)","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#weekly-milestones-weeks-716","title":"Weekly Milestones (Weeks 7\u201316)","text":"Week Date Milestone Status Week 7 Feb 24, 2025 Finalizing project scope and hardware/sensor availability. \u2705 Completed Week 8 Mar 3, 2025 ROS2 environment setup, VM configuration, TurtleBot4 base initialization. \u2705 Completed Week 9 Mar 10, 2025 Object detection with YOLOv8 using Oak-D camera. \u2705 Completed Week 10 Mar 17, 2025 Voice command parsing, audio processing and integration. \u2705 Completed Week 11 Mar 24, 2025 GUI development and voice-based system control. \u2705 Completed Week 12 Mar 31, 2025 ROS2 node integration and layered autonomy testing. \u2705 Completed Week 13 Apr 7, 2025 TurtleBot testing with full pipeline and live demos. \u2705 Completed Week 14 Apr 14, 2025 Final debugging, fallback strategies, and performance evaluation. \ud83d\udd04 In Progress Week 15 Apr 21, 2025 Documentation, GUI improvements, and final video preparation. \ud83d\udd04 In Progress Week 16 Apr 28, 2025 \ud83d\ude80 Final Demonstration &amp; Submission \ud83d\udd04 In Progress","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#gantt-chart-representation","title":"Gantt Chart Representation","text":"<pre><code>gantt\n    title Project Timeline (Weeks 7\u201316)\n    dateFormat  YYYY-MM-DD\n    axisFormat  %b %d\n\n    section Planning\n    Finalize Scope            :active,milestone1, 2025-02-24, 7d\n\n    section Implementation\n    ROS2 Setup                :active,milestone2, 2025-03-03, 7d\n    GUI Development           :active,milestone3, 2025-03-10, 7d\n    YOLOv8 + Oak-D            :active,milestone4, 2025-03-17, 7d\n    Voice Command             :crit,milestone5, 2025-03-24, 7d\n    ROS2 Integration          :crit,milestone6, 2025-03-31, 10d\n\n    section Testing &amp; Deployment\n    TurtleBot Testing         :crit,milestone7, 2025-04-10, 7d\n    Final Debug               :milestone8, 2025-04-17, 5d\n    Docs &amp; Video              :milestone9, 2025-04-22, 5d\n    Final Demo                :milestone10, 2025-04-28, 6d</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"charts/","title":"Charts","text":""},{"location":"charts/#project-visual-overview","title":"Project Visual Overview","text":"Section Chart Title Description Timeline Gantt Chart Shows weekly project phases from planning to deployment. Workflow Project Workflow Chart Displays the full robot pipeline from sensor data to GUI feedback. System Control System Architecture + ROS2 Nodes &amp; Topics Visualizes ROS2 nodes and topics, including collision avoidance logic. Autonomy Layers High-Level vs Low-Level Control Flow Separates voice-guided task selection from low-level obstacle avoidance with aligned ROS2 nodes and data flow. Future Work Cobot Arm Integration Flow Outlines the proposed pick-and-place functionality using a mounted arm."},{"location":"charts/#gantt-chart-project-timeline-overview","title":"Gantt Chart \u2013 Project Timeline Overview","text":"<pre><code>gantt\n    title Project Timeline (Weeks 7\u201316)\n    dateFormat  YYYY-MM-DD\n    axisFormat  %b %d\n\n    section Planning\n    Finalize Scope            :active,milestone1, 2025-02-24, 7d\n\n    section Implementation\n    ROS2 Setup                :active,milestone2, 2025-03-03, 7d\n    GUI Development           :active,milestone3, 2025-03-10, 7d\n    YOLOv8 + Oak-D            :active,milestone4, 2025-03-17, 7d\n    Voice Command             :crit,milestone5, 2025-03-24, 7d\n    ROS2 Integration          :crit,milestone6, 2025-03-31, 10d\n\n    section Testing &amp; Deployment\n    TurtleBot Testing         :crit,milestone7, 2025-04-10, 7d\n    Final Debug               :milestone8, 2025-04-17, 5d\n    Docs &amp; Video              :milestone9, 2025-04-22, 5d\n    Final Demo                :milestone10, 2025-04-28, 6d</code></pre>"},{"location":"charts/#main-project-pipeline","title":"Main Project Pipeline","text":"<ul> <li>The flowchart below represents the overall working of our Intelligent TurtleBot4 system. </li> <li>It starts with sensor data collection from the Oak-D camera, IMU, LiDAR, and microphone. </li> <li> <p>The data is then preprocessed and sent to two parallel modules: YOLOv8 for object detection and voice recognition for interpreting commands.</p> </li> <li> <p>Outputs from both modules are fed into a decision-making node that determines the robot's next action. </p> </li> <li>The chosen action is executed via the ROS2 navigation stack and published as movement commands. </li> <li>Simultaneously, the GUI updates with relevant feedback, allowing users to visualize object info and robot behavior in real time.</li> </ul>"},{"location":"charts/#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-c-c-d1-e-c-d2-f-e-g-f-g-g-h-i-k-m-g-j-l-m-classdef-sensors-filld0f0efstroke0097a7color000-classdef-perception-fille8eaf6stroke5c6bc0color000-classdef-decision-fillffe0e6stroked81b60color000-classdef-control-fillfff9c4strokefbc02dcolor000-classdef-gui-fillede7f6stroke7e57c2color000-classdef-startend-filleeeeeestroke757575color000-class-am-startend-class-b-sensors-class-cd1d2ef-perception-class-g-decision-class-hik-control-class-jl-gui","title":"<pre><code>graph TD\n  A[\"Start:&lt;br/&gt;TurtleBot4 Powered On\"]\n  B[\"Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic\"]\n  C[\"Sensor Data&lt;br/&gt;Preprocessing\"]\n  D1[\"YOLOv8&lt;br/&gt;Object Detection\"]\n  D2[\"Voice Command&lt;br/&gt;Recognition\"]\n  E[\"Detected Object Info\"]\n  F[\"Intent or Goal Command\"]\n  G[\"Decision-Making Node\"]\n  H[\"ROS2 Navigation Stack\"]\n  I[\"Movement Commands&lt;br/&gt;via /cmd_vel\"]\n  J[\"GUI Update:&lt;br/&gt;Object and Nav Info\"]\n  K[\"Actuator Response:&lt;br/&gt;TurtleBot Moves\"]\n  L[\"User Feedback:&lt;br/&gt;GUI Visualization\"]\n  M[\"End\"]\n\n  A --&gt; B --&gt; C\n  C --&gt; D1 --&gt; E\n  C --&gt; D2 --&gt; F\n  E --&gt; G\n  F --&gt; G\n  G --&gt; H --&gt; I --&gt; K --&gt; M\n  G --&gt; J --&gt; L --&gt; M\n\n  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef perception fill:#e8eaf6,stroke:#5c6bc0,color:#000\n  classDef decision fill:#ffe0e6,stroke:#d81b60,color:#000\n  classDef control fill:#fff9c4,stroke:#fbc02d,color:#000\n  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000\n  classDef startend fill:#eeeeee,stroke:#757575,color:#000\n\n  class A,M startend\n  class B sensors\n  class C,D1,D2,E,F perception\n  class G decision\n  class H,I,K control\n  class J,L gui\n</code></pre>","text":""},{"location":"charts/#system-control-and-autonomy-flow","title":"System Control and Autonomy Flow","text":"<ul> <li>The diagram below presents the complete decision and control loop of our TurtleBot4 system.</li> <li>From user voice inputs and real-time camera feeds, data is collected, processed, and passed through an autonomy layer to issue movement commands.</li> <li>Each subsystem (sensor, processing, autonomy) is driven by dedicated ROS2 nodes communicating through standard topics.</li> </ul>"},{"location":"charts/#graph-td-top-level-flow-aturtlebot4-booted-bsensor-data-collection-a-b-sensor-layer-subgraph-sensor_layer-ros2-sensor-layer-n1node-oakd_camera_node-t1rpi_13oakdpreviewimage_raw-n2node-voice_input_node-t2voice_input-end-processing-layer-subgraph-processing_layer-ros2-perception-command-parsing-n3node-yolov8_processor-t3yolov8_detections-n4node-voice_command_parser-t4voice_cmd-end-autonomy-layer-subgraph-autonomy_layer-ros2-autonomy-control-n5node-decision_maker_node-n6node-collision_avoidance_node-n7node-navigation_controller-t5action_cmd-t6cmd_vel-n5-t5-n5-n6-n7-t6-end-flow-between-subgraphs-b-sensor_layer-processing_layer-autonomy_layer-cturtlebot4-output-autonomy_layer-c","title":"<pre><code>graph TD\n\n  %% Top-Level Flow\n  A[TurtleBot4 Booted]\n  B[Sensor Data Collection]\n\n  A --&gt; B\n\n  %% Sensor Layer\n  subgraph Sensor_Layer [ROS2: Sensor Layer]\n    N1[Node: oakd_camera_node] --&gt; T1[/rpi_13/oakd/preview/image_raw/]\n    N2[Node: voice_input_node] --&gt; T2[/voice_input/]\n  end\n\n  %% Processing Layer\n  subgraph Processing_Layer [ROS2: Perception &amp; Command Parsing]\n    N3[Node: yolov8_processor] --&gt; T3[/yolov8_detections/]\n    N4[Node: voice_command_parser] --&gt; T4[/voice_cmd/]\n  end\n\n  %% Autonomy Layer\n  subgraph Autonomy_Layer [ROS2: Autonomy &amp; Control]\n    N5[Node: decision_maker_node]\n    N6[Node: collision_avoidance_node]\n    N7[Node: navigation_controller]\n    T5[/action_cmd/]\n    T6[/cmd_vel/]\n    N5 --&gt; T5\n    N5 --&gt; N6 --&gt; N7 --&gt; T6\n  end\n\n  %% Flow between subgraphs\n  B --&gt; Sensor_Layer --&gt; Processing_Layer --&gt; Autonomy_Layer\n  C[Turtlebot4 Output]\n  Autonomy_Layer --&gt; C\n</code></pre>","text":""},{"location":"charts/#hybrid-ros2-system-architecture","title":"Hybrid ROS2 System Architecture","text":"<ul> <li>This diagram separates high-level autonomy (top row) from low-level motion control (bottom row), with clearly labeled ROS2 nodes and topic communication.</li> <li>It maintains modular blocks for perception, decision-making, and actuation while improving visual alignment and clarity.</li> </ul>"},{"location":"charts/#graph-td-sensors-subgraph-sensors-sensor-inputs-camoak-d-camera-micmicrophone-lidarir-lidar-sensors-end-perception-layer-subgraph-perception-perception-nodes-yolonode-yolov8_processor-voice_rawnode-voice_input_node-yolo_outyolov8_detections-voice_invoice_input-cam-yolo-yolo_out-mic-voice_raw-voice_in-end-high-level-autonomy-subgraph-high_auto-high-level-autonomy-voice_parsernode-voice_command_parser-voice_cmdvoice_cmd-decidenode-decision_maker_node-act_cmdaction_cmd-voice_in-voice_parser-voice_cmd-yolo_out-decide-voice_cmd-decide-act_cmd-end-low-level-autonomy-subgraph-low_auto-low-level-control-coll_avoidnode-collision_avoidance_node-nav_ctrlnode-navigation_controller-cmd_velcmd_vel-lidar-coll_avoid-act_cmd-coll_avoid-nav_ctrl-cmd_vel-end-actuation-cmd_vel-tbotturtlebot-movement-block-to-block-flow-sensors-perception-high_auto-low_auto-tbot","title":"<pre><code>graph TD\n\n  %% Sensors\n  subgraph SENSORS [Sensor Inputs]\n    CAM[Oak-D Camera]\n    MIC[Microphone]\n    LIDAR[IR / LiDAR Sensors]\n  end\n\n  %% Perception Layer\n  subgraph PERCEPTION [Perception Nodes]\n    YOLO[Node: yolov8_processor]\n    VOICE_RAW[Node: voice_input_node]\n    YOLO_OUT[/yolov8_detections/]\n    VOICE_IN[/voice_input/]\n    CAM --&gt; YOLO --&gt; YOLO_OUT\n    MIC --&gt; VOICE_RAW --&gt; VOICE_IN\n  end\n\n  %% High-Level Autonomy\n  subgraph HIGH_AUTO [High-Level Autonomy]\n    VOICE_PARSER[Node: voice_command_parser]\n    VOICE_CMD[/voice_cmd/]\n    DECIDE[Node: decision_maker_node]\n    ACT_CMD[/action_cmd/]\n    VOICE_IN --&gt; VOICE_PARSER --&gt; VOICE_CMD\n    YOLO_OUT --&gt; DECIDE\n    VOICE_CMD --&gt; DECIDE --&gt; ACT_CMD\n  end\n\n  %% Low-Level Autonomy\n  subgraph LOW_AUTO [Low-Level Control]\n    COLL_AVOID[Node: collision_avoidance_node]\n    NAV_CTRL[Node: navigation_controller]\n    CMD_VEL[/cmd_vel/]\n    LIDAR --&gt; COLL_AVOID\n    ACT_CMD --&gt; COLL_AVOID --&gt; NAV_CTRL --&gt; CMD_VEL\n  end\n\n  %% Actuation\n  CMD_VEL --&gt; TBOT[TurtleBot Movement]\n\n  %% Block-to-block flow\n  SENSORS --&gt; PERCEPTION --&gt; HIGH_AUTO --&gt; LOW_AUTO --&gt; TBOT</code></pre>","text":""},{"location":"charts/#future-work-concept-turtlebot4-with-mounted-cobot-arm","title":"Future Work Concept: TurtleBot4 with Mounted Cobot Arm","text":"<p>This future work visual outlines the integration of a robotic arm on TurtleBot4.  The system uses object detection and coordinates from the perception pipeline to compute inverse kinematics and execute pick-and-place actions via a dedicated ROS2 control node.</p> <p>Goals to Capture Visually: - Addition of a robotic arm mounted on TurtleBot4 - Use of ROS2 for communication with the arm - Performing pick-and-place tasks - Integration with existing perception (e.g., YOLOv8 object detection for picking targets)</p> <pre><code>graph TD\n\n  %% Object Detection &amp; Localization\n  V1[\"Object Detection&lt;br/&gt;using YOLOv8\"]\n  V2[\"Target Object&lt;br/&gt;Coordinates\"]\n\n  %% Motion Planning\n  M1[\"Inverse Kinematics&lt;br/&gt;&amp; Arm Planning\"]\n\n  %% Platform Base\n  P2[\"TurtleBot4 Base Platform\"]\n  P1[\"Mounted Cobot Arm&lt;br/&gt;on Platform\"]\n\n  %% ROS2 Control\n  N1[\"ROS2 Arm&lt;br/&gt;Control Node\"]\n\n  %% Execution\n  E1[\"Pick and Place&lt;br/&gt;Execution\"]\n  E2[\"Object&lt;br/&gt;Grasped and Placed\"]\n\n  %% Connections\n  V1 --&gt; V2 --&gt; M1 --&gt; N1\n  P1 --&gt; P2 --&gt; N1\n  N1 --&gt; E1 --&gt; E2</code></pre>"},{"location":"esp-32-table/","title":"Sensors Table","text":""},{"location":"esp-32-table/#turtlebot-4-sensor-table","title":"TurtleBot 4 Sensor Table","text":"Sensor Implementation Image Depth Camera- OAK-D-Pro) Object detection and distance estimation LiDAR- RPLIDAR A1M8 SLAM-based navigation and obstacle detection IMU Enhancing motion stability and drift correction Microphone Capturing voice commands Speaker Responding with audio feedback on Host PC"},{"location":"obj_detect/","title":"Object Detection Page","text":"<p>Example: YOLOv8 implementation</p> <p></p>"},{"location":"pic-table/","title":"TurtleBot4 Hardware","text":"Feature TurtleBot 4 Lite TurtleBot 4 Size (L x W x H) 342 x 339 x 192 mm 342 x 339 x 351 mm Weight 3270 g 3945 g Base platform iRobot\u00ae Create\u00ae 3 iRobot\u00ae Create\u00ae 3 Wheels (Diameter) 72 mm 72 mm Ground Clearance 4.5 mm 4.5 mm On-board Computer Raspberry Pi 4B 4GB Raspberry Pi 4B 4GB Maximum linear velocity 0.31 m/s (safe mode), 0.46 m/s (no safe mode) 0.31 m/s (safe mode), 0.46 m/s (no safe mode) Maximum angular velocity 1.90 rad/s 1.90 rad/s Maximum payload 9 kg 9 kg Operation time 2h 30m - 4h (depending on load) 2h 30m - 4h (depending on load) Charging time 2h 30m 2h 30m Bluetooth Controller Not Included TurtleBot 4 Controller Lidar RPLIDAR A1M8 RPLIDAR A1M8 Camera OAK-D-Lite OAK-D-Pro User Power VBAT @1.9A  5V @ Low current  3.3V @ Low current VBAT @ 300 mA  12V @ 300 mA  5V @ 500 mA  3.3V @ 250 mA USB Expansion USB 2.0 (Type A) x2  USB 3.0 (Type A) x2 USB 2.0 (Type A) x2  USB 3.0 (Type A) x1  USB 3.0 (Type C) x4 Programmable LEDs Create\u00ae 3 Lightring Create\u00ae 3 Lightring  User LED x2 Status LEDs - Power LED  Motors LED  WiFi LED  Comms LED  Battery LED Buttons and Switches Create\u00ae 3 User buttons x2  Create\u00ae 3 Power Button x1 Create\u00ae 3 User buttons x2  Create\u00ae 3 Power Button x1  User Buttons x4 Battery 26 Wh Lithium Ion (14.4V nominal) 26 Wh Lithium Ion (14.4V nominal) Charging Dock Included Included"},{"location":"second-page/","title":"Speech Recognition Page","text":""},{"location":"component-selection-example/","title":"Component Selection Example","text":""},{"location":"component-selection-example/#examples","title":"Examples","text":""},{"location":"component-selection-example/#style-1","title":"Style 1","text":"<p>This is the example found in the assignment, uses more html</p> <p>Table 1: Example component selection</p> <p>External Clock Module</p> Solution Pros Cons Option 1. XC1259TR-ND surface mount crystal$1/eachlink to product * Inexpensive[^1]* Compatible with PSoC* Meets surface mount constraint of project * Requires external components and support circuitry for interface* Needs special PCB layout. * Option 2. * CTX936TR-ND surface mount oscillator * $1/each * Link to product * Outputs a square wave * Stable over operating temperature  * Direct interface with PSoC (no external circuitry required) range * More expensive * Slow shipping speed <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"component-selection-example/#style-2","title":"Style 2","text":"<p>Also acceptable, more markdown friendly</p> <p>External Clock Module</p> <ol> <li> <p>XC1259TR-ND surface mount crystal</p> <p></p> <ul> <li>$1/each</li> <li>link to product</li> </ul> Pros Cons Inexpensive Requires external components and support circuitry for interface Compatible with PSoC Needs special PCB layout. Meets surface mount constraint of project </li> <li> <p>CTX936TR-ND surface mount oscillator</p> <p></p> <ul> <li>$1/each</li> <li>Link to product</li> </ul> Pros Cons Outputs a square wave More expensive Stable over operating temperature Slow shipping speed Direct interface with PSoC (no external circuitry required) range </li> </ol> <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"},{"location":"subfolder/","title":"This is the index to a subfolder","text":"<p>Things to discuss</p>"},{"location":"subfolder/another-subfile/","title":"This is a secondary sub page","text":"<p>Things to discuss</p>"}]}