{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Project Intelligent TurtleBot4","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#team-information","title":"Team Information","text":"<ul> <li>Project Name: Intelligent TurtleBot: Deep Learning-Based Object Detection and Voice-Guided Navigation</li> <li>Team Number: 11</li> <li>Team Members: Anushka Gangadhar Satav, Adithya Konda, Sameerjeet Singh Chhabra</li> <li>Semester: Spring 2025</li> <li>University: Arizona State University</li> <li>Class: RAS 598 Experimentation and Deployment of Robots</li> <li>Professor: Dr. Dan Aukes</li> <li>Email: anushka.satav@asu.edu, akonda5@asu.edu, schhab18@asu.edu</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-plan","title":"Project Plan","text":"<p>Research Question:  How can a mobile robot effectively combine vision, speech, and autonomous navigation to create a responsive and interactive system in real-world environments?</p> <p>Concept: </p> <p>This project explores how TurtleBot4 can intelligently interact with its surroundings through vision-based object detection and speech-based communication. Using TurtleBot 4 with Create 3 and Raspberry Pi, our aim is to integrate:  </p> <ul> <li>Real-time object detection using YOLOv8 for recognizing and categorizing objects in the environment.  </li> <li>Voice command interaction for user control, allowing spoken instructions to guide the robot's behavior.  </li> <li>Autonomous navigation with obstacle avoidance, ensuring safe and efficient movement in dynamic spaces.  </li> <li>Dynamic responses based on detected objects, enabling context-aware robotic actions.  </li> <li>Fallback mechanisms for handling hardware/software limitations and ensuring system robustness.  </li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#gui-mockup-update-31032025","title":"GUI - Mockup Update (31/03/2025)","text":"<p>1. Using Inkscape</p> <p></p> <p>2. Proposed GUI For Control</p> <p></p> <p>3. Proposed GUI For Navigation </p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#test-01","title":"Test 01","text":"<p>Youtube Video Link: https://youtube.com/shorts/hz7PwtZZgPg?si=9SIvASX0w476p8vp</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#sensor-integration","title":"Sensor Integration","text":"<p>Utilization of Sensor Data</p> <p></p> <p>Check Sensors Table for more Information</p> <ul> <li>Depth Camera (OAK-D/RealSense): Object detection and distance estimation.</li> <li>LiDAR: SLAM-based navigation and obstacle detection.</li> <li>IMU: Enhancing motion stability and drift correction.</li> <li>Microphone: Capturing voice commands.</li> <li>Speaker: Responding with audio feedback.</li> </ul> <p>Testing and Demonstration</p> <ul> <li>Unit Testing: Each sensor tested in isolation.</li> <li>Integration Testing: Validating sensor fusion for decision-making.</li> <li>Final Demonstration: Robot detects objects, responds to queries, follows voice commands, and navigates autonomously.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#interaction-mechanism","title":"Interaction Mechanism","text":"<p>Behavioral Influence &amp; Interfaces</p> <ul> <li>Voice Command API: Users instruct the robot using predefined phrases.</li> <li>ROS2 RQT GUI/Web Dashboard: For remote monitoring.</li> </ul> <p>(Include a professional-looking UI sketch showing how users will interact with the system.)</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#control-autonomy","title":"Control &amp; Autonomy","text":"<ul> <li>Sensor-Driven Control Loop: Robot makes decisions based on camera, LiDAR, and IMU inputs.</li> <li>ROS2 Navigation Stack: Used for path planning and movement.</li> <li>Hierarchical Decision Model: Combining high-level commands with low-level execution.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#preparation-needs","title":"Preparation Needs","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#required-knowledge-topics-for-success","title":"Required Knowledge &amp; Topics for Success","text":"<ul> <li>Deep Learning for Object Detection (YOLOv8, OpenCV)</li> </ul> <ul> <li>ROS2 Navigation &amp; Collision Avoidance</li> </ul> <ul> <li>Speech Recognition</li> </ul> <ul> <li>Hardware-Level Control for TurtleBot 4</li> </ul> <p>Using communication between Host PC and TurtleBot4 to take Voice Commands as inputs and perform Pre-defined Actions</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#final-demonstration-plan","title":"Final Demonstration Plan","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#setup-execution","title":"Setup &amp; Execution","text":"<ul> <li>Classroom Resources: Open space for navigation demo.</li> <li>Demonstration Steps: The robot will identify objects, respond to user queries, navigate obstacles, and execute spoken commands.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#handling-environmental-variability-future-scope","title":"Handling Environmental Variability (future scope)","text":"<ul> <li>Adaptive Algorithms: Adjust object detection thresholds dynamically.</li> <li>Fallback Modes: If detection fails, switch to manual control or alternative recognition models.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#testing-evaluation-plan","title":"Testing &amp; Evaluation Plan","text":"<ul> <li>Simulated Testing in Gazebo before real-world deployment.</li> <li>Comparison Metrics:</li> <li>Object recognition accuracy.</li> <li>Speech command response time.</li> <li>Navigation success rate.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#impact","title":"Impact","text":"<p>This project will:</p> <ul> <li>Advance AI-driven robotics interactions for smart environments.</li> <li>Develop speech-integrated autonomous systems.</li> <li>Provide students hands-on experience with ROS2, AI, and embedded systems.</li> <li>Potentially contribute to assistive robotics research.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#references-subject-to-change","title":"References (Subject to change):","text":"<ol> <li>Deep Learning model options: https://yolov8.com/</li> <li>YOLOv8 example: https://rs-punia.medium.com/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956</li> <li>Speech Recognition Libraries: https://pypi.org/project/SpeechRecognition/</li> <li>Turtlebot4 Mapping Resource: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/generate_map.html</li> <li>Mapping, Localizing, Path planning packages for Turtlebot4: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/turtlebot4_navigator.html</li> </ol>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#advising-resources","title":"Advising &amp; Resources","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-advisor","title":"Project Advisor","text":"<ul> <li>Dr. Daniel Aukes </li> <li>Resource Needs: Hardware support, mentorship on TurtleBot4 Hardware integration with ROS2.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#weekly-milestones-weeks-7-16","title":"Weekly Milestones (Weeks 7-16)","text":"Week Date Milestone Status Week 7 Feb 24, 2025 Finalizing project scope, confirming sensor availability. \ud83d\udd04 In Progress Week 8 Mar 3, 2025 Setting up ROS2 communication between Raspberry Pi and host machine. \u23f3 Pending Week 9 Mar 10, 2025 Implementing object detection pipeline. \u23f3 Pending Week 10 Mar 17, 2025 Developing speech recognition module. \u23f3 Pending Week 11 Mar 24, 2025 Testing navigation with LiDAR and obstacle avoidance. \u23f3 Pending Week 12 Mar 31, 2025 Integrating all modules for unified control. \u23f3 Pending Week 13 Apr 7, 2025 Conducting real-world tests and debugging issues. \u23f3 Pending Week 14 Apr 14, 2025 Preparing final demonstration setup. \u23f3 Pending Week 15 Apr 21, 2025 Final testing, documentation, and advisor review. \u23f3 Pending Week 16 Apr 28, 2025 \ud83d\ude80 Final Demonstration &amp; Project Submission \ud83c\udfaf \ud83d\ude80 Upcoming","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#gantt-chart-representation","title":"Gantt Chart Representation","text":"<pre><code>gantt\n    title Project Timeline (Weeks 7-16)\n    dateFormat  YYYY-MM-DD\n    section Planning\n    Finalizing Scope :active, milestone1, 2025-02-24, 7d\n    section Implementation\n    ROS2 Setup :active, milestone2, 2025-03-03, 7d\n    Object Detection :milestone3, 2025-03-10, 7d\n    Speech Recognition :milestone4, 2025-03-17, 7d\n    Navigation Testing :milestone5, 2025-03-24, 7d\n    Integrate &amp; Debug :milestone6, 2025-03-31, 7d\n    section Testing &amp; Deployment\n    Real-World Testing :milestone7, 2025-04-07, 7d\n    Final Prep :milestone8, 2025-04-14, 7d\n    Document &amp; Review :milestone9, 2025-04-21, 7d\n    Final Demonstration :milestone10, 2025-04-28, 6d</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"charts/","title":"Charts","text":""},{"location":"charts/#project-visual-overview","title":"Project Visual Overview","text":"Section Chart Title Description Timeline Gantt Chart Shows weekly project phases from planning to deployment. Workflow Project Workflow Chart Displays the full robot pipeline from sensor data to GUI feedback. System Control System Architecture + ROS2 Nodes &amp; Topics Visualizes ROS2 nodes and topics, including collision avoidance logic. Future Work Cobot Arm Integration Flow Outlines the proposed pick-and-place functionality using a mounted arm."},{"location":"charts/#gantt-chart-project-timeline-overview","title":"Gantt Chart \u2013 Project Timeline Overview","text":"<pre><code>%%{ init: {\n  \"theme\": \"default\",\n  \"themeVariables\": {\n    \"taskTextColor\": \"#ffffff\",\n    \"taskBorderColor\": \"#666666\",\n    \"taskFill\": \"#444444\",\n    \"fontSize\": \"14px\",\n    \"gridColor\": \"#999999\"\n  },\n  \"gantt\": {\n    \"barHeight\": 30,\n    \"barGap\": 5\n  }\n} }%%\ngantt\n    title Project Timeline (Weeks 7\u201316)\n    dateFormat  YYYY-MM-DD\n    axisFormat  %b %d\n\n    section Planning\n    Finalize Scope            :milestone1, 2025-02-24, 7d\n\n    section Implementation\n    ROS2 Setup                :milestone2, 2025-03-03, 7d\n    GUI Development           :milestone3, 2025-03-10, 7d\n    YOLOv8 + Oak-D            :milestone4, 2025-03-17, 7d\n    Voice Command             :milestone5, 2025-03-24, 7d\n    ROS2 Integration          :milestone6, 2025-03-31, 10d\n\n    section Testing &amp; Deployment\n    TurtleBot Testing         :milestone7, 2025-04-10, 7d\n    Final Debug               :milestone8, 2025-04-17, 5d\n    Docs &amp; Video              :milestone9, 2025-04-22, 5d\n    Final Demo                :milestone10, 2025-04-28, 6d\n</code></pre>"},{"location":"charts/#main-project-pipeline","title":"Main Project Pipeline","text":"<ul> <li>The flowchart below represents the overall working of our Intelligent TurtleBot4 system. </li> <li>It starts with sensor data collection from the Oak-D camera, IMU, LiDAR, and microphone. </li> <li> <p>The data is then preprocessed and sent to two parallel modules: YOLOv8 for object detection and voice recognition for interpreting commands.</p> </li> <li> <p>Outputs from both modules are fed into a decision-making node that determines the robot's next action. </p> </li> <li>The chosen action is executed via the ROS2 navigation stack and published as movement commands. </li> <li>Simultaneously, the GUI updates with relevant feedback, allowing users to visualize object info and robot behavior in real time.</li> </ul>"},{"location":"charts/#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-c-c-d1-e-c-d2-f-e-g-f-g-g-h-i-k-m-g-j-l-m","title":"<pre><code>graph TD\n  A[\"Start:&lt;br/&gt;TurtleBot4 Powered On\"]\n  B[\"Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic\"]\n  C[\"Sensor Data&lt;br/&gt;Preprocessing\"]\n  D1[\"YOLOv8&lt;br/&gt;Object Detection\"]\n  D2[\"Voice Command&lt;br/&gt;Recognition\"]\n  E[\"Detected Object Info\"]\n  F[\"Intent or Goal Command\"]\n  G[\"Decision-Making Node\"]\n  H[\"ROS2 Navigation Stack\"]\n  I[\"Movement Commands&lt;br/&gt;via /cmd_vel\"]\n  J[\"GUI Update:&lt;br/&gt;Object and Nav Info\"]\n  K[\"Actuator Response:&lt;br/&gt;TurtleBot Moves\"]\n  L[\"User Feedback:&lt;br/&gt;GUI Visualization\"]\n  M[\"End\"]\n\n  A --&gt; B --&gt; C\n  C --&gt; D1 --&gt; E\n  C --&gt; D2 --&gt; F\n  E --&gt; G\n  F --&gt; G\n  G --&gt; H --&gt; I --&gt; K --&gt; M\n  G --&gt; J --&gt; L --&gt; M</code></pre>","text":""},{"location":"charts/#system-control-and-autonomy-flow","title":"System Control and Autonomy Flow","text":"<ul> <li>The diagram below presents the complete decision and control loop of our TurtleBot4 system.</li> <li>From user voice inputs and real-time camera feeds, data is collected, processed, and passed through an autonomy layer to issue movement commands.</li> <li>Each subsystem (sensor, processing, autonomy) is driven by dedicated ROS2 nodes communicating through standard topics.</li> </ul>"},{"location":"charts/#graph-td-top-level-flow-aturtlebot4-booted-bsensor-data-collection-a-b-sensor-layer-subgraph-sensor_layer-ros2-sensor-layer-n1node-oakd_camera_node-t1rpi_13oakdpreviewimage_raw-n2node-voice_input_node-t2voice_input-end-processing-layer-subgraph-processing_layer-ros2-perception-command-parsing-n3node-yolov8_processor-t3yolov8_detections-n4node-voice_command_parser-t4voice_cmd-end-autonomy-layer-subgraph-autonomy_layer-ros2-autonomy-control-n5node-decision_maker_node-n6node-collision_avoidance_node-n7node-navigation_controller-t5action_cmd-t6cmd_vel-n5-t5-n5-n6-n7-t6-end-flow-between-subgraphs-b-sensor_layer-processing_layer-autonomy_layer-cturtlebot4-output-autonomy_layer-c","title":"<pre><code>graph TD\n\n  %% Top-Level Flow\n  A[TurtleBot4 Booted]\n  B[Sensor Data Collection]\n\n  A --&gt; B\n\n  %% Sensor Layer\n  subgraph Sensor_Layer [ROS2: Sensor Layer]\n    N1[Node: oakd_camera_node] --&gt; T1[/rpi_13/oakd/preview/image_raw/]\n    N2[Node: voice_input_node] --&gt; T2[/voice_input/]\n  end\n\n  %% Processing Layer\n  subgraph Processing_Layer [ROS2: Perception &amp; Command Parsing]\n    N3[Node: yolov8_processor] --&gt; T3[/yolov8_detections/]\n    N4[Node: voice_command_parser] --&gt; T4[/voice_cmd/]\n  end\n\n  %% Autonomy Layer\n  subgraph Autonomy_Layer [ROS2: Autonomy &amp; Control]\n    N5[Node: decision_maker_node]\n    N6[Node: collision_avoidance_node]\n    N7[Node: navigation_controller]\n    T5[/action_cmd/]\n    T6[/cmd_vel/]\n    N5 --&gt; T5\n    N5 --&gt; N6 --&gt; N7 --&gt; T6\n  end\n\n  %% Flow between subgraphs\n  B --&gt; Sensor_Layer --&gt; Processing_Layer --&gt; Autonomy_Layer\n  C[Turtlebot4 Output]\n  Autonomy_Layer --&gt; C\n</code></pre>","text":""},{"location":"charts/#hybrid-ros2-system-architecture","title":"Hybrid ROS2 System Architecture","text":"<ul> <li>This diagram separates high-level autonomy (top row) from low-level motion control (bottom row), with clearly labeled ROS2 nodes and topic communication.</li> <li>It maintains modular blocks for perception, decision-making, and actuation while improving visual alignment and clarity.</li> </ul>"},{"location":"charts/#graph-td-sensors-subgraph-sensors-sensor-inputs-camoak-d-camera-micmicrophone-lidarir-lidar-sensors-end-perception-layer-subgraph-perception-perception-nodes-yolonode-yolov8_processor-voice_rawnode-voice_input_node-yolo_outyolov8_detections-voice_invoice_input-cam-yolo-yolo_out-mic-voice_raw-voice_in-end-high-level-autonomy-subgraph-high_auto-high-level-autonomy-voice_parsernode-voice_command_parser-voice_cmdvoice_cmd-decidenode-decision_maker_node-act_cmdaction_cmd-voice_in-voice_parser-voice_cmd-yolo_out-decide-voice_cmd-decide-act_cmd-end-low-level-autonomy-subgraph-low_auto-low-level-control-coll_avoidnode-collision_avoidance_node-nav_ctrlnode-navigation_controller-cmd_velcmd_vel-lidar-coll_avoid-act_cmd-coll_avoid-nav_ctrl-cmd_vel-end-actuation-cmd_vel-tbotturtlebot-movement-block-to-block-flow-sensors-perception-high_auto-low_auto-tbot","title":"<pre><code>graph TD\n\n  %% Sensors\n  subgraph SENSORS [Sensor Inputs]\n    CAM[Oak-D Camera]\n    MIC[Microphone]\n    LIDAR[IR / LiDAR Sensors]\n  end\n\n  %% Perception Layer\n  subgraph PERCEPTION [Perception Nodes]\n    YOLO[Node: yolov8_processor]\n    VOICE_RAW[Node: voice_input_node]\n    YOLO_OUT[/yolov8_detections/]\n    VOICE_IN[/voice_input/]\n    CAM --&gt; YOLO --&gt; YOLO_OUT\n    MIC --&gt; VOICE_RAW --&gt; VOICE_IN\n  end\n\n  %% High-Level Autonomy\n  subgraph HIGH_AUTO [High-Level Autonomy]\n    VOICE_PARSER[Node: voice_command_parser]\n    VOICE_CMD[/voice_cmd/]\n    DECIDE[Node: decision_maker_node]\n    ACT_CMD[/action_cmd/]\n    VOICE_IN --&gt; VOICE_PARSER --&gt; VOICE_CMD\n    YOLO_OUT --&gt; DECIDE\n    VOICE_CMD --&gt; DECIDE --&gt; ACT_CMD\n  end\n\n  %% Low-Level Autonomy\n  subgraph LOW_AUTO [Low-Level Control]\n    COLL_AVOID[Node: collision_avoidance_node]\n    NAV_CTRL[Node: navigation_controller]\n    CMD_VEL[/cmd_vel/]\n    LIDAR --&gt; COLL_AVOID\n    ACT_CMD --&gt; COLL_AVOID --&gt; NAV_CTRL --&gt; CMD_VEL\n  end\n\n  %% Actuation\n  CMD_VEL --&gt; TBOT[TurtleBot Movement]\n\n  %% Block-to-block flow\n  SENSORS --&gt; PERCEPTION --&gt; HIGH_AUTO --&gt; LOW_AUTO --&gt; TBOT</code></pre>","text":""},{"location":"charts/#future-work-concept-turtlebot4-with-mounted-cobot-arm","title":"Future Work Concept: TurtleBot4 with Mounted Cobot Arm","text":"<p>This future work visual outlines the integration of a robotic arm on TurtleBot4.  The system uses object detection and coordinates from the perception pipeline to compute inverse kinematics and execute pick-and-place actions via a dedicated ROS2 control node.</p> <p>Goals to Capture Visually: - Addition of a robotic arm mounted on TurtleBot4 - Use of ROS2 for communication with the arm - Performing pick-and-place tasks - Integration with existing perception (e.g., YOLOv8 object detection for picking targets)</p> <pre><code>graph TD\n\n  %% Object Detection &amp; Localization\n  V1[\"Object Detection&lt;br/&gt;using YOLOv8\"]\n  V2[\"Target Object&lt;br/&gt;Coordinates\"]\n\n  %% Motion Planning\n  M1[\"Inverse Kinematics&lt;br/&gt;&amp; Arm Planning\"]\n\n  %% Platform Base\n  P2[\"TurtleBot4 Base Platform\"]\n  P1[\"Mounted Cobot Arm&lt;br/&gt;on Platform\"]\n\n  %% ROS2 Control\n  N1[\"ROS2 Arm&lt;br/&gt;Control Node\"]\n\n  %% Execution\n  E1[\"Pick and Place&lt;br/&gt;Execution\"]\n  E2[\"Object&lt;br/&gt;Grasped and Placed\"]\n\n  %% Connections\n  V1 --&gt; V2 --&gt; M1 --&gt; N1\n  P1 --&gt; P2 --&gt; N1\n  N1 --&gt; E1 --&gt; E2</code></pre>"},{"location":"esp-32-table/","title":"Sensors Table","text":""},{"location":"esp-32-table/#turtlebot-4-sensor-table","title":"TurtleBot 4 Sensor Table","text":"Sensor Implementation Image Depth Camera- OAK-D-Pro) Object detection and distance estimation LiDAR- RPLIDAR A1M8 SLAM-based navigation and obstacle detection IMU Enhancing motion stability and drift correction Microphone Capturing voice commands Speaker Responding with audio feedback on Host PC"},{"location":"obj_detect/","title":"Object Detection Page","text":"<p>Example: YOLOv8 implementation</p> <p></p>"},{"location":"pic-table/","title":"TurtleBot4 Hardware","text":"Feature TurtleBot 4 Lite TurtleBot 4 Size (L x W x H) 342 x 339 x 192 mm 342 x 339 x 351 mm Weight 3270 g 3945 g Base platform iRobot\u00ae Create\u00ae 3 iRobot\u00ae Create\u00ae 3 Wheels (Diameter) 72 mm 72 mm Ground Clearance 4.5 mm 4.5 mm On-board Computer Raspberry Pi 4B 4GB Raspberry Pi 4B 4GB Maximum linear velocity 0.31 m/s (safe mode), 0.46 m/s (no safe mode) 0.31 m/s (safe mode), 0.46 m/s (no safe mode) Maximum angular velocity 1.90 rad/s 1.90 rad/s Maximum payload 9 kg 9 kg Operation time 2h 30m - 4h (depending on load) 2h 30m - 4h (depending on load) Charging time 2h 30m 2h 30m Bluetooth Controller Not Included TurtleBot 4 Controller Lidar RPLIDAR A1M8 RPLIDAR A1M8 Camera OAK-D-Lite OAK-D-Pro User Power VBAT @1.9A  5V @ Low current  3.3V @ Low current VBAT @ 300 mA  12V @ 300 mA  5V @ 500 mA  3.3V @ 250 mA USB Expansion USB 2.0 (Type A) x2  USB 3.0 (Type A) x2 USB 2.0 (Type A) x2  USB 3.0 (Type A) x1  USB 3.0 (Type C) x4 Programmable LEDs Create\u00ae 3 Lightring Create\u00ae 3 Lightring  User LED x2 Status LEDs - Power LED  Motors LED  WiFi LED  Comms LED  Battery LED Buttons and Switches Create\u00ae 3 User buttons x2  Create\u00ae 3 Power Button x1 Create\u00ae 3 User buttons x2  Create\u00ae 3 Power Button x1  User Buttons x4 Battery 26 Wh Lithium Ion (14.4V nominal) 26 Wh Lithium Ion (14.4V nominal) Charging Dock Included Included"},{"location":"second-page/","title":"Speech Recognition Page","text":""},{"location":"component-selection-example/","title":"Component Selection Example","text":""},{"location":"component-selection-example/#examples","title":"Examples","text":""},{"location":"component-selection-example/#style-1","title":"Style 1","text":"<p>This is the example found in the assignment, uses more html</p> <p>Table 1: Example component selection</p> <p>External Clock Module</p> Solution Pros Cons Option 1. XC1259TR-ND surface mount crystal$1/eachlink to product * Inexpensive[^1]* Compatible with PSoC* Meets surface mount constraint of project * Requires external components and support circuitry for interface* Needs special PCB layout. * Option 2. * CTX936TR-ND surface mount oscillator * $1/each * Link to product * Outputs a square wave * Stable over operating temperature  * Direct interface with PSoC (no external circuitry required) range * More expensive * Slow shipping speed <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"component-selection-example/#style-2","title":"Style 2","text":"<p>Also acceptable, more markdown friendly</p> <p>External Clock Module</p> <ol> <li> <p>XC1259TR-ND surface mount crystal</p> <p></p> <ul> <li>$1/each</li> <li>link to product</li> </ul> Pros Cons Inexpensive Requires external components and support circuitry for interface Compatible with PSoC Needs special PCB layout. Meets surface mount constraint of project </li> <li> <p>CTX936TR-ND surface mount oscillator</p> <p></p> <ul> <li>$1/each</li> <li>Link to product</li> </ul> Pros Cons Outputs a square wave More expensive Stable over operating temperature Slow shipping speed Direct interface with PSoC (no external circuitry required) range </li> </ol> <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"},{"location":"subfolder/","title":"This is the index to a subfolder","text":"<p>Things to discuss</p>"},{"location":"subfolder/another-subfile/","title":"This is a secondary sub page","text":"<p>Things to discuss</p>"}]}