{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#team-information","title":"Team Information","text":"<ul> <li>Project Name: Intelligent TurtleBot: Deep Learning-Based Object Detection and Voice-Guided Navigation</li> <li>Team Number: 11</li> <li>Team Members: Anushka Gangadhar Satav, Adithya Konda, Sameerjeet Singh Chhabra</li> <li>Semester: Spring 2025</li> <li>University: Arizona State University</li> <li>Class: RAS 598 Experimentation and Deployment of Robots</li> <li>Professor: Dr. Dan Aukes</li> <li>Email: anushka.satav@asu.edu, akonda5@asu.edu, schhab18@asu.edu</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-description","title":"PROJECT DESCRIPTION","text":"<p>Research Question \ud83d\udd0d:</p> <p>How can a mobile robot effectively combine vision, speech, navigation, and manipulation to create a responsive and intelligent system for real-world environments?</p> <p>Answer: </p> <p>Our project, Intelligent Voice-Controlled Mobile Manipulator, transforms the TurtleBot4 mobile base and the MyCobot robotic arm into a unified, intelligent, multi-modal robotic system.  Built entirely on ROS 2, the system fuses real-time perception (YOLOv8 object detection), voice interaction (Whisper speech-to-text), LiDAR-based obstacle mapping, and IMU-driven stability monitoring into a modular architecture.  A custom PyQt5 GUI offers live visualization of all sensor inputs and inference feedback. </p> <p>We successfully installed the MyCobot arm on the TurtleBot4 and developed our own inverse kinematics solver for accurate and smooth arm positioning. The system is now capable of autonomous object recognition, spatial understanding, and physical actuation. As part of our future scope, we aim to integrate voice-guided arm manipulation\u2014enabling fully hands-free mobile pick-and-place capabilities for applications like warehouse automation and home assistance.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-goals-update","title":"Project Goals Update \ud83c\udfaf","text":"<ol> <li> <p>At the start of the semester, our primary goal was to enable a TurtleBot4 robot to autonomously detect objects and navigate based on voice commands. As the project progressed, we expanded the scope to include real-time sensor integration (LiDAR, IMU), object detection using YOLOv8, and a custom GUI to visualize system performance.</p> </li> <li> <p>Midway through the project, we incorporated the MyCobot robotic arm onto the TurtleBot4 base and developed a custom inverse kinematics (IK) solver for precise manipulation tasks. This marked a major milestone, transforming our system from a mobile robot into a full-fledged mobile manipulator.</p> </li> <li> <p>Currently, the robot can perceive its environment, detect and classify objects, receive voice commands, and accurately move its robotic arm to target positions. In the near future, we plan to extend our voice interaction system to control the robotic arm\u2019s pick-and-place capabilities\u2014moving closer to a truly intelligent, voice-controlled mobile manipulator.</p> </li> </ol> <p>This evolution in project scope reflects our team\u2019s ambition to build a versatile robotic system capable of interacting with dynamic environments in real-time.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-plan","title":"Project Plan \ud83d\udccc","text":"<p>Concept Overview: </p> <p>This project investigates the design and development of a modular, intelligent robotic system built on the TurtleBot4 platform, enhanced with the MyCobot robotic arm. Our objective is to enable seamless integration of perception, voice interaction, navigation, and manipulation through ROS 2.</p> <p>The key functionalities implemented include:</p> <ul> <li> <p>Real-Time Object Detection: \ud83d\udd0d    Integration of YOLOv8 with live camera feeds enables robust object classification and visual awareness in dynamic environments.</p> </li> <li> <p>Voice Command Interaction: \ud83d\udde3\ufe0f    Whisper-based transcription allows natural human-robot interaction, enabling the robot to respond to spoken commands and queries.</p> </li> <li> <p>Sensor-Based State Awareness: \ud83d\udcca   LiDAR data is visualized in a radar-style map to display detected obstacles, while IMU data is plotted to reflect the robot\u2019s real-time motion and stability.</p> </li> <li> <p>Robotic Arm Integration and Control: \ud83e\uddbe   The MyCobot arm was mounted and controlled using a custom inverse kinematics solver for precise motion. Future work includes extending this to support voice-guided manipulation tasks.</p> </li> <li> <p>System Robustness and Fallback Mechanisms: \ud83e\udde9    The architecture includes modular nodes for fallback behaviors in case of sensor failure, unstable commands, or loss of feedback\u2014ensuring continuity of operation.</p> </li> </ul> <p>Together, these capabilities aim to demonstrate a cohesive and extensible mobile manipulator platform capable of real-time, human-guided autonomy.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#work-and-discussions","title":"Work and Discussions \ud83e\udde0","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#1-sensor-fusion-and-real-time-awareness","title":"1. Sensor Fusion and Real-Time Awareness","text":"<p>The Intelligent TurtleBot4 system fuses data from multiple sensory modalities \u2014 vision (camera), audio (microphone), and proximity (LiDAR + IMU) \u2014 to understand its environment and respond accordingly.  While we do not implement full navigation or motion planning, the system builds rich situational awareness using ROS2 and real-time data visualization.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#2-sensor-inputs","title":"2. Sensor Inputs","text":"<ul> <li>\ud83d\udcf7 Oak-D Camera: Provides RGB image streams for visual object detection using YOLOv8.</li> <li>\ud83c\udfa4 Microphone: Captures user voice input for natural language interaction.</li> <li>\ud83d\udce1 LiDAR: Streams distance measurements for environmental awareness, visualized in a radar-style plot.</li> <li>\ud83e\udded IMU: Reports linear acceleration and angular velocity, plotted live to assess robot stability and motion.</li> </ul> <p>Each of these sensors publishes to separate ROS2 topics and is handled by dedicated subscriber nodes for processing and visualization.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#3-perception-and-processing-nodes","title":"3. Perception and Processing Nodes","text":"<ul> <li>\ud83e\udde0 <code>object_detector_node</code>: Subscribes to <code>/oakd/rgb/preview/image_raw</code>, runs real-time object detection, and publishes annotated images and detected labels.</li> <li>\ud83d\udde3\ufe0f <code>mic_listener_node</code>: Captures and transcribes voice using Whisper.cpp, publishing text to <code>/voice_text</code>.</li> <li>\ud83d\udcd1 <code>voice_command_parser</code>: Parses text input into discrete commands and publishes to <code>/voice_command</code>.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#4-gui-node","title":"4. GUI Node","text":"<ul> <li>\ud83d\udcca <code>web_dashboard_node</code>: This node powers the real-time graphical user interface (GUI) for monitoring and interacting with the robot. Built using PyQt5, the GUI displays live camera feeds, YOLOv8-detected objects, incoming voice commands, and transcribed text. It also visualizes LiDAR scans around the TurtleBot4, and plots IMU data (linear acceleration and angular velocity) in real-time. </li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#voice-command-processing","title":"\ud83c\udfa4 Voice Command Processing","text":"<p>We use Whisper.cpp \u2014 a lightweight C++ port of OpenAI's Whisper \u2014 to transcribe raw audio into text on-device.  This allows the robot to operate voice interfaces without relying on external services, ensuring speed and privacy.  Transcribed commands are interpreted and displayed live on the GUI.</p> <p>Voice Command Set and Behaviors</p> <p>The following voice trigger phrases were recognized and mapped to robot behaviors using the <code>command_parser_node</code>. These commands were parsed from live audio using Whisper.cpp and translated into action via ROS 2 topics.</p> Voice Command Description <code>move_forward</code> Moves the TurtleBot forward. \ud83d\udeb6\u200d\u2642\ufe0f <code>move_backward</code> Moves the TurtleBot backward. \ud83d\udd19 <code>turn_left</code> Rotates the TurtleBot to the left. \u21a9\ufe0f <code>turn_right</code> Rotates the TurtleBot to the right. \u21aa\ufe0f <code>stop</code> Immediately halts all robot movement. \ud83d\uded1 <code>look_for_object</code> Initiates YOLOv8 object detection routine and reports objects seen. \ud83d\udd0d <code>go_to_object</code> Navigates toward the most recently detected object (future scope). \ud83c\udfaf <code>return_to_base</code> Commands the robot to return to the initial starting location. \ud83c\udfe0 <code>scan</code> Activates a LiDAR/IMU data scan visualization in the GUI. \ud83d\udce1 <code>repeat</code> Repeats the last recognized command. \ud83d\udd01 <code>dock</code> Sends a dock action request to return to the charging base. \ud83d\udd0c <code>undock</code> Sends an undock action request to leave the dock. \ud83d\udd0b <code>spin</code> Performs a 360\u00b0 rotation in place (playful/diagnostic motion). \ud83c\udf00 <code>shake</code> Moves the robotic arm in a 'shake' gesture. \ud83e\udd1d <code>take_a_picture</code> Captures an image from the camera feed (future scope). \ud83d\udcf7 <code>home_position</code> Sends the robotic arm to its defined home position. \ud83c\udfe1 <code>list_position</code> Moves the arm to a \u2018listening\u2019 posture (e.g., raised ready state). \ud83d\udc42 <code>relax_position</code> Sends the arm to a resting configuration. \ud83d\ude0c <code>wave_emote</code> Executes a waving gesture using the robotic arm. \ud83d\udc4b <code>grasp</code> Activates the gripper to grasp an object (on detection). \u270a <p>This command vocabulary enables natural interaction with the robot across multiple modes \u2014 navigation, perception, and manipulation.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#object-detection-with-yolov8","title":"\ud83d\udd0d Object Detection with YOLOv8","text":"<p>YOLOv8 Nano, integrated via Python and Ultralytics API, detects objects in real-time from camera streams. Detected labels are annotated on image frames and published to ROS topics. The modular object detector also logs the types of objects seen and supports a placeholder for future task planning.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#4-visualization-centered-architecture","title":"4. Visualization-Centered Architecture","text":"<p>Unlike traditional motion-planning stacks, our system focuses on responsive perception and human-in-the-loop monitoring through:</p> <ul> <li>\ud83d\udce1 LiDAR visualizations of the robot\u2019s surrounding environment</li> <li>\ud83d\udcc8 IMU plots for acceleration and angular motion over time</li> <li>\ud83d\udcf7 Live display of raw and detected camera feeds</li> <li>\ud83d\udde3\ufe0f Real-time display of voice input and parsed commands</li> <li>\ud83d\udd0d Display of currently detected objects (e.g., \u201cI see: person, chair\u201d)</li> </ul> <p>This approach lays the groundwork for robust perception and monitoring, while enabling future upgrades for autonomous manipulation.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#5-toward-semi-autonomous-intelligence","title":"5. Toward Semi-Autonomous Intelligence","text":"<p>We currently focus on perception, interface, and modular integration. Future versions will introduce:</p> <ul> <li>\ud83d\udd01 Voice-guided robotic arm manipulation</li> <li>\ud83e\udd16 Autonomous pick-and-place planning</li> <li>\ud83d\udce6 Voice-instructed object localization and handling</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#updated-goals-and-trade-offs","title":"\ud83d\udd04 Updated Goals and Trade-offs","text":"<p>Over the course of the project, several goals evolved and practical trade-offs had to be made due to technical challenges and platform constraints. These are summarized below:</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#changes-in-project-goals","title":"Changes in Project Goals \ud83d\udd27","text":"<ul> <li>Initially, the focus was only on voice-based control of the TurtleBot4 using Whisper transcription and YOLOv8 perception.</li> <li>Due to hardware limitations (e.g., non-functional laptop microphone at times), a PyQt5-based GUI control system was added that mirrors voice commands to allow fallback manual control. \ud83c\udf9b\ufe0f</li> <li>We later expanded the system to include real-time visualization of LiDAR and IMU data to increase system observability and allow performance monitoring. \ud83d\udcca</li> <li>Integration of the MyCobot arm was initially a future goal but was successfully completed. This included implementing a custom inverse kinematics solver and achieving precise pick-and-place movements. \ud83e\udd16</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#technical-trade-offs-made","title":"Technical Trade-offs Made \u2696\ufe0f","text":"<ul> <li>We began with Cyclone DDS over Wi-Fi for ROS 2 middleware communication. While topics were visible, we experienced significant instability \u2014 especially with image topics where the camera stream would crash or lag badly. \ud83d\udcf7\ud83d\udca5</li> <li>Switching to Fast DDS made topics visible but data streams were consistently empty \u2014 this issue consumed valuable time with no reliable resolution. \u26a0\ufe0f</li> <li> <p>Ultimately, we reverted to Cyclone DDS over a direct Ethernet connection, which worked flawlessly and delivered fast, stable ROS2 communication across devices. \ud83d\ude80</p> </li> <li> <p>YOLOv8 Nano model was chosen for object detection due to its balance of speed and accuracy. Larger models performed better but slowed inference drastically on the Raspberry Pi. \ud83d\udc22</p> </li> <li>For speech recognition, we initially explored OpenAI\u2019s Whisper via online inference, but it was extremely resource-intensive and unsuitable for local deployment. \ud83e\udde0\ud83d\udcbb</li> <li>Whisper.cpp was selected for its CPU-only support and faster inference. However, the tiny model gave poor transcription results, so we used the base model instead, which slightly compromised speed but greatly improved accuracy. \u23f3</li> <li>IMU and LiDAR data were initially unused, but were later added for feedback visualization on GUI to assist debugging and showcase real-time sensing. \ud83d\udce1</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#ros2-nodes-and-topics","title":"\ud83d\udce1 ROS2 Nodes and Topics","text":"<p>The following nodes form the backbone of our system, each responsible for a key capability:</p> <ul> <li><code>mic_listener_node</code>: Listens via microphone and transcribes speech to text using Whisper.cpp.</li> <li><code>command_parser_node</code>: Parses voice/GUIs commands and converts them into robot actions.</li> <li><code>movement_controller_node</code>: Publishes velocity commands to <code>/cmd_vel</code> and controls movement logic.</li> <li><code>object_detector_node</code>: Runs YOLOv8-based real-time object detection on live camera feed.</li> <li><code>web_dashboard_node</code>: Hosts the PyQt5 GUI, displaying all live system feedback and telemetry.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#docking-and-undocking-actions","title":"\ud83d\udd0c Docking and Undocking Actions","text":"<p>As part of the robot's mobility and autonomy framework, we integrated basic ROS 2 action-based behaviors for:</p> <ul> <li>\u2705 Docking: Sending a goal to the Create 3 base to autonomously return to its dock station for charging.</li> <li>\ud83d\udd04 Undocking: Sending an undock goal to initiate departure from the charging dock and resume exploration.</li> </ul> <p>These actions were tested and triggered via GUI or voice commands. The docking state was also monitored using the /rpi_13/dock_status topic. This provides a critical capability for future long-duration missions where power management becomes essential.</p> <p>RQT_GRAPH: </p> <p>Below is the ROS2 rqt_graph showing the interconnection of topics, services, and nodes:</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#gui-pyqt5-based","title":"GUI - PYQT5 BASED \ud83d\udda5\ufe0f","text":"<p>The <code>web_dashboard_node</code> provides a centralized PyQt5-based dashboard for real-time visualization and control of the TurtleBot4 system. This graphical interface allows users to monitor sensor feedback, camera inputs, object detection, voice commands, and system diagnostics \u2014 all in one place.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#subscribed-topics","title":"\ud83d\udce5 Subscribed Topics:","text":"<ul> <li><code>/oakd/rgb/preview/image_raw</code>: Displays live RGB camera feed.</li> <li><code>/yolo_image_raw</code>: Visualizes YOLOv8-annotated object detection output.</li> <li><code>/voice_text</code>: Shows the latest voice-transcribed command.</li> <li><code>/voice_command</code>: Displays the parsed voice or GUI command.</li> <li><code>/detected_objects</code>: Lists current objects detected by YOLO in real-time.</li> <li><code>/rpi_13/imu</code>: Streams IMU data (linear acceleration &amp; angular velocity) for plotting.</li> <li><code>/scan</code>: Plots LiDAR scan as a radial obstacle map with robot at center.</li> <li><code>/rpi_13/battery_state</code>: Can be used to monitor remaining battery capacity (future feature).</li> <li><code>/rpi_13/dock_status</code>: Monitors whether the robot is docked (used in future docking logic).</li> <li><code>/diagnostics</code>: Reads system-level diagnostics (optional, unused in current GUI).</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#published-topics","title":"\ud83d\udce4 Published Topics:","text":"<ul> <li><code>/voice_text</code>: Republished as needed (e.g., when GUI is used to simulate input).</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#features","title":"\ud83e\udde9 Features:","text":"<ul> <li>Displays two camera feeds (live and object detection) and a placeholder image panel.</li> <li>Real-time sensor visualizations for IMU (x, y, z linear &amp; angular plots) and LiDAR data.</li> <li>Text display boxes for transcribed voice text, parsed voice/GUI command, and current detected objects.</li> <li>Color-coded feedback and emojis for an intuitive, user-friendly experience.</li> <li>Modular layout with expandable PyQt5 widgets for future features such as battery visualization, GUI button control, and docking status indicators.</li> </ul> <p>This dashboard complements voice control by offering a manual fallback method for monitoring and interaction \u2014 especially useful when microphone input is unavailable or when debugging perception components.</p> <p>Empty GUI:</p> <p></p> <p>GUI With Data:</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#demonstration-videos","title":"Demonstration Videos \ud83d\udda5\ufe0f","text":"<p>Test 01: Object Detection + GUI Overlay</p> <p> Real-time object detection using YOLOv8 with GUI overlay showing detected objects and confidence scores.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#additional-demonstration-videos","title":"Additional Demonstration Videos","text":"<p>Test 02: Controlling the Cobot Robotic Arm</p> <p> </p> <p>A short demo showcasing voice-command-based control of TurtleBot4 in a dynamic environment.</p> <p>Test 03: Full Autonomy Simulation (Without GUI)</p> <p> </p> <p>Test 04: Live Demonstration Test 2 (With GUI)</p> <p> </p> <p>Test 05: Turtlebot4 Autonomy with MyCobot (Mobile Manipulator)</p> <p> </p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#final-project-demonstration","title":"\ud83d\udda5\ufe0f Final Project Demonstration \ud83d\udda5\ufe0f","text":"<p>A future test combining voice input, object detection, and autonomous navigation on real hardware.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#ros2-and-gui","title":"ROS2 and GUI","text":"<p>The graphical user interface (GUI) acts as a crucial feedback layer between perception, autonomy, and user interaction. It helps visualize how the robot interprets its environment, processes inputs, and executes decisions.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#41-gui-integration-overview","title":"4.1 GUI Integration Overview","text":"<p>The GUI provides live visualization of:</p> <ul> <li>Real-time video feed with bounding boxes from YOLOv8 detections</li> <li>Voice command recognition status and interpreted actions</li> <li>Robot's velocity, heading, and navigation path</li> <li>Sensor diagnostics such as LiDAR, IMU, and hazard detection</li> </ul> <p>ROS2 topics connected to the GUI include:</p> <ul> <li><code>/yolov8_detections</code> \u2013 Object detection overlays</li> <li><code>/voice_cmd</code> \u2013 Recognized voice command topics</li> <li><code>/cmd_vel</code> \u2013 Velocity control signals</li> <li><code>/rpi_13/hazard_detection</code> \u2013 Safety monitoring</li> <li><code>/rpi_13/imu</code> \u2013 IMU motion feedback</li> </ul> <p>RQT GRAPH:</p> <ol> <li>From Dummy nodes (expected rqt_graph):</li> </ol> <p></p> <p>Final RQT-GRAPH: </p> <p>FINAL RQT GRAPH WITH GUI</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#42-gui-layout-design-concepts","title":"4.2 GUI Layout &amp; Design Concepts","text":"<p>Initial Mockup using Inkscape </p> <p>Proposed Interactive GUI for ROS2 Integration </p> <p>Test 1: GUI Working on Real-time Data - Faced issue with camera (not fixed) - IMU - LiDAR</p> <ol> <li>GUI Updating IMU Data Live:</li> </ol> <p></p> <ol> <li>GUI Updating LiDAR Data Live:</li> </ol> <p></p> <p>3. GUI Updating Camera Data Live:</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#these-gui-designs-aim-to-present-important-real-time-system-data-in-an-intuitive-and-user-friendly-format-enabling-both-operator-awareness-and-effective-debugging","title":"These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#43-live-sensor-visualization-in-gui","title":"4.3 Live Sensor Visualization in GUI","text":"<p>The GUI aggregates multiple sensor streams into a unified dashboard. Key visualized components:</p> <ul> <li> <p>Object Detection Feed:   Annotated Oak-D camera frames showing bounding boxes and labels.</p> </li> <li> <p>Voice Command Status:   Display of the latest interpreted command and its mapped action.</p> </li> <li> <p>Navigation Planner View:   Real-time trajectory paths generated by the navigation stack.</p> </li> <li> <p>Sensor Diagnostics:   Live data from LiDAR, IMU, and hazard modules.</p> </li> </ul> <p>GUI With Live Sensor Data</p> <p></p> <p>GUI WITH ACTUAL DATA:</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#44-gui-demonstration-video","title":"4.4 GUI Demonstration Video","text":"<p>Test 01: Object Detection + GUI Overlay</p> <p> Real-time object detection using YOLOv8 with GUI overlay showing detected objects and confidence scores.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#additional-demonstration-videos_1","title":"Additional Demonstration Videos","text":"<p>Test 02: Controlling the Cobot Robotic Arm</p> <p> </p> <p>A short demo showcasing voice-command-based control of TurtleBot4 in a dynamic environment.</p> <p>Test 03: Full Autonomy Simulation (Without GUI)</p> <p> </p> <p>Test 04: Live Demonstration Test 2 (With GUI)</p> <p> </p> <p>Test 04: Turtlebot4 Autonomy with MyCobot (Mobile Manipulator)</p> <p> </p> <p>A future test combining voice input, object detection, and autonomous navigation on real hardware.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#45-control-architecture-in-ros2","title":"4.5 Control Architecture in ROS2","text":"<p>The following flowchart summarizes the control pipeline, organized by data type and decision layer:</p> <pre><code>%%{ init: { \"theme\": \"default\" } }%%\ngraph TD\n\n  %% Subgraphs\n  subgraph SENSORS [\"Sensor Inputs\"]\n    CAM[Oak-D Camera]\n    MIC[Microphone]\n    LIDAR[IR / LiDAR Sensors]\n  end\n\n  subgraph PERCEPTION [\"Perception Nodes\"]\n    YOLO_NODE[Node: yolov8_processor]\n    VOICE_NODE[Node: voice_input_node]\n    DETECTIONS[/yolov8_detections/]\n    VOICE_RAW[/voice_input/]\n    CAM --&gt; YOLO_NODE --&gt; DETECTIONS\n    MIC --&gt; VOICE_NODE --&gt; VOICE_RAW\n  end\n\n  subgraph AUTONOMY [\"High-Level Autonomy\"]\n    VOICE_PARSER[Node: voice_command_parser]\n    VOICE_CMD[/voice_cmd/]\n    DECISION[Node: decision_maker_node]\n    ACT_CMD[/action_cmd/]\n    VOICE_RAW --&gt; VOICE_PARSER --&gt; VOICE_CMD\n    DETECTIONS --&gt; DECISION\n    VOICE_CMD --&gt; DECISION --&gt; ACT_CMD\n  end\n\n  subgraph CONTROL [\"Low-Level Control\"]\n    COLLISION[Node: collision_avoidance_node]\n    NAV[Node: navigation_controller]\n    CMD[/cmd_vel/]\n    LIDAR --&gt; COLLISION\n    ACT_CMD --&gt; COLLISION --&gt; NAV --&gt; CMD\n  end\n\n  CMD --&gt; MOVE[TurtleBot Movement]\n\n  %% Color classes\n  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef perception fill:#e6e6fa,stroke:#7e57c2,color:#000\n  classDef autonomy fill:#ffe0e6,stroke:#e91e63,color:#000\n  classDef control fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef output fill:#eeeeee,stroke:#757575,color:#000\n\n  %% Assign classes\n  class CAM,MIC,LIDAR sensors\n  class YOLO_NODE,VOICE_NODE,DETECTIONS,VOICE_RAW perception\n  class VOICE_PARSER,VOICE_CMD,DECISION,ACT_CMD autonomy\n  class COLLISION,NAV,CMD control\n  class MOVE output\n</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#interaction-mechanism","title":"Interaction Mechanism","text":"<p>Behavioral Influence &amp; Interfaces</p> <ul> <li>Voice Command API: Users instruct the robot using predefined phrases.</li> <li>ROS2 RQT GUI/Web Dashboard: For remote monitoring.</li> </ul> <p>(Include a professional-looking UI sketch showing how users will interact with the system.)</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#preparation-needs","title":"Preparation Needs","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#required-knowledge-topics-for-success","title":"Required Knowledge &amp; Topics for Success","text":"<pre><code>graph TD\n  A[Preparation Needs]\n\n  A --&gt; B1[Object Detection]\n  A --&gt; B2[ROS2 Navigation]\n  A --&gt; B3[Speech Recognition]\n  A --&gt; B4[Hardware Control]\n\n  B1 --&gt; C1[YOLOv8]\n  B1 --&gt; C2[OpenCV]\n\n  B2 --&gt; C3[SLAM]\n  B2 --&gt; C4[Collision Avoidance]\n\n  B3 --&gt; C5[Microphone Input]\n  B3 --&gt; C6[Command Parsing]\n\n  B4 --&gt; C7[PC-Robot Communication]\n  B4 --&gt; C8[Predefined Actions]\n\n  %% Color styling\n  classDef detect fill:#ffe0e6,stroke:#e91e63,color:#000\n  classDef nav fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef speech fill:#e6e6fa,stroke:#7e57c2,color:#000\n  classDef hardware fill:#f9fbe7,stroke:#c0ca33,color:#000\n  classDef root fill:#f5f5f5,stroke:#9e9e9e,color:#000\n\n  class A root\n  class B1,C1,C2 detect\n  class B2,C3,C4 nav\n  class B3,C5,C6 speech\n  class B4,C7,C8 hardware\n\n</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-workflow","title":"Project Workflow","text":"<p>The system operates by capturing data from multiple sensors, interpreting user commands, and performing autonomous navigation and feedback. The diagram below outlines the overall data and control flow of the TurtleBot4 system.</p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui","title":"<pre><code>\n    graph TD\n      A[\"Start:&lt;br/&gt;TurtleBot4 Powered On\"]\n      B[\"Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic\"]\n      C[\"Sensor Data&lt;br/&gt;Preprocessing\"]\n      D1[\"YOLOv8&lt;br/&gt;Object Detection\"]\n      D2[\"Voice Command&lt;br/&gt;Recognition\"]\n      E[\"Detected Object Info\"]\n      F[\"Intent or Goal Command\"]\n      G[\"Decision-Making Node\"]\n      H[\"ROS2 Navigation Stack\"]\n      I[\"Movement Commands&lt;br/&gt;via /cmd_vel\"]\n      J[\"GUI Update:&lt;br/&gt;Object and Nav Info\"]\n      K[\"Actuator Response:&lt;br/&gt;TurtleBot Moves\"]\n      L[\"User Feedback:&lt;br/&gt;GUI Visualization\"]\n      M[\"End\"]\n\n      A --&gt; B\n      B --&gt; C\n      C --&gt; D1\n      C --&gt; D2\n      D1 --&gt; E\n      D2 --&gt; F\n      E --&gt; G\n      F --&gt; G\n      G --&gt; H\n      H --&gt; I\n      I --&gt; K\n      K --&gt; M\n      G --&gt; J\n      J --&gt; L\n      L --&gt; M\n\n      classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000\n      classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000\n      classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000\n      classDef decision fill:#fce4ec,stroke:#ec407a,color:#000\n      classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000\n      classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000\n\n      class A,M startend\n      class B sensing\n      class C,D1,D2,E,F processing\n      class G decision\n      class H,I,K navctrl\n      class J,L gui\n</code></pre>","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#final-demonstration-plan","title":"Final Demonstration Plan","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#setup-execution","title":"Setup &amp; Execution","text":"<ul> <li>Classroom Resources: Open space for navigation demo.</li> <li>Demonstration Steps: The robot will identify objects, respond to user queries, navigate obstacles, and execute spoken commands.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#handling-environmental-variability-future-scope","title":"Handling Environmental Variability (future scope)","text":"<ul> <li>Adaptive Algorithms: Adjust object detection thresholds dynamically.</li> <li>Fallback Modes: If detection fails, switch to manual control or alternative recognition models.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#testing-evaluation-plan","title":"Testing &amp; Evaluation Plan","text":"<ul> <li>Simulated Testing in Gazebo before real-world deployment.</li> <li>Comparison Metrics:</li> <li>Object recognition accuracy.</li> <li>Speech command response time.</li> <li>Navigation success rate.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#impact","title":"Impact","text":"<p>This project will:</p> <ul> <li>Advance AI-driven robotics interactions for smart environments.</li> <li>Develop speech-integrated autonomous systems.</li> <li>Provide students hands-on experience with ROS2, AI, and embedded systems.</li> <li>Potentially contribute to assistive robotics research.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#future-scope","title":"Future Scope:","text":"<p>In future iterations, the system can be extended with:</p> <ul> <li>Robotic arm integration mounted on TurtleBot4 for executing pick-and-place actions based on detected objects.</li> <li>Task-oriented planning using semantic understanding of scenes (e.g., pick red object and place it near the wall).</li> </ul> <p></p> <p></p> <p>Here is the generated image of a cobot mounted on a TurtleBot4 using CHATGPT 4o</p> <p>FINAL GUI :</p> <p></p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#references-subject-to-change","title":"References (Subject to change):","text":"<ol> <li>Deep Learning model options: https://yolov8.com/</li> <li>YOLOv8 example: https://rs-punia.medium.com/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956</li> <li>Speech Recognition Libraries: https://pypi.org/project/SpeechRecognition/</li> <li>Turtlebot4 Mapping Resource: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/generate_map.html</li> <li>Mapping, Localizing, Path planning packages for Turtlebot4: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/turtlebot4_navigator.html</li> </ol>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#advising-resources","title":"Advising &amp; Resources","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#project-advisor","title":"Project Advisor","text":"<ul> <li>Dr. Daniel Aukes </li> <li>Resource Needs: Hardware support, mentorship on TurtleBot4 Hardware integration with ROS2.</li> </ul>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#weekly-milestones-weeks-7-16","title":"Weekly Milestones (Weeks 7-16)","text":"","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#weekly-milestones-weeks-716","title":"Weekly Milestones (Weeks 7\u201316)","text":"Week Date Milestone Status Week 7 Feb 24, 2025 Finalizing project scope and hardware/sensor availability. \u2705 Completed Week 8 Mar 3, 2025 ROS2 environment setup, VM configuration, TurtleBot4 base initialization. \u2705 Completed Week 9 Mar 10, 2025 Object detection with YOLOv8 using Oak-D camera. \u2705 Completed Week 10 Mar 17, 2025 Voice command parsing, audio processing and integration. \u2705 Completed Week 11 Mar 24, 2025 GUI development and voice-based system control. \u2705 Completed Week 12 Mar 31, 2025 ROS2 node integration and layered autonomy testing. \u2705 Completed Week 13 Apr 7, 2025 TurtleBot testing with full pipeline and live demos. \u2705 Completed Week 14 Apr 14, 2025 Final debugging, fallback strategies, and performance evaluation. \ud83d\udd04 In Progress Week 15 Apr 21, 2025 Documentation, GUI improvements, and final video preparation. \ud83d\udd04 In Progress Week 16 Apr 28, 2025 \ud83d\ude80 Final Demonstration &amp; Submission \ud83d\udd04 In Progress","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#gantt-chart-representation","title":"Gantt Chart Representation","text":"<pre><code>gantt\n    title Project Timeline (Weeks 7\u201316)\n    dateFormat  YYYY-MM-DD\n    axisFormat  %b %d\n\n    section Planning\n    Finalize Scope            :active,milestone1, 2025-02-24, 7d\n\n    section Implementation\n    ROS2 Setup                :active,milestone2, 2025-03-03, 7d\n    GUI Development           :active,milestone3, 2025-03-10, 7d\n    YOLOv8 + Oak-D            :active,milestone4, 2025-03-17, 7d\n    Voice Command             :crit,milestone5, 2025-03-24, 7d\n    ROS2 Integration          :crit,milestone6, 2025-03-31, 10d\n\n    section Testing &amp; Deployment\n    TurtleBot Testing         :crit,milestone7, 2025-04-10, 7d\n    Final Debug               :milestone8, 2025-04-17, 5d\n    Docs &amp; Video              :milestone9, 2025-04-22, 5d\n    Final Demo                :milestone10, 2025-04-28, 6d</code></pre>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"#innovation-showcase-at-arizona-state-university-spring-2025","title":"\ud83c\udf1f Innovation Showcase at Arizona State University | Spring 2025 \ud83c\udf1f","text":"<p>Watch on YouTube:</p> <p> </p> <p>We presented \"Intelligent Voice-Guided Mobile Manipulator: Real-Time Object Detection and Autonomous Navigation Using TurtleBot4 and MyCobot Robot Arm\" at the Innovation Showcase hosted at ASU - The Polytechnic School!  This was our final project for the course RAS 598: Experimentation and Deployment of Robots, led by Professor Dan Aukes.</p> <p>\ud83d\udca1Our project featured a live demo and poster presentation of an autonomous, voice-guided robot that combines real-time speech interaction, object detection, and navigation \u2014 built on TurtleBot4 and enhanced with a MyCobot arm. </p>","tags":["Robotics","AI","Object Detection","Speech Recognition"]},{"location":"charts/","title":"Charts","text":""},{"location":"charts/#project-visual-overview","title":"Project Visual Overview","text":"Section Chart Title Description Timeline Gantt Chart Shows weekly project phases from planning to deployment. Workflow Project Workflow Chart Displays the full robot pipeline from sensor data to GUI feedback. System Control System Architecture + ROS2 Nodes &amp; Topics Visualizes ROS2 nodes and topics, including collision avoidance logic. Autonomy Layers High-Level vs Low-Level Control Flow Separates voice-guided task selection from low-level obstacle avoidance with aligned ROS2 nodes and data flow. Future Work Cobot Arm Integration Flow Outlines the proposed pick-and-place functionality using a mounted arm."},{"location":"charts/#gantt-chart-project-timeline-overview","title":"Gantt Chart \u2013 Project Timeline Overview","text":"<pre><code>gantt\n    title Project Timeline (Weeks 7\u201316)\n    dateFormat  YYYY-MM-DD\n    axisFormat  %b %d\n\n    section Planning\n    Finalize Scope            :active,milestone1, 2025-02-24, 7d\n\n    section Implementation\n    ROS2 Setup                :active,milestone2, 2025-03-03, 7d\n    GUI Development           :active,milestone3, 2025-03-10, 7d\n    YOLOv8 + Oak-D            :active,milestone4, 2025-03-17, 7d\n    Voice Command             :active,milestone5, 2025-03-24, 7d\n    ROS2 Integration          :active,milestone6, 2025-03-31, 10d\n\n    section Testing &amp; Deployment\n    TurtleBot Testing         :active,milestone7, 2025-04-10, 7d\n    Final Debug               :active,milestone8, 2025-04-17, 5d\n    Docs &amp; Video              :active,milestone9, 2025-04-22, 5d\n    Final Demo                :active,milestone10, 2025-04-28, 6d</code></pre>"},{"location":"charts/#main-project-pipeline","title":"Main Project Pipeline","text":"<ul> <li>The flowchart below represents the overall working of our Intelligent TurtleBot4 system. </li> <li>It starts with sensor data collection from the Oak-D camera, IMU, LiDAR, and microphone. </li> <li> <p>The data is then preprocessed and sent to two parallel modules: YOLOv8 for object detection and voice recognition for interpreting commands.</p> </li> <li> <p>Outputs from both modules are fed into a decision-making node that determines the robot's next action. </p> </li> <li>The chosen action is executed via the ROS2 navigation stack and published as movement commands. </li> <li>Simultaneously, the GUI updates with relevant feedback, allowing users to visualize object info and robot behavior in real time.</li> </ul>"},{"location":"charts/#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-c-c-d1-e-c-d2-f-e-g-f-g-g-h-i-k-m-g-j-l-m-classdef-sensors-filld0f0efstroke0097a7color000-classdef-perception-fille8eaf6stroke5c6bc0color000-classdef-decision-fillffe0e6stroked81b60color000-classdef-control-fillfff9c4strokefbc02dcolor000-classdef-gui-fillede7f6stroke7e57c2color000-classdef-startend-filleeeeeestroke757575color000-class-am-startend-class-b-sensors-class-cd1d2ef-perception-class-g-decision-class-hik-control-class-jl-gui","title":"<pre><code>graph TD\n  A[\"Start:&lt;br/&gt;TurtleBot4 Powered On\"]\n  B[\"Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic\"]\n  C[\"Sensor Data&lt;br/&gt;Preprocessing\"]\n  D1[\"YOLOv8&lt;br/&gt;Object Detection\"]\n  D2[\"Voice Command&lt;br/&gt;Recognition\"]\n  E[\"Detected Object Info\"]\n  F[\"Intent or Goal Command\"]\n  G[\"Decision-Making Node\"]\n  H[\"ROS2 Navigation Stack\"]\n  I[\"Movement Commands&lt;br/&gt;via /cmd_vel\"]\n  J[\"GUI Update:&lt;br/&gt;Object and Nav Info\"]\n  K[\"Actuator Response:&lt;br/&gt;TurtleBot Moves\"]\n  L[\"User Feedback:&lt;br/&gt;GUI Visualization\"]\n  M[\"End\"]\n\n  A --&gt; B --&gt; C\n  C --&gt; D1 --&gt; E\n  C --&gt; D2 --&gt; F\n  E --&gt; G\n  F --&gt; G\n  G --&gt; H --&gt; I --&gt; K --&gt; M\n  G --&gt; J --&gt; L --&gt; M\n\n  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000\n  classDef perception fill:#e8eaf6,stroke:#5c6bc0,color:#000\n  classDef decision fill:#ffe0e6,stroke:#d81b60,color:#000\n  classDef control fill:#fff9c4,stroke:#fbc02d,color:#000\n  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000\n  classDef startend fill:#eeeeee,stroke:#757575,color:#000\n\n  class A,M startend\n  class B sensors\n  class C,D1,D2,E,F perception\n  class G decision\n  class H,I,K control\n  class J,L gui\n</code></pre>","text":""},{"location":"charts/#system-control-and-autonomy-flow","title":"System Control and Autonomy Flow","text":"<ul> <li>The diagram below presents the complete decision and control loop of our TurtleBot4 system.</li> <li>From user voice inputs and real-time camera feeds, data is collected, processed, and passed through an autonomy layer to issue movement commands.</li> <li>Each subsystem (sensor, processing, autonomy) is driven by dedicated ROS2 nodes communicating through standard topics.</li> </ul>"},{"location":"charts/#graph-td-top-level-flow-aturtlebot4-booted-bsensor-data-collection-a-b-sensor-layer-subgraph-sensor_layer-ros2-sensor-layer-n1node-oakd_camera_node-t1rpi_13oakdpreviewimage_raw-n2node-voice_input_node-t2voice_input-end-processing-layer-subgraph-processing_layer-ros2-perception-command-parsing-n3node-yolov8_processor-t3yolov8_detections-n4node-voice_command_parser-t4voice_cmd-end-autonomy-layer-subgraph-autonomy_layer-ros2-autonomy-control-n5node-decision_maker_node-n6node-collision_avoidance_node-n7node-navigation_controller-t5action_cmd-t6cmd_vel-n5-t5-n5-n6-n7-t6-end-flow-between-subgraphs-b-sensor_layer-processing_layer-autonomy_layer-cturtlebot4-output-autonomy_layer-c","title":"<pre><code>graph TD\n\n  %% Top-Level Flow\n  A[TurtleBot4 Booted]\n  B[Sensor Data Collection]\n\n  A --&gt; B\n\n  %% Sensor Layer\n  subgraph Sensor_Layer [ROS2: Sensor Layer]\n    N1[Node: oakd_camera_node] --&gt; T1[/rpi_13/oakd/preview/image_raw/]\n    N2[Node: voice_input_node] --&gt; T2[/voice_input/]\n  end\n\n  %% Processing Layer\n  subgraph Processing_Layer [ROS2: Perception &amp; Command Parsing]\n    N3[Node: yolov8_processor] --&gt; T3[/yolov8_detections/]\n    N4[Node: voice_command_parser] --&gt; T4[/voice_cmd/]\n  end\n\n  %% Autonomy Layer\n  subgraph Autonomy_Layer [ROS2: Autonomy &amp; Control]\n    N5[Node: decision_maker_node]\n    N6[Node: collision_avoidance_node]\n    N7[Node: navigation_controller]\n    T5[/action_cmd/]\n    T6[/cmd_vel/]\n    N5 --&gt; T5\n    N5 --&gt; N6 --&gt; N7 --&gt; T6\n  end\n\n  %% Flow between subgraphs\n  B --&gt; Sensor_Layer --&gt; Processing_Layer --&gt; Autonomy_Layer\n  C[Turtlebot4 Output]\n  Autonomy_Layer --&gt; C\n</code></pre>","text":""},{"location":"charts/#hybrid-ros2-system-architecture","title":"Hybrid ROS2 System Architecture","text":"<ul> <li>This diagram separates high-level autonomy (top row) from low-level motion control (bottom row), with clearly labeled ROS2 nodes and topic communication.</li> <li>It maintains modular blocks for perception, decision-making, and actuation while improving visual alignment and clarity.</li> </ul>"},{"location":"charts/#graph-td-sensors-subgraph-sensors-sensor-inputs-camoak-d-camera-micmicrophone-lidarir-lidar-sensors-end-perception-layer-subgraph-perception-perception-nodes-yolonode-yolov8_processor-voice_rawnode-voice_input_node-yolo_outyolov8_detections-voice_invoice_input-cam-yolo-yolo_out-mic-voice_raw-voice_in-end-high-level-autonomy-subgraph-high_auto-high-level-autonomy-voice_parsernode-voice_command_parser-voice_cmdvoice_cmd-decidenode-decision_maker_node-act_cmdaction_cmd-voice_in-voice_parser-voice_cmd-yolo_out-decide-voice_cmd-decide-act_cmd-end-low-level-autonomy-subgraph-low_auto-low-level-control-coll_avoidnode-collision_avoidance_node-nav_ctrlnode-navigation_controller-cmd_velcmd_vel-lidar-coll_avoid-act_cmd-coll_avoid-nav_ctrl-cmd_vel-end-actuation-cmd_vel-tbotturtlebot-movement-block-to-block-flow-sensors-perception-high_auto-low_auto-tbot","title":"<pre><code>graph TD\n\n  %% Sensors\n  subgraph SENSORS [Sensor Inputs]\n    CAM[Oak-D Camera]\n    MIC[Microphone]\n    LIDAR[IR / LiDAR Sensors]\n  end\n\n  %% Perception Layer\n  subgraph PERCEPTION [Perception Nodes]\n    YOLO[Node: yolov8_processor]\n    VOICE_RAW[Node: voice_input_node]\n    YOLO_OUT[/yolov8_detections/]\n    VOICE_IN[/voice_input/]\n    CAM --&gt; YOLO --&gt; YOLO_OUT\n    MIC --&gt; VOICE_RAW --&gt; VOICE_IN\n  end\n\n  %% High-Level Autonomy\n  subgraph HIGH_AUTO [High-Level Autonomy]\n    VOICE_PARSER[Node: voice_command_parser]\n    VOICE_CMD[/voice_cmd/]\n    DECIDE[Node: decision_maker_node]\n    ACT_CMD[/action_cmd/]\n    VOICE_IN --&gt; VOICE_PARSER --&gt; VOICE_CMD\n    YOLO_OUT --&gt; DECIDE\n    VOICE_CMD --&gt; DECIDE --&gt; ACT_CMD\n  end\n\n  %% Low-Level Autonomy\n  subgraph LOW_AUTO [Low-Level Control]\n    COLL_AVOID[Node: collision_avoidance_node]\n    NAV_CTRL[Node: navigation_controller]\n    CMD_VEL[/cmd_vel/]\n    LIDAR --&gt; COLL_AVOID\n    ACT_CMD --&gt; COLL_AVOID --&gt; NAV_CTRL --&gt; CMD_VEL\n  end\n\n  %% Actuation\n  CMD_VEL --&gt; TBOT[TurtleBot Movement]\n\n  %% Block-to-block flow\n  SENSORS --&gt; PERCEPTION --&gt; HIGH_AUTO --&gt; LOW_AUTO --&gt; TBOT</code></pre>","text":""},{"location":"charts/#future-work-concept-turtlebot4-with-mounted-cobot-arm","title":"Future Work Concept: TurtleBot4 with Mounted Cobot Arm","text":"<p>This future work visual outlines the integration of a robotic arm on TurtleBot4.  The system uses object detection and coordinates from the perception pipeline to compute inverse kinematics and execute pick-and-place actions via a dedicated ROS2 control node.</p> <p>Goals to Capture Visually: - Addition of a robotic arm mounted on TurtleBot4 - Use of ROS2 for communication with the arm - Performing pick-and-place tasks - Integration with existing perception (e.g., YOLOv8 object detection for picking targets)</p> <pre><code>graph TD\n\n  %% Object Detection &amp; Localization\n  V1[\"Object Detection&lt;br/&gt;using YOLOv8\"]\n  V2[\"Target Object&lt;br/&gt;Coordinates\"]\n\n  %% Motion Planning\n  M1[\"Inverse Kinematics&lt;br/&gt;&amp; Arm Planning\"]\n\n  %% Platform Base\n  P2[\"TurtleBot4 Base Platform\"]\n  P1[\"Mounted Cobot Arm&lt;br/&gt;on Platform\"]\n\n  %% ROS2 Control\n  N1[\"ROS2 Arm&lt;br/&gt;Control Node\"]\n\n  %% Execution\n  E1[\"Pick and Place&lt;br/&gt;Execution\"]\n  E2[\"Object&lt;br/&gt;Grasped and Placed\"]\n\n  %% Connections\n  V1 --&gt; V2 --&gt; M1 --&gt; N1\n  P1 --&gt; P2 --&gt; N1\n  N1 --&gt; E1 --&gt; E2</code></pre>"},{"location":"esp-32-table/","title":"Sensors Table","text":""},{"location":"esp-32-table/#turtlebot-4-sensor-table","title":"TurtleBot 4 Sensor Table","text":"Sensor Implementation Image Depth Camera- OAK-D-Pro) Object detection and distance estimation LiDAR- RPLIDAR A1M8 SLAM-based navigation and obstacle detection IMU Enhancing motion stability and drift correction Microphone Capturing voice commands Speaker Responding with audio feedback on Host PC"},{"location":"obj_detect/","title":"Object Detection Page","text":""},{"location":"obj_detect/#object-detection-with-yolov8","title":"Object Detection with YOLOv8 \ud83d\udd0e","text":"<p>We implemented object detection using the YOLOv8 Nano model by Ultralytics, selected for its balance of speed and accuracy on resource-constrained hardware like the Raspberry Pi. This model performs real-time object detection from the Oak-D camera feed and publishes annotated frames along with recognized object labels.</p> <p></p> <p>The detection results are displayed live on our custom GUI, and the recognized labels are also logged and published to a ROS 2 topic for downstream decision-making. This enables both GUI-based visualization and voice-interactive robot behavior based on perceived objects.</p> <p>Some Object Detection Results:</p> <p></p>"},{"location":"pic-table/","title":"TurtleBot4 Hardware","text":"Feature TurtleBot 4 Lite TurtleBot 4 Size (L x W x H) 342 x 339 x 192 mm 342 x 339 x 351 mm Weight 3270 g 3945 g Base platform iRobot\u00ae Create\u00ae 3 iRobot\u00ae Create\u00ae 3 Wheels (Diameter) 72 mm 72 mm Ground Clearance 4.5 mm 4.5 mm On-board Computer Raspberry Pi 4B 4GB Raspberry Pi 4B 4GB Maximum linear velocity 0.31 m/s (safe mode), 0.46 m/s (no safe mode) 0.31 m/s (safe mode), 0.46 m/s (no safe mode) Maximum angular velocity 1.90 rad/s 1.90 rad/s Maximum payload 9 kg 9 kg Operation time 2h 30m - 4h (depending on load) 2h 30m - 4h (depending on load) Charging time 2h 30m 2h 30m Bluetooth Controller Not Included TurtleBot 4 Controller Lidar RPLIDAR A1M8 RPLIDAR A1M8 Camera OAK-D-Lite OAK-D-Pro User Power VBAT @1.9A  5V @ Low current  3.3V @ Low current VBAT @ 300 mA  12V @ 300 mA  5V @ 500 mA  3.3V @ 250 mA USB Expansion USB 2.0 (Type A) x2  USB 3.0 (Type A) x2 USB 2.0 (Type A) x2  USB 3.0 (Type A) x1  USB 3.0 (Type C) x4 Programmable LEDs Create\u00ae 3 Lightring Create\u00ae 3 Lightring  User LED x2 Status LEDs - Power LED  Motors LED  WiFi LED  Comms LED  Battery LED Buttons and Switches Create\u00ae 3 User buttons x2  Create\u00ae 3 Power Button x1 Create\u00ae 3 User buttons x2  Create\u00ae 3 Power Button x1  User Buttons x4 Battery 26 Wh Lithium Ion (14.4V nominal) 26 Wh Lithium Ion (14.4V nominal) Charging Dock Included Included"},{"location":"source_code/","title":"Backend Framework Overview","text":"<p>This project features a modular, ROS 2-based architecture that turns the TurtleBot4 + MyCobot arm into an intelligent mobile manipulator. The system supports real-time sensor processing, voice-guided navigation, object detection, and GUI-based interaction using custom ROS 2 nodes.</p> <p>Key functionalities include:</p> <ul> <li>\ud83c\udfaf YOLOv8-based object detection</li> <li>\ud83c\udfa4 Whisper.cpp-based speech transcription</li> <li>\ud83d\udce1 LiDAR-based environment sensing</li> <li>\ud83d\udcc8 IMU-based robot state estimation</li> <li>\ud83d\udda5\ufe0f Custom-built PyQt5 GUI to display real-time data</li> <li>\ud83e\udd16 Integration-ready support for MyCobot arm manipulation</li> </ul>"},{"location":"source_code/#ros-2-nodes-their-responsibilities","title":"ROS 2 Nodes &amp; Their Responsibilities","text":"<ol> <li> <p><code>mic_listener_node</code> Records a short 4-second audio clip every few seconds and uses Whisper.cpp for on-device transcription. Publishes transcribed text to <code>/voice_text</code>.</p> </li> <li> <p><code>command_parser_node</code> Parses the text received from <code>/voice_text</code> and matches it against a list of valid commands. Converts recognized speech to structured instructions published on <code>/voice_command</code>.</p> </li> <li> <p><code>movement_controller_node</code> Executes motion commands based on <code>/voice_command</code>, translating them into <code>geometry_msgs/Twist</code> messages sent to <code>/cmd_vel</code>.</p> </li> <li> <p><code>object_detector_node</code> Subscribes to camera feed (<code>/oakd/rgb/preview/image_raw</code>), runs YOLOv8 in real time, and publishes an annotated image to <code>/yolo_image_raw</code> and a string summary to <code>/detected_objects</code>.</p> </li> <li> <p><code>web_dashboard_node</code>     PyQt5 GUI-based node that visualizes:</p> <ul> <li>Camera feed</li> <li>Object detection results</li> <li>IMU acceleration and angular velocity plots</li> <li>LiDAR scan as a radar map</li> <li>Voice text, command log, and detected objects in live text feeds</li> </ul> </li> </ol>"},{"location":"source_code/#topics-overview","title":"\ud83d\udd01 Topics Overview","text":"Topic Type Description <code>/voice_text</code> <code>std_msgs/String</code> Transcribed text from mic <code>/voice_command</code> <code>std_msgs/String</code> Parsed structured command <code>/cmd_vel</code> <code>geometry_msgs/Twist</code> Velocity command to TurtleBot <code>/oakd/rgb/preview/image_raw</code> <code>sensor_msgs/Image</code> RGB camera feed <code>/yolo_image_raw</code> <code>sensor_msgs/Image</code> Annotated YOLO output image <code>/detected_objects</code> <code>std_msgs/String</code> Summary of objects seen <code>/scan</code> <code>sensor_msgs/LaserScan</code> LiDAR data used for obstacle proximity map <code>/rpi_13/imu</code> <code>sensor_msgs/Imu</code> IMU data for linear/angular tracking <code>/rpi_13/dock_status</code> <code>irobot_create_msgs/DockStatus</code> Docking state <code>/rpi_13/battery_state</code> <code>sensor_msgs/BatteryState</code> Live battery level info"},{"location":"source_code/#launch-file","title":"Launch File","text":"<p>All nodes are launched via the voice control launch file:</p> <pre><code># voice_control.launch.py\n\n- mic_listener_node\n- command_parser_node\n- movement_controller_node\n- object_detector_node\n- web_dashboard_node\n</code></pre> <p>To run:</p> <pre><code>ros2 launch voice_controlled_turtlebot voice_control.launch.py\n</code></pre>"},{"location":"source_code/#gui-architecture","title":"\ud83c\udf10 GUI Architecture","text":"<p>The <code>web_dashboard_node</code> is a custom PyQt5 GUI that displays:</p> <ul> <li>Three camera areas (camera feed, YOLO output, and placeholder)</li> <li>Voice transcription, command parser log, and live object detection string</li> <li>LiDAR radar-style scan with red dots and TurtleBot center in green</li> <li>IMU plots for linear (X/Y/Z) and angular (X/Y/Z) data over time</li> <li>Docking state and battery level (subscribed internally, display optional)</li> </ul>"},{"location":"source_code/#ros-2-data-flow-diagram-mermaid","title":"\ud83d\udd04 ROS 2 Data Flow Diagram (Mermaid)","text":"<pre><code>graph TD\n    MicListener --&gt;|/voice_text| CommandParser\n    CommandParser --&gt;|/voice_command| MovementController\n    MovementController --&gt;|/cmd_vel| TurtleBot\n\n    CameraFeed --&gt;|/oakd/rgb/preview/image_raw| ObjectDetector\n    ObjectDetector --&gt;|/yolo_image_raw| DashboardGUI\n    ObjectDetector --&gt;|/detected_objects| DashboardGUI\n\n    MicListener --&gt;|/voice_text| DashboardGUI\n    CommandParser --&gt;|/voice_command| DashboardGUI\n\n    IMUData --&gt;|/rpi_13/imu| DashboardGUI\n    LiDARData --&gt;|/scan| DashboardGUI\n    BatteryState --&gt;|/rpi_13/battery_state| DashboardGUI\n    DockStatus --&gt;|/rpi_13/dock_status| DashboardGUI</code></pre>"},{"location":"source_code/#source-code-installation","title":"\ud83d\udcc1 Source Code &amp; Installation","text":"<p>The full codebase is available at: \u27a1\ufe0f GitHub: voice_controlled_turtlebot</p> <p>To install and build:</p> <pre><code>cd ~/anu_ws/src\ngit clone https://github.com/voice_controlled_turtlebot.git\ncd ..\ncolcon build\nsource install/setup.bash\n</code></pre> <p>To Run: <pre><code>ros2 launch voice_controlled_turtlebot voice_control.launch.py\n</code></pre></p>"},{"location":"source_code/#external-tools-used","title":"\ud83d\udce6 External Tools Used","text":"Tool Description Version / Link YOLOv8 Real-time object detection Ultralytics v8 Whisper.cpp Lightweight CPU-based speech-to-text engine whisper.cpp PyQt5 GUI framework for Python PyQt5 (pip) OpenCV Image processing &amp; visualization OpenCV 4.x <p>Let me know if you\u2019d like this formatted as a new .md page or inserted into the main README!</p>"},{"location":"videos/","title":"Intelligent TurtleBot4 + MyCobot: Voice-Guided Mobile Manipulator Showcase \ud83c\udfac","text":"<p>This project began as an experiment in voice-guided navigation, but rapidly evolved into a full-stack robotics platform integrating vision, voice, LiDAR, and robotic manipulation.  Our system transformed from a basic ROS2-controlled TurtleBot4 into a multi-modal mobile manipulator equipped with the MyCobot arm, YOLOv8 object detection, Whisper-based voice transcription, and a custom-built GUI for real-time interaction and debugging.</p> <p>Below is a curated collection of short demonstration videos that highlight key milestones from early testing to full-system deployment.</p>"},{"location":"videos/#test-01-object-detection-gui-overlay","title":"Test 01: Object Detection + GUI Overlay \ud83c\udfaf","text":"<p> Real-time object detection using YOLOv8, with bounding boxes and confidence scores visualized on our PyQt5 GUI.</p>"},{"location":"videos/#test-02-controlling-the-cobot-robotic-arm","title":"Test 02: Controlling the Cobot Robotic Arm \ud83e\udd16","text":"<p> A demonstration of precise control of the MyCobot arm. Commands were sent from the TurtleBot4\u2019s voice pipeline using ROS2 topics.</p>"},{"location":"videos/#test-03-full-autonomy-simulation-without-gui","title":"Test 03: Full Autonomy Simulation (Without GUI) \ud83d\udea6","text":"<p> A raw test of autonomous task execution where the system responded to voice commands and object detection in terminal-based feedback mode.</p>"},{"location":"videos/#test-04-live-demonstration-with-gui","title":"Test 04: Live Demonstration with GUI \ud83e\uddea","text":"<p> The GUI-enhanced version of our autonomy system in action \u2014 showcasing LiDAR scan mapping, IMU plots, voice interactions, and YOLOv8 detection feedback.</p>"},{"location":"videos/#test-05-turtlebot4-mycobot-integration-mobile-manipulator","title":"Test 05: TurtleBot4 + MyCobot Integration (Mobile Manipulator) \ud83e\uddbe","text":"<p> The MyCobot arm is fully mounted and functional \u2014 executing joint movements with our inverse kinematics solver. This video captures the precision and coordination between the base and manipulator.</p>"},{"location":"videos/#final-project-demonstration","title":"Final Project Demonstration \ud83c\udfc1","text":"<p>A polished overview of our complete system: voice-activated navigation, camera vision, LiDAR awareness, and robotic arm manipulation \u2014 all brought together in a seamless ROS2 framework.</p> <p>\ud83d\udccc All videos are hosted on YouTube and embedded here for easy access. For technical deep-dives, check out our Code Walkthrough and ROS Architecture pages.</p>"},{"location":"voice/","title":"Voice Control","text":"<p>We implemented speech recognition on the TurtleBot4 system using Whisper.cpp \u2013 a lightweight, high-performance C++ implementation of OpenAI\u2019s Whisper model.  It enables voice-to-text transcription directly on-device, eliminating the need for cloud services and internet connectivity.</p> <p></p>"},{"location":"voice/#how-it-works","title":"How It Works \ud83d\udcac","text":"<ul> <li>The microphone on the host computer or external USB mic records a short audio clip (4 seconds).</li> <li>The audio is saved temporarily as a <code>.wav</code> file.</li> <li>Whisper.cpp processes this audio file using the <code>base.en</code> model to generate a transcription.</li> <li>The transcribed text is published to the ROS 2 topic <code>/voice_text</code>.</li> </ul> <p>This node runs periodically and enables seamless integration of voice control into our ROS2 pipeline.</p>"},{"location":"voice/#tradeoffs","title":"\ud83d\udee0\ufe0f Tradeoffs","text":"<ul> <li>We initially tried online Whisper APIs, but due to computational load and latency, we switched to Whisper.cpp for real-time inference.</li> <li>Tiny model was fast but inaccurate for short commands. We upgraded to the base model for better transcription at the cost of minor latency.</li> </ul>"},{"location":"voice/#ros-2-topics-used","title":"ROS 2 Topics Used","text":"<ul> <li>Subscribed: None</li> <li>Published: <code>/voice_text</code> (std_msgs/msg/String)</li> </ul>"},{"location":"component-selection-example/","title":"Component Selection Example","text":""},{"location":"component-selection-example/#examples","title":"Examples","text":""},{"location":"component-selection-example/#style-1","title":"Style 1","text":"<p>This is the example found in the assignment, uses more html</p> <p>Table 1: Example component selection</p> <p>External Clock Module</p> Solution Pros Cons Option 1. XC1259TR-ND surface mount crystal$1/eachlink to product * Inexpensive[^1]* Compatible with PSoC* Meets surface mount constraint of project * Requires external components and support circuitry for interface* Needs special PCB layout. * Option 2. * CTX936TR-ND surface mount oscillator * $1/each * Link to product * Outputs a square wave * Stable over operating temperature  * Direct interface with PSoC (no external circuitry required) range * More expensive * Slow shipping speed <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"component-selection-example/#style-2","title":"Style 2","text":"<p>Also acceptable, more markdown friendly</p> <p>External Clock Module</p> <ol> <li> <p>XC1259TR-ND surface mount crystal</p> <p></p> <ul> <li>$1/each</li> <li>link to product</li> </ul> Pros Cons Inexpensive Requires external components and support circuitry for interface Compatible with PSoC Needs special PCB layout. Meets surface mount constraint of project </li> <li> <p>CTX936TR-ND surface mount oscillator</p> <p></p> <ul> <li>$1/each</li> <li>Link to product</li> </ul> Pros Cons Outputs a square wave More expensive Stable over operating temperature Slow shipping speed Direct interface with PSoC (no external circuitry required) range </li> </ol> <p>Choice: Option 2: CTX936TR-ND surface mount oscillator</p> <p>Rationale: A clock oscillator is easier to work with because it requires no external circuitry in order to interface with the PSoC. This is particularly important because we are not sure of the electrical characteristics of the PCB, which could affect the oscillation of a crystal. While the shipping speed is slow, according to the website if we order this week it will arrive within 3 weeks.</p>"},{"location":"static/node_modules/mathjax/","title":"MathJax","text":""},{"location":"static/node_modules/mathjax/#beautiful-math-in-all-browsers","title":"Beautiful math in all browsers","text":"<p>MathJax is an open-source JavaScript display engine for LaTeX, MathML, and AsciiMath notation that works in all modern browsers.  It was designed with the goal of consolidating the recent advances in web technologies into a single, definitive, math-on-the-web platform supporting the major browsers and operating systems.  It requires no setup on the part of the user (no plugins to download or software to install), so the page author can write web documents that include mathematics and be confident that users will be able to view it naturally and easily.  Simply include MathJax and some mathematics in a web page, and MathJax does the rest.</p> <p>Some of the main features of MathJax include:</p> <ul> <li> <p>High-quality display of LaTeX, MathML, and AsciiMath notation in HTML pages</p> </li> <li> <p>Supported in most browsers with no plug-ins, extra fonts, or special   setup for the reader</p> </li> <li> <p>Easy for authors, flexible for publishers, extensible for developers</p> </li> <li> <p>Supports math accessibility, cut-and-paste interoperability, and other   advanced functionality</p> </li> <li> <p>Powerful API for integration with other web applications</p> </li> </ul> <p>See http://www.mathjax.org/ for additional details about MathJax, and https://docs.mathjax.org for the MathJax documentation.</p>"},{"location":"static/node_modules/mathjax/#mathjax-components","title":"MathJax Components","text":"<p>MathJax version 3 uses files called components that contain the various MathJax modules that you can include in your web pages or access on a server through NodeJS.  Some components combine all the pieces you need to run MathJax with one or more input formats and a particular output format, while other components are pieces that can be loaded on demand when needed, or by a configuration that specifies the pieces you want to combine in a custom way.  For usage instructions, see the MathJax documentation.</p> <p>Components provide a convenient packaging of MathJax's modules, but it is possible for you to form your own custom components, or to use MathJax's modules directly in a node application on a server.  There are web examples showing how to use MathJax in web pages and how to build your own components, and node examples illustrating how to use components in node applications or call MathJax modules directly.</p>"},{"location":"static/node_modules/mathjax/#whats-in-this-repository","title":"What's in this Repository","text":"<p>This repository contains only the component files for MathJax, not the source code for MathJax (which are available in a separate MathJax source repository).  These component files are the ones served by the CDNs that offer MathJax to the web.  In version 2, the files used on the web were also the source files for MathJax, but in version 3, the source files are no longer on the CDN, as they are not what are run in the browser.</p> <p>The components are stored in the <code>es5</code> directory, and are in ES5 format for the widest possible compatibility.  In the future, we may make an <code>es6</code> directory containing ES6 versions of the components.</p>"},{"location":"static/node_modules/mathjax/#installation-and-use","title":"Installation and Use","text":""},{"location":"static/node_modules/mathjax/#using-mathjax-components-from-a-cdn-on-the-web","title":"Using MathJax components from a CDN on the web","text":"<p>If you are loading MathJax from a CDN into a web page, there is no need to install anything.  Simply use a <code>script</code> tag that loads MathJax from the CDN.  E.g.,</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>See the MathJax documentation, the MathJax Web Demos, and the MathJax Component Repository for more information.</p>"},{"location":"static/node_modules/mathjax/#hosting-your-own-copy-of-the-mathjax-components","title":"Hosting your own copy of the MathJax Components","text":"<p>If you want to host MathJax from your own server, you can do so by installing the <code>mathjax</code> package using <code>npm</code> and moving the <code>es5</code> directory to an appropriate location on your server:</p> <pre><code>npm install mathjax@3\nmv node_modules/mathjax/es5 &lt;path-to-server-location&gt;/mathjax\n</code></pre> <p>Note that we are still making updates to version 2, so include <code>@3</code> when you install, since the latest chronological version may not be version 3.</p> <p>Alternatively, you can get the files via GitHub:</p> <pre><code>git clone https://github.com/mathjax/MathJax.git mj-tmp\nmv mj-tmp/es5 &lt;path-to-server-location&gt;/mathjax\nrm -rf mj-tmp\n</code></pre> <p>Then (in either case) you can use a script tag like the following:</p> <pre><code>&lt;script id=\"MathJax-script\" async src=\"&lt;url-to-your-site&gt;/mathjax/tex-chtml.js\"&gt;&lt;/script&gt;\n</code></pre> <p>where <code>&lt;url-to-your-site&gt;</code> is replaced by the URL to the location where you moved the MathJax files above.</p> <p>See the documentation for details.</p>"},{"location":"static/node_modules/mathjax/#using-mathjax-components-in-a-node-application","title":"Using MathJax components in a node application","text":"<p>To use MathJax components in a node application, install the <code>mathjax</code> package:</p> <pre><code>npm install mathjax@3\n</code></pre> <p>(we are still making updates to version 2, so you should include <code>@3</code> since the latest chronological version may not be version 3).</p> <p>Then require <code>mathjax</code> within your application:</p> <pre><code>require('mathjax').init({ ... }).then((MathJax) =&gt; { ... });\n</code></pre> <p>where the first <code>{ ... }</code> is a MathJax configuration, and the second <code>{ ... }</code> is the code to run after MathJax has been loaded.  E.g.</p> <pre><code>require('mathjax').init({\nloader: {load: ['input/tex', 'output/svg']}\n}).then((MathJax) =&gt; {\nconst svg = MathJax.tex2svg('\\\\frac{1}{x^2-1}', {display: true});\nconsole.log(MathJax.startup.adaptor.outerHTML(svg));\n}).catch((err) =&gt; console.log(err.message));\n</code></pre> <p>Note: this technique is for node-based application only, not for browser applications.  This method sets up an alternative DOM implementation, which you don't need in the browser, and tells MathJax to use node's <code>require()</code> command to load external modules.  This setup will not work properly in the browser, even if you webpack it or bundle it in other ways.</p> <p>See the documentation and the MathJax Node Repository for more details.</p>"},{"location":"static/node_modules/mathjax/#reducing-the-size-of-the-components-directory","title":"Reducing the Size of the Components Directory","text":"<p>Since the <code>es5</code> directory contains all the component files, so if you are only planning one use one configuration, you can reduce the size of the MathJax directory by removing unused components. For example, if you are using the <code>tex-chtml.js</code> component, then you can remove the <code>tex-mml-chtml.js</code>, <code>tex-svg.js</code>, <code>tex-mml-svg.js</code>, <code>tex-chtml-full.js</code>, and <code>tex-svg-full.js</code> configurations, which will save considerable space.  Indeed, you should be able to remove everything other than <code>tex-chtml.js</code>, and the <code>input/tex/extensions</code>, <code>output/chtml/fonts/woff-v2</code>, <code>adaptors</code>, <code>a11y</code>, and <code>sre</code> directories.  If you are using the results only on the web, you can remove <code>adaptors</code> as well.</p> <p>If you are not using A11Y support (e.g., speech generation, or semantic enrichment), then you can remove <code>a11y</code> and <code>sre</code> as well (though in this case you may need to disable the assistive tools in the MathJax contextual menu in order to avoid MathJax trying to load them when they aren't there).</p> <p>If you are using SVG rather than CommonHTML output (e.g., <code>tex-svg.js</code> rather than <code>tex-chtml.js</code>), you can remove the <code>output/chtml/fonts/woff-v2</code> directory.  If you are using MathML input rather than TeX (e.g., <code>mml-chtml.js</code> rather than <code>tex-chtml.js</code>), then you can remove <code>input/tex/extensions</code> as well.</p>"},{"location":"static/node_modules/mathjax/#the-component-files-and-pull-requests","title":"The Component Files and Pull Requests","text":"<p>The <code>es5</code> directory is generated automatically from the contents of the MathJax source repository.  You can rebuild the components using the command</p> <pre><code>npm run make-es5 --silent\n</code></pre> <p>Note that since the contents of this repository are generated automatically, you should not submit pull requests that modify the contents of the <code>es5</code> directory.  If you wish to submit a modification to MathJax, you should make a pull request in the MathJax source repository.</p>"},{"location":"static/node_modules/mathjax/#mathjax-community","title":"MathJax Community","text":"<p>The main MathJax website is http://www.mathjax.org, and it includes announcements and other important information.  A MathJax user forum for asking questions and getting assistance is hosted at Google, and the MathJax bug tracker is hosted at GitHub.</p> <p>Before reporting a bug, please check that it has not already been reported.  Also, please use the bug tracker (rather than the help forum) for reporting bugs, and use the user's forum (rather than the bug tracker) for questions about how to use MathJax.</p>"},{"location":"static/node_modules/mathjax/#mathjax-resources","title":"MathJax Resources","text":"<ul> <li>MathJax Documentation</li> <li>MathJax Components</li> <li>MathJax Source Code</li> <li>MathJax Web Examples</li> <li>MathJax Node Examples</li> <li>MathJax Bug Tracker</li> <li>MathJax Users' Group</li> </ul>"},{"location":"subfolder/","title":"This is the index to a subfolder","text":"<p>Things to discuss</p>"},{"location":"subfolder/another-subfile/","title":"This is a secondary sub page","text":"<p>Things to discuss</p>"}]}