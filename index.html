
<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta name="robots" content="nofollow" />


      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://RAS598-2025-S-Team11.github.io/">
      
      
      
        <link rel="next" href="charts/">
      
      
      <link rel="icon" href="static/logo1.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.4.0">
            

        

    
      
        <title>Introduction - Intelligent TurtleBot - Voice Guided Navigation and Object Detection</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.9f615399.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.649f08f9.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    


   <link href="assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#team-information" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" class="md-header__button md-logo" aria-label="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" data-md-component="logo">
      
  <img src="static/logo1.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Intelligent TurtleBot - Voice Guided Navigation and Object Detection
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Introduction
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="." class="md-tabs__link">
        
  
    
  
  Introduction

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="charts/" class="md-tabs__link">
        
  
    
  
  Charts

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="esp-32-table/" class="md-tabs__link">
        
  
    
  
  Sensors Table

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="obj_detect/" class="md-tabs__link">
        
  
    
  
  Object Detection Page

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="pic-table/" class="md-tabs__link">
        
  
    
  
  TurtleBot4 Hardware Specifications

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="source_code/" class="md-tabs__link">
        
  
    
  
  Backend Logic

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="videos/" class="md-tabs__link">
        
  
    
  
  Video Gallery

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="voice/" class="md-tabs__link">
        
  
    
  
  Voice Control

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="component-selection-example/" class="md-tabs__link">
          
  
    
  
  Component selection example

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="subfolder/" class="md-tabs__link">
          
  
    
  
  Subfolder

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" class="md-nav__button md-logo" aria-label="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" data-md-component="logo">
      
  <img src="static/logo1.svg" alt="logo">

    </a>
    Intelligent TurtleBot - Voice Guided Navigation and Object Detection
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Introduction
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#team-information" class="md-nav__link">
    Team Information
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-description" class="md-nav__link">
    PROJECT DESCRIPTION
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-goals-update" class="md-nav__link">
    Project Goals Update 🎯
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-plan" class="md-nav__link">
    Project Plan 📌
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#work-and-discussions" class="md-nav__link">
    Work and Discussions 🧠
  </a>
  
    <nav class="md-nav" aria-label="Work and Discussions 🧠">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-sensor-fusion-and-real-time-awareness" class="md-nav__link">
    1. Sensor Fusion and Real-Time Awareness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-sensor-inputs" class="md-nav__link">
    2. Sensor Inputs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-perception-and-processing-nodes" class="md-nav__link">
    3. Perception and Processing Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-gui-node" class="md-nav__link">
    4. GUI Node
  </a>
  
    <nav class="md-nav" aria-label="4. GUI Node">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#voice-command-processing" class="md-nav__link">
    🎤 Voice Command Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#object-detection-with-yolov8" class="md-nav__link">
    🔍 Object Detection with YOLOv8
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-visualization-centered-architecture" class="md-nav__link">
    4. Visualization-Centered Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-toward-semi-autonomous-intelligence" class="md-nav__link">
    5. Toward Semi-Autonomous Intelligence
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#updated-goals-and-trade-offs" class="md-nav__link">
    🔄 Updated Goals and Trade-offs
  </a>
  
    <nav class="md-nav" aria-label="🔄 Updated Goals and Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changes-in-project-goals" class="md-nav__link">
    Changes in Project Goals 🔧
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#technical-trade-offs-made" class="md-nav__link">
    Technical Trade-offs Made ⚖️
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ros2-nodes-and-topics" class="md-nav__link">
    📡 ROS2 Nodes and Topics
  </a>
  
    <nav class="md-nav" aria-label="📡 ROS2 Nodes and Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docking-and-undocking-actions" class="md-nav__link">
    🔌 Docking and Undocking Actions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gui-pyqt5-based" class="md-nav__link">
    GUI - PYQT5 BASED 🖥️
  </a>
  
    <nav class="md-nav" aria-label="GUI - PYQT5 BASED 🖥️">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#subscribed-topics" class="md-nav__link">
    📥 Subscribed Topics:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#published-topics" class="md-nav__link">
    📤 Published Topics:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    🧩 Features:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demonstration-videos" class="md-nav__link">
    Demonstration Videos 🖥️
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-demonstration-videos" class="md-nav__link">
    Additional Demonstration Videos
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-project-demonstration" class="md-nav__link">
    🖥️ Final Project Demonstration 🖥️
  </a>
  
    <nav class="md-nav" aria-label="🖥️ Final Project Demonstration 🖥️">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ros2-and-gui" class="md-nav__link">
    ROS2 and GUI
  </a>
  
    <nav class="md-nav" aria-label="ROS2 and GUI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-gui-integration-overview" class="md-nav__link">
    4.1 GUI Integration Overview
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-gui-layout-design-concepts" class="md-nav__link">
    4.2 GUI Layout &amp; Design Concepts
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#these-gui-designs-aim-to-present-important-real-time-system-data-in-an-intuitive-and-user-friendly-format-enabling-both-operator-awareness-and-effective-debugging" class="md-nav__link">
    These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.
  </a>
  
    <nav class="md-nav" aria-label="These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#43-live-sensor-visualization-in-gui" class="md-nav__link">
    4.3 Live Sensor Visualization in GUI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-gui-demonstration-video" class="md-nav__link">
    4.4 GUI Demonstration Video
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-demonstration-videos_1" class="md-nav__link">
    Additional Demonstration Videos
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#45-control-architecture-in-ros2" class="md-nav__link">
    4.5 Control Architecture in ROS2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interaction-mechanism" class="md-nav__link">
    Interaction Mechanism
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preparation-needs" class="md-nav__link">
    Preparation Needs
  </a>
  
    <nav class="md-nav" aria-label="Preparation Needs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#required-knowledge-topics-for-success" class="md-nav__link">
    Required Knowledge &amp; Topics for Success
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-workflow" class="md-nav__link">
    Project Workflow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui" class="md-nav__link">
    
    graph TD
      A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
      B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
      C["Sensor Data&lt;br/&gt;Preprocessing"]
      D1["YOLOv8&lt;br/&gt;Object Detection"]
      D2["Voice Command&lt;br/&gt;Recognition"]
      E["Detected Object Info"]
      F["Intent or Goal Command"]
      G["Decision-Making Node"]
      H["ROS2 Navigation Stack"]
      I["Movement Commands&lt;br/&gt;via /cmd_vel"]
      J["GUI Update:&lt;br/&gt;Object and Nav Info"]
      K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
      L["User Feedback:&lt;br/&gt;GUI Visualization"]
      M["End"]

      A --&gt; B
      B --&gt; C
      C --&gt; D1
      C --&gt; D2
      D1 --&gt; E
      D2 --&gt; F
      E --&gt; G
      F --&gt; G
      G --&gt; H
      H --&gt; I
      I --&gt; K
      K --&gt; M
      G --&gt; J
      J --&gt; L
      L --&gt; M

      classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000
      classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000
      classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000
      classDef decision fill:#fce4ec,stroke:#ec407a,color:#000
      classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000
      classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000

      class A,M startend
      class B sensing
      class C,D1,D2,E,F processing
      class G decision
      class H,I,K navctrl
      class J,L gui

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-demonstration-plan" class="md-nav__link">
    Final Demonstration Plan
  </a>
  
    <nav class="md-nav" aria-label="Final Demonstration Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-execution" class="md-nav__link">
    Setup &amp; Execution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handling-environmental-variability-future-scope" class="md-nav__link">
    Handling Environmental Variability (future scope)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing-evaluation-plan" class="md-nav__link">
    Testing &amp; Evaluation Plan
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    Impact
  </a>
  
    <nav class="md-nav" aria-label="Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#future-scope" class="md-nav__link">
    Future Scope:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-subject-to-change" class="md-nav__link">
    References (Subject to change):
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advising-resources" class="md-nav__link">
    Advising &amp; Resources
  </a>
  
    <nav class="md-nav" aria-label="Advising & Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-advisor" class="md-nav__link">
    Project Advisor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weekly-milestones-weeks-7-16" class="md-nav__link">
    Weekly Milestones (Weeks 7-16)
  </a>
  
    <nav class="md-nav" aria-label="Weekly Milestones (Weeks 7-16)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weekly-milestones-weeks-716" class="md-nav__link">
    Weekly Milestones (Weeks 7–16)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gantt-chart-representation" class="md-nav__link">
    Gantt Chart Representation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#innovation-showcase-at-arizona-state-university-spring-2025" class="md-nav__link">
    🌟 Innovation Showcase at Arizona State University | Spring 2025 🌟
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="charts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Charts
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="esp-32-table/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sensors Table
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="obj_detect/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Detection Page
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="pic-table/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TurtleBot4 Hardware Specifications
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="source_code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Backend Logic
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="videos/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Video Gallery
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="voice/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Voice Control
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="component-selection-example/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Component selection example
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="subfolder/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Subfolder
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#team-information" class="md-nav__link">
    Team Information
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-description" class="md-nav__link">
    PROJECT DESCRIPTION
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-goals-update" class="md-nav__link">
    Project Goals Update 🎯
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-plan" class="md-nav__link">
    Project Plan 📌
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#work-and-discussions" class="md-nav__link">
    Work and Discussions 🧠
  </a>
  
    <nav class="md-nav" aria-label="Work and Discussions 🧠">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-sensor-fusion-and-real-time-awareness" class="md-nav__link">
    1. Sensor Fusion and Real-Time Awareness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-sensor-inputs" class="md-nav__link">
    2. Sensor Inputs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-perception-and-processing-nodes" class="md-nav__link">
    3. Perception and Processing Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-gui-node" class="md-nav__link">
    4. GUI Node
  </a>
  
    <nav class="md-nav" aria-label="4. GUI Node">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#voice-command-processing" class="md-nav__link">
    🎤 Voice Command Processing
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#object-detection-with-yolov8" class="md-nav__link">
    🔍 Object Detection with YOLOv8
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-visualization-centered-architecture" class="md-nav__link">
    4. Visualization-Centered Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-toward-semi-autonomous-intelligence" class="md-nav__link">
    5. Toward Semi-Autonomous Intelligence
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#updated-goals-and-trade-offs" class="md-nav__link">
    🔄 Updated Goals and Trade-offs
  </a>
  
    <nav class="md-nav" aria-label="🔄 Updated Goals and Trade-offs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changes-in-project-goals" class="md-nav__link">
    Changes in Project Goals 🔧
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#technical-trade-offs-made" class="md-nav__link">
    Technical Trade-offs Made ⚖️
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ros2-nodes-and-topics" class="md-nav__link">
    📡 ROS2 Nodes and Topics
  </a>
  
    <nav class="md-nav" aria-label="📡 ROS2 Nodes and Topics">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docking-and-undocking-actions" class="md-nav__link">
    🔌 Docking and Undocking Actions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gui-pyqt5-based" class="md-nav__link">
    GUI - PYQT5 BASED 🖥️
  </a>
  
    <nav class="md-nav" aria-label="GUI - PYQT5 BASED 🖥️">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#subscribed-topics" class="md-nav__link">
    📥 Subscribed Topics:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#published-topics" class="md-nav__link">
    📤 Published Topics:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    🧩 Features:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#demonstration-videos" class="md-nav__link">
    Demonstration Videos 🖥️
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-demonstration-videos" class="md-nav__link">
    Additional Demonstration Videos
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-project-demonstration" class="md-nav__link">
    🖥️ Final Project Demonstration 🖥️
  </a>
  
    <nav class="md-nav" aria-label="🖥️ Final Project Demonstration 🖥️">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ros2-and-gui" class="md-nav__link">
    ROS2 and GUI
  </a>
  
    <nav class="md-nav" aria-label="ROS2 and GUI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-gui-integration-overview" class="md-nav__link">
    4.1 GUI Integration Overview
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-gui-layout-design-concepts" class="md-nav__link">
    4.2 GUI Layout &amp; Design Concepts
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#these-gui-designs-aim-to-present-important-real-time-system-data-in-an-intuitive-and-user-friendly-format-enabling-both-operator-awareness-and-effective-debugging" class="md-nav__link">
    These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.
  </a>
  
    <nav class="md-nav" aria-label="These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#43-live-sensor-visualization-in-gui" class="md-nav__link">
    4.3 Live Sensor Visualization in GUI
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-gui-demonstration-video" class="md-nav__link">
    4.4 GUI Demonstration Video
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#additional-demonstration-videos_1" class="md-nav__link">
    Additional Demonstration Videos
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#45-control-architecture-in-ros2" class="md-nav__link">
    4.5 Control Architecture in ROS2
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interaction-mechanism" class="md-nav__link">
    Interaction Mechanism
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preparation-needs" class="md-nav__link">
    Preparation Needs
  </a>
  
    <nav class="md-nav" aria-label="Preparation Needs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#required-knowledge-topics-for-success" class="md-nav__link">
    Required Knowledge &amp; Topics for Success
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-workflow" class="md-nav__link">
    Project Workflow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui" class="md-nav__link">
    
    graph TD
      A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
      B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
      C["Sensor Data&lt;br/&gt;Preprocessing"]
      D1["YOLOv8&lt;br/&gt;Object Detection"]
      D2["Voice Command&lt;br/&gt;Recognition"]
      E["Detected Object Info"]
      F["Intent or Goal Command"]
      G["Decision-Making Node"]
      H["ROS2 Navigation Stack"]
      I["Movement Commands&lt;br/&gt;via /cmd_vel"]
      J["GUI Update:&lt;br/&gt;Object and Nav Info"]
      K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
      L["User Feedback:&lt;br/&gt;GUI Visualization"]
      M["End"]

      A --&gt; B
      B --&gt; C
      C --&gt; D1
      C --&gt; D2
      D1 --&gt; E
      D2 --&gt; F
      E --&gt; G
      F --&gt; G
      G --&gt; H
      H --&gt; I
      I --&gt; K
      K --&gt; M
      G --&gt; J
      J --&gt; L
      L --&gt; M

      classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000
      classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000
      classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000
      classDef decision fill:#fce4ec,stroke:#ec407a,color:#000
      classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000
      classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000

      class A,M startend
      class B sensing
      class C,D1,D2,E,F processing
      class G decision
      class H,I,K navctrl
      class J,L gui

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-demonstration-plan" class="md-nav__link">
    Final Demonstration Plan
  </a>
  
    <nav class="md-nav" aria-label="Final Demonstration Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-execution" class="md-nav__link">
    Setup &amp; Execution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handling-environmental-variability-future-scope" class="md-nav__link">
    Handling Environmental Variability (future scope)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing-evaluation-plan" class="md-nav__link">
    Testing &amp; Evaluation Plan
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    Impact
  </a>
  
    <nav class="md-nav" aria-label="Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#future-scope" class="md-nav__link">
    Future Scope:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-subject-to-change" class="md-nav__link">
    References (Subject to change):
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advising-resources" class="md-nav__link">
    Advising &amp; Resources
  </a>
  
    <nav class="md-nav" aria-label="Advising & Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-advisor" class="md-nav__link">
    Project Advisor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weekly-milestones-weeks-7-16" class="md-nav__link">
    Weekly Milestones (Weeks 7-16)
  </a>
  
    <nav class="md-nav" aria-label="Weekly Milestones (Weeks 7-16)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#weekly-milestones-weeks-716" class="md-nav__link">
    Weekly Milestones (Weeks 7–16)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gantt-chart-representation" class="md-nav__link">
    Gantt Chart Representation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#innovation-showcase-at-arizona-state-university-spring-2025" class="md-nav__link">
    🌟 Innovation Showcase at Arizona State University | Spring 2025 🌟
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io/edit/main/docs/index.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


<h2 id="team-information">Team Information</h2>
<ul>
<li><strong>Project Name:</strong> Intelligent TurtleBot: Deep Learning-Based Object Detection and Voice-Guided Navigation</li>
<li><strong>Team Number:</strong> 11</li>
<li><strong>Team Members:</strong> Anushka Gangadhar Satav, Adithya Konda, Sameerjeet Singh Chhabra</li>
<li><strong>Semester:</strong> Spring 2025</li>
<li><strong>University:</strong> Arizona State University</li>
<li><strong>Class:</strong> RAS 598 Experimentation and Deployment of Robots</li>
<li><strong>Professor:</strong> Dr. Dan Aukes</li>
<li><strong>Email:</strong> anushka.satav@asu.edu, akonda5@asu.edu, schhab18@asu.edu</li>
</ul>
<hr />
<h2 id="project-description">PROJECT DESCRIPTION</h2>
<blockquote>
<p>Research Question 🔍:</p>
<p><em><strong>How can a mobile robot effectively combine vision, speech, navigation, and manipulation to create a responsive and intelligent system for real-world environments?</strong></em></p>
</blockquote>
<p><strong><em>Answer:</em></strong> </p>
<p>Our project, Intelligent Voice-Controlled Mobile Manipulator, transforms the TurtleBot4 mobile base and the MyCobot robotic arm into a unified, intelligent, multi-modal robotic system. 
Built entirely on ROS 2, the system fuses real-time perception (YOLOv8 object detection), voice interaction (Whisper speech-to-text), LiDAR-based obstacle mapping, and IMU-driven stability monitoring into a modular architecture. 
A custom PyQt5 GUI offers live visualization of all sensor inputs and inference feedback. </p>
<p>We successfully installed the MyCobot arm on the TurtleBot4 and developed our own inverse kinematics solver for accurate and smooth arm positioning. The system is now capable of autonomous object recognition, spatial understanding, and physical actuation. As part of our future scope, we aim to integrate voice-guided arm manipulation—enabling fully hands-free mobile pick-and-place capabilities for applications like warehouse automation and home assistance.</p>
<hr />
<h2 id="project-goals-update">Project Goals Update 🎯</h2>
<ol>
<li>
<p>At the start of the semester, our primary goal was to enable a TurtleBot4 robot to autonomously detect objects and navigate based on voice commands. As the project progressed, we expanded the scope to include real-time sensor integration (LiDAR, IMU), object detection using YOLOv8, and a custom GUI to visualize system performance.</p>
</li>
<li>
<p>Midway through the project, we incorporated the MyCobot robotic arm onto the TurtleBot4 base and developed a custom inverse kinematics (IK) solver for precise manipulation tasks. This marked a major milestone, transforming our system from a mobile robot into a full-fledged mobile manipulator.</p>
</li>
<li>
<p>Currently, the robot can perceive its environment, detect and classify objects, receive voice commands, and accurately move its robotic arm to target positions. In the near future, we plan to extend our voice interaction system to control the robotic arm’s pick-and-place capabilities—moving closer to a truly intelligent, voice-controlled mobile manipulator.</p>
</li>
</ol>
<p>This evolution in project scope reflects our team’s ambition to build a versatile robotic system capable of interacting with dynamic environments in real-time.</p>
<hr />
<h2 id="project-plan">Project Plan 📌</h2>
<p><strong>Concept Overview:</strong>  </p>
<p>This project investigates the design and development of a modular, intelligent robotic system built on the TurtleBot4 platform, enhanced with the MyCobot robotic arm. Our objective is to enable seamless integration of perception, voice interaction, navigation, and manipulation through ROS 2.</p>
<p>The key functionalities implemented include:</p>
<ul>
<li>
<p><strong>Real-Time Object Detection:</strong> 🔍 <br />
  Integration of <strong>YOLOv8</strong> with live camera feeds enables robust object classification and visual awareness in dynamic environments.</p>
</li>
<li>
<p><strong>Voice Command Interaction:</strong> 🗣️ 
  <strong>Whisper</strong>-based transcription allows natural human-robot interaction, enabling the robot to respond to spoken commands and queries.</p>
</li>
<li>
<p><strong>Sensor-Based State Awareness:</strong> 📊<br />
  LiDAR data is visualized in a radar-style map to display detected obstacles, while IMU data is plotted to reflect the robot’s real-time motion and stability.</p>
</li>
<li>
<p><strong>Robotic Arm Integration and Control:</strong> 🦾<br />
  The MyCobot arm was mounted and controlled using a custom inverse kinematics solver for precise motion. Future work includes extending this to support voice-guided manipulation tasks.</p>
</li>
<li>
<p><strong>System Robustness and Fallback Mechanisms:</strong> 🧩 
  The architecture includes modular nodes for fallback behaviors in case of sensor failure, unstable commands, or loss of feedback—ensuring continuity of operation.</p>
</li>
</ul>
<p>Together, these capabilities aim to demonstrate a cohesive and extensible mobile manipulator platform capable of real-time, human-guided autonomy.</p>
<hr />
<h2 id="work-and-discussions">Work and Discussions 🧠</h2>
<h3 id="1-sensor-fusion-and-real-time-awareness">1. Sensor Fusion and Real-Time Awareness</h3>
<p>The Intelligent TurtleBot4 system fuses data from multiple sensory modalities — vision (camera), audio (microphone), and proximity (LiDAR + IMU) — to understand its environment and respond accordingly. 
While we do not implement full navigation or motion planning, the system builds <em><strong>rich situational awareness using ROS2 and real-time data visualization</strong></em>.</p>
<h3 id="2-sensor-inputs">2. Sensor Inputs</h3>
<ul>
<li>📷 <strong>Oak-D Camera:</strong> Provides RGB image streams for visual object detection using YOLOv8.</li>
<li>🎤 <strong>Microphone:</strong> Captures user voice input for natural language interaction.</li>
<li>📡 <strong>LiDAR:</strong> Streams distance measurements for environmental awareness, visualized in a radar-style plot.</li>
<li>🧭 <strong>IMU:</strong> Reports linear acceleration and angular velocity, plotted live to assess robot stability and motion.</li>
</ul>
<p>Each of these sensors publishes to separate ROS2 topics and is handled by dedicated subscriber nodes for processing and visualization.</p>
<h3 id="3-perception-and-processing-nodes">3. Perception and Processing Nodes</h3>
<ul>
<li>🧠 <code>object_detector_node</code>: Subscribes to <code>/oakd/rgb/preview/image_raw</code>, runs real-time object detection, and publishes annotated images and detected labels.</li>
<li>🗣️ <code>mic_listener_node</code>: Captures and transcribes voice using Whisper.cpp, publishing text to <code>/voice_text</code>.</li>
<li>📑 <code>voice_command_parser</code>: Parses text input into discrete commands and publishes to <code>/voice_command</code>.</li>
</ul>
<h3 id="4-gui-node">4. GUI Node</h3>
<ul>
<li>📊 <code>web_dashboard_node</code>: This node powers the real-time graphical user interface (GUI) for monitoring and interacting with the robot. Built using PyQt5, the GUI displays live camera feeds, YOLOv8-detected objects, incoming voice commands, and transcribed text. It also visualizes LiDAR scans around the TurtleBot4, and plots IMU data (linear acceleration and angular velocity) in real-time. </li>
</ul>
<hr />
<h4 id="voice-command-processing">🎤 Voice Command Processing</h4>
<p>We use Whisper.cpp — a lightweight C++ port of OpenAI's Whisper — to transcribe raw audio into text on-device. 
This allows the robot to operate voice interfaces without relying on external services, ensuring speed and privacy. 
Transcribed commands are interpreted and displayed live on the GUI.</p>
<p><strong>Voice Command Set and Behaviors</strong></p>
<p>The following voice trigger phrases were recognized and mapped to robot behaviors using the <code>command_parser_node</code>. These commands were parsed from live audio using Whisper.cpp and translated into action via ROS 2 topics.</p>
<table>
<thead>
<tr>
<th>Voice Command</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>move_forward</code></td>
<td>Moves the TurtleBot forward. 🚶‍♂️</td>
</tr>
<tr>
<td><code>move_backward</code></td>
<td>Moves the TurtleBot backward. 🔙</td>
</tr>
<tr>
<td><code>turn_left</code></td>
<td>Rotates the TurtleBot to the left. ↩️</td>
</tr>
<tr>
<td><code>turn_right</code></td>
<td>Rotates the TurtleBot to the right. ↪️</td>
</tr>
<tr>
<td><code>stop</code></td>
<td>Immediately halts all robot movement. 🛑</td>
</tr>
<tr>
<td><code>look_for_object</code></td>
<td>Initiates YOLOv8 object detection routine and reports objects seen. 🔍</td>
</tr>
<tr>
<td><code>go_to_object</code></td>
<td>Navigates toward the most recently detected object (future scope). 🎯</td>
</tr>
<tr>
<td><code>return_to_base</code></td>
<td>Commands the robot to return to the initial starting location. 🏠</td>
</tr>
<tr>
<td><code>scan</code></td>
<td>Activates a LiDAR/IMU data scan visualization in the GUI. 📡</td>
</tr>
<tr>
<td><code>repeat</code></td>
<td>Repeats the last recognized command. 🔁</td>
</tr>
<tr>
<td><code>dock</code></td>
<td>Sends a dock action request to return to the charging base. 🔌</td>
</tr>
<tr>
<td><code>undock</code></td>
<td>Sends an undock action request to leave the dock. 🔋</td>
</tr>
<tr>
<td><code>spin</code></td>
<td>Performs a 360° rotation in place (playful/diagnostic motion). 🌀</td>
</tr>
<tr>
<td><code>shake</code></td>
<td>Moves the robotic arm in a 'shake' gesture. 🤝</td>
</tr>
<tr>
<td><code>take_a_picture</code></td>
<td>Captures an image from the camera feed (future scope). 📷</td>
</tr>
<tr>
<td><code>home_position</code></td>
<td>Sends the robotic arm to its defined home position. 🏡</td>
</tr>
<tr>
<td><code>list_position</code></td>
<td>Moves the arm to a ‘listening’ posture (e.g., raised ready state). 👂</td>
</tr>
<tr>
<td><code>relax_position</code></td>
<td>Sends the arm to a resting configuration. 😌</td>
</tr>
<tr>
<td><code>wave_emote</code></td>
<td>Executes a waving gesture using the robotic arm. 👋</td>
</tr>
<tr>
<td><code>grasp</code></td>
<td>Activates the gripper to grasp an object (on detection). ✊</td>
</tr>
</tbody>
</table>
<p>This command vocabulary enables natural interaction with the robot across multiple modes — navigation, perception, and manipulation.</p>
<h4 id="object-detection-with-yolov8">🔍 Object Detection with YOLOv8</h4>
<p>YOLOv8 Nano, integrated via Python and Ultralytics API, detects objects in real-time from camera streams. Detected labels are annotated on image frames and published to ROS topics. The modular object detector also logs the types of objects seen and supports a placeholder for future task planning.</p>
<hr />
<h3 id="4-visualization-centered-architecture">4. Visualization-Centered Architecture</h3>
<p>Unlike traditional motion-planning stacks, our system focuses on responsive perception and human-in-the-loop monitoring through:</p>
<ul>
<li>📡 LiDAR visualizations of the robot’s surrounding environment</li>
<li>📈 IMU plots for acceleration and angular motion over time</li>
<li>📷 Live display of raw and detected camera feeds</li>
<li>🗣️ Real-time display of voice input and parsed commands</li>
<li>🔍 Display of currently detected objects (e.g., “I see: person, chair”)</li>
</ul>
<p>This approach lays the groundwork for robust perception and monitoring, while enabling future upgrades for autonomous manipulation.</p>
<hr />
<h3 id="5-toward-semi-autonomous-intelligence">5. Toward Semi-Autonomous Intelligence</h3>
<p>We currently focus on perception, interface, and modular integration. Future versions will introduce:</p>
<ul>
<li>🔁 Voice-guided robotic arm manipulation</li>
<li>🤖 Autonomous pick-and-place planning</li>
<li>📦 Voice-instructed object localization and handling</li>
</ul>
<hr />
<h2 id="updated-goals-and-trade-offs">🔄 Updated Goals and Trade-offs</h2>
<p>Over the course of the project, several goals evolved and practical trade-offs had to be made due to technical challenges and platform constraints. These are summarized below:</p>
<h3 id="changes-in-project-goals">Changes in Project Goals 🔧</h3>
<ul>
<li>Initially, the focus was only on voice-based control of the TurtleBot4 using Whisper transcription and YOLOv8 perception.</li>
<li>Due to hardware limitations (e.g., non-functional laptop microphone at times), a PyQt5-based GUI control system was added that mirrors voice commands to allow fallback manual control. 🎛️</li>
<li>We later expanded the system to include real-time visualization of LiDAR and IMU data to increase system observability and allow performance monitoring. 📊</li>
<li>Integration of the MyCobot arm was initially a future goal but was successfully completed. This included implementing a custom inverse kinematics solver and achieving precise pick-and-place movements. 🤖</li>
</ul>
<h3 id="technical-trade-offs-made">Technical Trade-offs Made ⚖️</h3>
<ul>
<li>We began with Cyclone DDS over Wi-Fi for ROS 2 middleware communication. While topics were visible, we experienced significant instability — especially with image topics where the camera stream would crash or lag badly. 📷💥</li>
<li>Switching to Fast DDS made topics visible but data streams were consistently empty — this issue consumed valuable time with no reliable resolution. ⚠️</li>
<li>
<p>Ultimately, we reverted to Cyclone DDS over a direct Ethernet connection, which worked flawlessly and delivered fast, stable ROS2 communication across devices. 🚀</p>
</li>
<li>
<p>YOLOv8 Nano model was chosen for object detection due to its balance of speed and accuracy. Larger models performed better but slowed inference drastically on the Raspberry Pi. 🐢</p>
</li>
<li>For speech recognition, we initially explored OpenAI’s Whisper via online inference, but it was extremely resource-intensive and unsuitable for local deployment. 🧠💻</li>
<li>Whisper.cpp was selected for its CPU-only support and faster inference. However, the tiny model gave poor transcription results, so we used the base model instead, which slightly compromised speed but greatly improved accuracy. ⏳</li>
<li>IMU and LiDAR data were initially unused, but were later added for feedback visualization on GUI to assist debugging and showcase real-time sensing. 📡</li>
</ul>
<hr />
<h2 id="ros2-nodes-and-topics">📡 ROS2 Nodes and Topics</h2>
<p>The following nodes form the backbone of our system, each responsible for a key capability:</p>
<ul>
<li><code>mic_listener_node</code>: Listens via microphone and transcribes speech to text using Whisper.cpp.</li>
<li><code>command_parser_node</code>: Parses voice/GUIs commands and converts them into robot actions.</li>
<li><code>movement_controller_node</code>: Publishes velocity commands to <code>/cmd_vel</code> and controls movement logic.</li>
<li><code>object_detector_node</code>: Runs YOLOv8-based real-time object detection on live camera feed.</li>
<li><code>web_dashboard_node</code>: Hosts the PyQt5 GUI, displaying all live system feedback and telemetry.</li>
</ul>
<h3 id="docking-and-undocking-actions">🔌 Docking and Undocking Actions</h3>
<p>As part of the robot's mobility and autonomy framework, we integrated basic ROS 2 action-based behaviors for:</p>
<ul>
<li>✅ Docking: Sending a goal to the Create 3 base to autonomously return to its dock station for charging.</li>
<li>🔄 Undocking: Sending an undock goal to initiate departure from the charging dock and resume exploration.</li>
</ul>
<p>These actions were tested and triggered via GUI or voice commands. The docking state was also monitored using the /rpi_13/dock_status topic. This provides a critical capability for future long-duration missions where power management becomes essential.</p>
<p><strong>RQT_GRAPH:</strong> </p>
<p>Below is the ROS2 rqt_graph showing the interconnection of topics, services, and nodes:</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/fb399128-6ba2-4958-80da-ad9d15f14a62" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="ROS 2 Architecture Diagram" src="https://github.com/user-attachments/assets/fb399128-6ba2-4958-80da-ad9d15f14a62" /></a></p>
<hr />
<h2 id="gui-pyqt5-based">GUI - PYQT5 BASED 🖥️</h2>
<p>The <code>web_dashboard_node</code> provides a centralized PyQt5-based dashboard for real-time visualization and control of the TurtleBot4 system. This graphical interface allows users to monitor sensor feedback, camera inputs, object detection, voice commands, and system diagnostics — all in one place.</p>
<h4 id="subscribed-topics">📥 Subscribed Topics:</h4>
<ul>
<li><code>/oakd/rgb/preview/image_raw</code>: Displays live RGB camera feed.</li>
<li><code>/yolo_image_raw</code>: Visualizes YOLOv8-annotated object detection output.</li>
<li><code>/voice_text</code>: Shows the latest voice-transcribed command.</li>
<li><code>/voice_command</code>: Displays the parsed voice or GUI command.</li>
<li><code>/detected_objects</code>: Lists current objects detected by YOLO in real-time.</li>
<li><code>/rpi_13/imu</code>: Streams IMU data (linear acceleration &amp; angular velocity) for plotting.</li>
<li><code>/scan</code>: Plots LiDAR scan as a radial obstacle map with robot at center.</li>
<li><code>/rpi_13/battery_state</code>: Can be used to monitor remaining battery capacity (future feature).</li>
<li><code>/rpi_13/dock_status</code>: Monitors whether the robot is docked (used in future docking logic).</li>
<li><code>/diagnostics</code>: Reads system-level diagnostics (optional, unused in current GUI).</li>
</ul>
<h4 id="published-topics">📤 Published Topics:</h4>
<ul>
<li><code>/voice_text</code>: Republished as needed (e.g., when GUI is used to simulate input).</li>
</ul>
<h4 id="features">🧩 Features:</h4>
<ul>
<li>Displays two camera feeds (live and object detection) and a placeholder image panel.</li>
<li>Real-time sensor visualizations for IMU (x, y, z linear &amp; angular plots) and LiDAR data.</li>
<li>Text display boxes for transcribed voice text, parsed voice/GUI command, and current detected objects.</li>
<li>Color-coded feedback and emojis for an intuitive, user-friendly experience.</li>
<li>Modular layout with expandable PyQt5 widgets for future features such as battery visualization, GUI button control, and docking status indicators.</li>
</ul>
<p>This dashboard complements voice control by offering a manual fallback method for monitoring and interaction — especially useful when microphone input is unavailable or when debugging perception components.</p>
<p><strong>Empty GUI:</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/0c15b374-2ac2-466d-816b-6da410848007" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/0c15b374-2ac2-466d-816b-6da410848007" /></a></p>
<p><strong>GUI With Data:</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/4707b81f-a5d7-4002-89ff-bbb62c0ab408" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/4707b81f-a5d7-4002-89ff-bbb62c0ab408" /></a></p>
<hr />
<h4 id="demonstration-videos">Demonstration Videos 🖥️</h4>
<p><strong>Test 01: Object Detection + GUI Overlay</strong></p>
<p><a href="https://youtube.com/shorts/hz7PwtZZgPg?si=9SIvASX0w476p8vp"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/hz7PwtZZgPg/0.jpg" /></a><br />
<em>Real-time object detection using YOLOv8 with GUI overlay showing detected objects and confidence scores.</em></p>
<hr />
<h4 id="additional-demonstration-videos"><em>Additional Demonstration Videos</em></h4>
<p><strong>Test 02: Controlling the Cobot Robotic Arm</strong></p>
<p><a href="https://youtube.com/shorts/DwuBvafB8k8?si=TU1XXYiRUYbrtGBi"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/DwuBvafB8k8/0.jpg" /></a>  </p>
<p><em>A short demo showcasing voice-command-based control of TurtleBot4 in a dynamic environment.</em></p>
<p><strong>Test 03: Full Autonomy Simulation <em>(Without GUI)</em></strong></p>
<p><a href="https://youtu.be/DtQAx4mQFKQ"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/DtQAx4mQFKQ/0.jpg" /></a> </p>
<p><strong>Test 04: Live Demonstration Test 2 <em>(With GUI)</em></strong></p>
<p><a href="https://youtu.be/EEqiLhgY0YM"><img alt="Live Demostration Take 2" src="https://img.youtube.com/vi/EEqiLhgY0YM/0.jpg" /></a> </p>
<p><strong>Test 05: Turtlebot4 Autonomy with MyCobot <em>(Mobile Manipulator)</em></strong></p>
<p><a href="https://youtu.be/K1KcAVdhhBg"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/K1KcAVdhhBg/0.jpg" /></a>  </p>
<h2 id="final-project-demonstration"><strong>🖥️ Final Project Demonstration 🖥️</strong></h2>
<p><a href="https://youtu.be/K1KcAVdhhBg"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/K1KcAVdhhBg/0.jpg" /></a>  </p>
<p><em>A future test combining voice input, object detection, and autonomous navigation on real hardware.</em></p>
<hr />
<hr />
<h3 id="ros2-and-gui">ROS2 and GUI</h3>
<p>The graphical user interface (GUI) acts as a crucial feedback layer between perception, autonomy, and user interaction. It helps visualize how the robot interprets its environment, processes inputs, and executes decisions.</p>
<hr />
<h4 id="41-gui-integration-overview">4.1 GUI Integration Overview</h4>
<p>The GUI provides live visualization of:</p>
<ul>
<li>Real-time video feed with bounding boxes from YOLOv8 detections</li>
<li>Voice command recognition status and interpreted actions</li>
<li>Robot's velocity, heading, and navigation path</li>
<li>Sensor diagnostics such as LiDAR, IMU, and hazard detection</li>
</ul>
<p>ROS2 topics connected to the GUI include:</p>
<ul>
<li><code>/yolov8_detections</code> – Object detection overlays</li>
<li><code>/voice_cmd</code> – Recognized voice command topics</li>
<li><code>/cmd_vel</code> – Velocity control signals</li>
<li><code>/rpi_13/hazard_detection</code> – Safety monitoring</li>
<li><code>/rpi_13/imu</code> – IMU motion feedback</li>
</ul>
<p><strong>RQT GRAPH:</strong></p>
<ol>
<li>From Dummy nodes (expected rqt_graph):</li>
</ol>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/fd1ee4ba-7d3d-4987-896c-d40baefa6a27" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="makework rqt graph" src="https://github.com/user-attachments/assets/fd1ee4ba-7d3d-4987-896c-d40baefa6a27" /></a></p>
<p><strong>Final RQT-GRAPH:</strong>
<a class="glightbox" href="https://github.com/user-attachments/assets/2d02a218-08bb-422e-9ec4-ca9f4a42d8d0" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="rqt_graph_1_test" src="https://github.com/user-attachments/assets/2d02a218-08bb-422e-9ec4-ca9f4a42d8d0" /></a></p>
<p><strong>FINAL RQT GRAPH WITH GUI</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/6290fe27-37ce-4cea-ada2-2565fd77b094" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="finalrqt" src="https://github.com/user-attachments/assets/6290fe27-37ce-4cea-ada2-2565fd77b094" /></a></p>
<hr />
<h4 id="42-gui-layout-design-concepts">4.2 GUI Layout &amp; Design Concepts</h4>
<p><strong>Initial Mockup using Inkscape</strong><br />
<a class="glightbox" href="https://github.com/user-attachments/assets/069d103c-103e-4c88-a693-fd7bdd21b459" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="GUI Mockup" src="https://github.com/user-attachments/assets/069d103c-103e-4c88-a693-fd7bdd21b459" /></a></p>
<p><strong>Proposed Interactive GUI for ROS2 Integration</strong><br />
<a class="glightbox" href="https://github.com/user-attachments/assets/fb130357-8975-4d73-b05b-b7d9f37f7a61" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Proposed GUI" src="https://github.com/user-attachments/assets/fb130357-8975-4d73-b05b-b7d9f37f7a61" /></a></p>
<p><strong>Test 1: GUI Working on Real-time Data</strong>
- Faced issue with camera (not fixed)
- IMU
- LiDAR</p>
<ol>
<li><strong>GUI Updating IMU Data Live:</strong></li>
</ol>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/b1b43848-33e1-435c-b1bd-480aac67d069" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/b1b43848-33e1-435c-b1bd-480aac67d069" /></a></p>
<ol>
<li><strong>GUI Updating LiDAR Data Live:</strong></li>
</ol>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/cebbeee1-7b54-4118-8094-66bc6d0dcb0c" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/cebbeee1-7b54-4118-8094-66bc6d0dcb0c" /></a></p>
<p><strong>3. GUI Updating Camera Data Live:</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/07bd7b57-199c-46f4-9020-029c12635dd9" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/07bd7b57-199c-46f4-9020-029c12635dd9" /></a></p>
<h2 id="these-gui-designs-aim-to-present-important-real-time-system-data-in-an-intuitive-and-user-friendly-format-enabling-both-operator-awareness-and-effective-debugging"><em>These GUI designs aim to present important real-time system data in an intuitive and user-friendly format, enabling both operator awareness and effective debugging.</em></h2>
<h4 id="43-live-sensor-visualization-in-gui">4.3 Live Sensor Visualization in GUI</h4>
<p>The GUI aggregates multiple sensor streams into a unified dashboard. Key visualized components:</p>
<ul>
<li>
<p><strong>Object Detection Feed:</strong><br />
  Annotated Oak-D camera frames showing bounding boxes and labels.</p>
</li>
<li>
<p><strong>Voice Command Status:</strong><br />
  Display of the latest interpreted command and its mapped action.</p>
</li>
<li>
<p><strong>Navigation Planner View:</strong><br />
  Real-time trajectory paths generated by the navigation stack.</p>
</li>
<li>
<p><strong>Sensor Diagnostics:</strong><br />
  Live data from LiDAR, IMU, and hazard modules.</p>
</li>
</ul>
<p><strong>GUI With Live Sensor Data</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/8a287965-9850-4afe-82e7-7ecfd774a1b3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="MYU FINAL GUI" src="https://github.com/user-attachments/assets/8a287965-9850-4afe-82e7-7ecfd774a1b3" /></a></p>
<p><strong>GUI WITH ACTUAL DATA:</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/3e7af140-66c6-4675-96dc-92d3f298474b" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="FINALLLLLLLGUI" src="https://github.com/user-attachments/assets/3e7af140-66c6-4675-96dc-92d3f298474b" /></a></p>
<hr />
<h4 id="44-gui-demonstration-video">4.4 GUI Demonstration Video</h4>
<p><strong>Test 01: Object Detection + GUI Overlay</strong></p>
<p><a href="https://youtube.com/shorts/hz7PwtZZgPg?si=9SIvASX0w476p8vp"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/hz7PwtZZgPg/0.jpg" /></a><br />
<em>Real-time object detection using YOLOv8 with GUI overlay showing detected objects and confidence scores.</em></p>
<hr />
<h4 id="additional-demonstration-videos_1"><em>Additional Demonstration Videos</em></h4>
<p><strong>Test 02: Controlling the Cobot Robotic Arm</strong></p>
<p><a href="https://youtube.com/shorts/DwuBvafB8k8?si=TU1XXYiRUYbrtGBi"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/DwuBvafB8k8/0.jpg" /></a>  </p>
<p><em>A short demo showcasing voice-command-based control of TurtleBot4 in a dynamic environment.</em></p>
<p><strong>Test 03: Full Autonomy Simulation <em>(Without GUI)</em></strong></p>
<p><a href="https://youtu.be/DtQAx4mQFKQ"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/DtQAx4mQFKQ/0.jpg" /></a> </p>
<p><strong>Test 04: Live Demonstration Test 2 <em>(With GUI)</em></strong></p>
<p><a href="https://youtu.be/EEqiLhgY0YM"><img alt="Live Demostration Take 2" src="https://img.youtube.com/vi/EEqiLhgY0YM/0.jpg" /></a> </p>
<p><strong>Test 04: Turtlebot4 Autonomy with MyCobot <em>(Mobile Manipulator)</em></strong></p>
<p><a href="https://youtube.com/shorts/hz7PwtZZgPg?si=9SIvASX0w476p8vp"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/hz7PwtZZgPg/55.jpg" /></a>  </p>
<p><em>A future test combining voice input, object detection, and autonomous navigation on real hardware.</em></p>
<hr />
<h2 id="45-control-architecture-in-ros2">4.5 Control Architecture in ROS2</h2>
<p>The following flowchart summarizes the control pipeline, organized by data type and decision layer:</p>
<pre class="mermaid"><code>%%{ init: { "theme": "default" } }%%
graph TD

  %% Subgraphs
  subgraph SENSORS ["Sensor Inputs"]
    CAM[Oak-D Camera]
    MIC[Microphone]
    LIDAR[IR / LiDAR Sensors]
  end

  subgraph PERCEPTION ["Perception Nodes"]
    YOLO_NODE[Node: yolov8_processor]
    VOICE_NODE[Node: voice_input_node]
    DETECTIONS[/yolov8_detections/]
    VOICE_RAW[/voice_input/]
    CAM --&gt; YOLO_NODE --&gt; DETECTIONS
    MIC --&gt; VOICE_NODE --&gt; VOICE_RAW
  end

  subgraph AUTONOMY ["High-Level Autonomy"]
    VOICE_PARSER[Node: voice_command_parser]
    VOICE_CMD[/voice_cmd/]
    DECISION[Node: decision_maker_node]
    ACT_CMD[/action_cmd/]
    VOICE_RAW --&gt; VOICE_PARSER --&gt; VOICE_CMD
    DETECTIONS --&gt; DECISION
    VOICE_CMD --&gt; DECISION --&gt; ACT_CMD
  end

  subgraph CONTROL ["Low-Level Control"]
    COLLISION[Node: collision_avoidance_node]
    NAV[Node: navigation_controller]
    CMD[/cmd_vel/]
    LIDAR --&gt; COLLISION
    ACT_CMD --&gt; COLLISION --&gt; NAV --&gt; CMD
  end

  CMD --&gt; MOVE[TurtleBot Movement]

  %% Color classes
  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000
  classDef perception fill:#e6e6fa,stroke:#7e57c2,color:#000
  classDef autonomy fill:#ffe0e6,stroke:#e91e63,color:#000
  classDef control fill:#d0f0ef,stroke:#0097a7,color:#000
  classDef output fill:#eeeeee,stroke:#757575,color:#000

  %% Assign classes
  class CAM,MIC,LIDAR sensors
  class YOLO_NODE,VOICE_NODE,DETECTIONS,VOICE_RAW perception
  class VOICE_PARSER,VOICE_CMD,DECISION,ACT_CMD autonomy
  class COLLISION,NAV,CMD control
  class MOVE output
</code></pre>
<h2 id="interaction-mechanism">Interaction Mechanism</h2>
<p><strong>Behavioral Influence &amp; Interfaces</strong></p>
<ul>
<li><strong>Voice Command API</strong>: Users instruct the robot using predefined phrases.</li>
<li><strong>ROS2 RQT GUI/Web Dashboard</strong>: For remote monitoring.</li>
</ul>
<p><em>(Include a professional-looking UI sketch showing how users will interact with the system.)</em></p>
<hr />
<h2 id="preparation-needs">Preparation Needs</h2>
<h3 id="required-knowledge-topics-for-success">Required Knowledge &amp; Topics for Success</h3>
<pre class="mermaid"><code>graph TD
  A[Preparation Needs]

  A --&gt; B1[Object Detection]
  A --&gt; B2[ROS2 Navigation]
  A --&gt; B3[Speech Recognition]
  A --&gt; B4[Hardware Control]

  B1 --&gt; C1[YOLOv8]
  B1 --&gt; C2[OpenCV]

  B2 --&gt; C3[SLAM]
  B2 --&gt; C4[Collision Avoidance]

  B3 --&gt; C5[Microphone Input]
  B3 --&gt; C6[Command Parsing]

  B4 --&gt; C7[PC-Robot Communication]
  B4 --&gt; C8[Predefined Actions]

  %% Color styling
  classDef detect fill:#ffe0e6,stroke:#e91e63,color:#000
  classDef nav fill:#d0f0ef,stroke:#0097a7,color:#000
  classDef speech fill:#e6e6fa,stroke:#7e57c2,color:#000
  classDef hardware fill:#f9fbe7,stroke:#c0ca33,color:#000
  classDef root fill:#f5f5f5,stroke:#9e9e9e,color:#000

  class A root
  class B1,C1,C2 detect
  class B2,C3,C4 nav
  class B3,C5,C6 speech
  class B4,C7,C8 hardware

</code></pre>
<h2 id="project-workflow">Project Workflow</h2>
<p>The system operates by capturing data from multiple sensors, interpreting user commands, and performing autonomous navigation and feedback. The diagram below outlines the overall data and control flow of the TurtleBot4 system.</p>
<h2 id="graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui"><pre class="mermaid"><code>
    graph TD
      A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
      B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
      C["Sensor Data&lt;br/&gt;Preprocessing"]
      D1["YOLOv8&lt;br/&gt;Object Detection"]
      D2["Voice Command&lt;br/&gt;Recognition"]
      E["Detected Object Info"]
      F["Intent or Goal Command"]
      G["Decision-Making Node"]
      H["ROS2 Navigation Stack"]
      I["Movement Commands&lt;br/&gt;via /cmd_vel"]
      J["GUI Update:&lt;br/&gt;Object and Nav Info"]
      K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
      L["User Feedback:&lt;br/&gt;GUI Visualization"]
      M["End"]

      A --&gt; B
      B --&gt; C
      C --&gt; D1
      C --&gt; D2
      D1 --&gt; E
      D2 --&gt; F
      E --&gt; G
      F --&gt; G
      G --&gt; H
      H --&gt; I
      I --&gt; K
      K --&gt; M
      G --&gt; J
      J --&gt; L
      L --&gt; M

      classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000
      classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000
      classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000
      classDef decision fill:#fce4ec,stroke:#ec407a,color:#000
      classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000
      classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000

      class A,M startend
      class B sensing
      class C,D1,D2,E,F processing
      class G decision
      class H,I,K navctrl
      class J,L gui
</code></pre></h2>
<hr />
<h2 id="final-demonstration-plan">Final Demonstration Plan</h2>
<h3 id="setup-execution">Setup &amp; Execution</h3>
<ul>
<li><strong>Classroom Resources</strong>: Open space for navigation demo.</li>
<li><strong>Demonstration Steps</strong>: The robot will identify objects, respond to user queries, navigate obstacles, and execute spoken commands.</li>
</ul>
<h3 id="handling-environmental-variability-future-scope">Handling Environmental Variability (future scope)</h3>
<ul>
<li><strong>Adaptive Algorithms</strong>: Adjust object detection thresholds dynamically.</li>
<li><strong>Fallback Modes</strong>: If detection fails, switch to manual control or alternative recognition models.</li>
</ul>
<h3 id="testing-evaluation-plan">Testing &amp; Evaluation Plan</h3>
<ul>
<li><strong>Simulated Testing in Gazebo</strong> before real-world deployment.</li>
<li><strong>Comparison Metrics</strong>:</li>
<li>Object recognition accuracy.</li>
<li>Speech command response time.</li>
<li>Navigation success rate.</li>
</ul>
<hr />
<h2 id="impact">Impact</h2>
<p>This project will:</p>
<ul>
<li>Advance AI-driven robotics interactions for smart environments.</li>
<li>Develop speech-integrated autonomous systems.</li>
<li>Provide students hands-on experience with ROS2, AI, and embedded systems.</li>
<li>Potentially contribute to assistive robotics research.</li>
</ul>
<hr />
<hr />
<h3 id="future-scope">Future Scope:</h3>
<p>In future iterations, the system can be extended with:</p>
<ul>
<li>Robotic arm integration mounted on TurtleBot4 for executing pick-and-place actions based on detected objects.</li>
<li>Task-oriented planning using semantic understanding of scenes (e.g., pick red object and place it near the wall).</li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/320d1ff7-a982-4fe4-ac4c-eabfbe49d17d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/320d1ff7-a982-4fe4-ac4c-eabfbe49d17d" /></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/d01f286a-b695-4d40-ac33-74acaa0e303e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d01f286a-b695-4d40-ac33-74acaa0e303e" /></a></p>
<p>Here is the generated image of a cobot mounted on a TurtleBot4 using CHATGPT 4o</p>
<p><strong>FINAL GUI :</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/06ada5be-e975-4c3e-a406-8e9aff208034" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="FINALGUIGUIGUI" src="https://github.com/user-attachments/assets/06ada5be-e975-4c3e-a406-8e9aff208034" /></a></p>
<h2 id="references-subject-to-change">References <em>(Subject to change)</em>:</h2>
<ol>
<li>Deep Learning model options: https://yolov8.com/</li>
<li>YOLOv8 example: https://rs-punia.medium.com/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956</li>
<li>Speech Recognition Libraries: https://pypi.org/project/SpeechRecognition/</li>
<li>Turtlebot4 Mapping Resource: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/generate_map.html</li>
<li>Mapping, Localizing, Path planning packages for Turtlebot4: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/turtlebot4_navigator.html</li>
</ol>
<hr />
<h2 id="advising-resources">Advising &amp; Resources</h2>
<h3 id="project-advisor">Project Advisor</h3>
<ul>
<li><strong>Dr. Daniel Aukes</strong> </li>
<li><strong>Resource Needs</strong>: Hardware support, mentorship on TurtleBot4 Hardware integration with ROS2.</li>
</ul>
<hr />
<h1 id="weekly-milestones-weeks-7-16">Weekly Milestones (Weeks 7-16)</h1>
<h2 id="weekly-milestones-weeks-716">Weekly Milestones (Weeks 7–16)</h2>
<table>
<thead>
<tr>
<th><strong>Week</strong></th>
<th><strong>Date</strong></th>
<th><strong>Milestone</strong></th>
<th><strong>Status</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Week 7</strong></td>
<td>Feb 24, 2025</td>
<td>Finalizing project scope and hardware/sensor availability.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 8</strong></td>
<td>Mar 3, 2025</td>
<td>ROS2 environment setup, VM configuration, TurtleBot4 base initialization.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 9</strong></td>
<td>Mar 10, 2025</td>
<td>Object detection with YOLOv8 using Oak-D camera.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 10</strong></td>
<td>Mar 17, 2025</td>
<td>Voice command parsing, audio processing and integration.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 11</strong></td>
<td>Mar 24, 2025</td>
<td>GUI development and voice-based system control.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 12</strong></td>
<td>Mar 31, 2025</td>
<td>ROS2 node integration and layered autonomy testing.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 13</strong></td>
<td>Apr 7, 2025</td>
<td>TurtleBot testing with full pipeline and live demos.</td>
<td>✅ Completed</td>
</tr>
<tr>
<td><strong>Week 14</strong></td>
<td>Apr 14, 2025</td>
<td>Final debugging, fallback strategies, and performance evaluation.</td>
<td>🔄 In Progress</td>
</tr>
<tr>
<td><strong>Week 15</strong></td>
<td>Apr 21, 2025</td>
<td>Documentation, GUI improvements, and final video preparation.</td>
<td>🔄 In Progress</td>
</tr>
<tr>
<td><strong>Week 16</strong></td>
<td>Apr 28, 2025</td>
<td>🚀 <strong>Final Demonstration &amp; Submission</strong></td>
<td>🔄 In Progress</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="gantt-chart-representation">Gantt Chart Representation</h2>
<pre class="mermaid"><code>gantt
    title Project Timeline (Weeks 7–16)
    dateFormat  YYYY-MM-DD
    axisFormat  %b %d

    section Planning
    Finalize Scope            :active,milestone1, 2025-02-24, 7d

    section Implementation
    ROS2 Setup                :active,milestone2, 2025-03-03, 7d
    GUI Development           :active,milestone3, 2025-03-10, 7d
    YOLOv8 + Oak-D            :active,milestone4, 2025-03-17, 7d
    Voice Command             :crit,milestone5, 2025-03-24, 7d
    ROS2 Integration          :crit,milestone6, 2025-03-31, 10d

    section Testing &amp; Deployment
    TurtleBot Testing         :crit,milestone7, 2025-04-10, 7d
    Final Debug               :milestone8, 2025-04-17, 5d
    Docs &amp; Video              :milestone9, 2025-04-22, 5d
    Final Demo                :milestone10, 2025-04-28, 6d</code></pre>
<hr />
<h2 id="innovation-showcase-at-arizona-state-university-spring-2025">🌟 Innovation Showcase at Arizona State University | Spring 2025 🌟</h2>
<p>Watch on YouTube:</p>
<p><a href="https://youtu.be/K1KcAVdhhBg"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/K1KcAVdhhBg/0.jpg" /></a>  </p>
<p>We presented "Intelligent Voice-Guided Mobile Manipulator: Real-Time Object Detection and Autonomous Navigation Using TurtleBot4 and MyCobot Robot Arm" at the Innovation Showcase hosted at ASU - The Polytechnic School! 
This was our final project for the course RAS 598: Experimentation and Deployment of Robots, led by Professor Dan Aukes.</p>
<p>💡Our project featured a live demo and poster presentation of an autonomous, voice-guided robot that combines real-time speech interaction, object detection, and navigation — built on TurtleBot4 and enhanced with a MyCobot arm. </p>
<hr />





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
        
          
          <a href="charts/" class="md-footer__link md-footer__link--next" aria-label="Next: Charts" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Charts
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 team-name-here
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="/2023_fall" target="_blank" rel="noopener" title="2023 Site" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M64 464c-8.8 0-16-7.2-16-16V64c0-8.8 7.2-16 16-16h160v80c0 17.7 14.3 32 32 32h80v288c0 8.8-7.2 16-16 16H64zM64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V154.5c0-17-6.7-33.3-18.7-45.3l-90.6-90.5C262.7 6.7 246.5 0 229.5 0H64zm97 289c9.4-9.4 9.4-24.6 0-33.9s-24.6-9.4-33.9 0L79 303c-9.4 9.4-9.4 24.6 0 33.9l48 48c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-31-31 31-31zm96-34c-9.4-9.4-24.6-9.4-33.9 0s-9.4 24.6 0 33.9l31 31-31 31c-9.4 9.4-9.4 24.6 0 33.9s24.6 9.4 33.9 0l48-48c9.4-9.4 9.4-24.6 0-33.9l-48-48z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.tabs", "navigation.tabs.sticky", "toc.follow", "navigation.top", "navigation.path", "navigation.indexes", "navigation.prune", "content.action.edit", "navigation.footer"], "search": "assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
        <script src="javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>