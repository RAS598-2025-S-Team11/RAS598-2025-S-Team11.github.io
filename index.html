
<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta name="robots" content="nofollow" />


      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://RAS598-2025-S-Team11.github.io/">
      
      
      
        <link rel="next" href="charts/">
      
      
      <link rel="icon" href="static/logo1.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.4.0">
            

        

    
      
        <title>Project Intelligent TurtleBot4 - Intelligent TurtleBot - Voice Guided Navigation and Object Detection</title>
      
    
    
      <link rel="stylesheet" href="assets/stylesheets/main.9f615399.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.649f08f9.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="stylesheets/extra.css">
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    


   <link href="assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#team-information" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" class="md-header__button md-logo" aria-label="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" data-md-component="logo">
      
  <img src="static/logo1.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Intelligent TurtleBot - Voice Guided Navigation and Object Detection
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Project Intelligent TurtleBot4
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="." class="md-tabs__link">
        
  
    
  
  Project Intelligent TurtleBot4

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="charts/" class="md-tabs__link">
        
  
    
  
  Charts

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="esp-32-table/" class="md-tabs__link">
        
  
    
  
  Sensors Table

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="obj_detect/" class="md-tabs__link">
        
  
    
  
  Object Detection Page

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="pic-table/" class="md-tabs__link">
        
  
    
  
  TurtleBot4 Hardware Specifications

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="second-page/" class="md-tabs__link">
        
  
    
  
  Speech Recognition Page

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="component-selection-example/" class="md-tabs__link">
          
  
    
  
  Component selection example

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="subfolder/" class="md-tabs__link">
          
  
    
  
  Subfolder

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" class="md-nav__button md-logo" aria-label="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" data-md-component="logo">
      
  <img src="static/logo1.svg" alt="logo">

    </a>
    Intelligent TurtleBot - Voice Guided Navigation and Object Detection
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Project Intelligent TurtleBot4
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Project Intelligent TurtleBot4
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#team-information" class="md-nav__link">
    Team Information
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-plan" class="md-nav__link">
    Project Plan
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-workflow" class="md-nav__link">
    Project Workflow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#init-theme-default-themevariables-fontsize-14px-primarycolor-ffffff-edgelabelbackground-ffffff-graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-flow-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-styling-groups-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-assign-node-classes-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui" class="md-nav__link">
    %%{ init: {
  "theme": "default",
  "themeVariables": {
    "fontSize": "14px",
    "primaryColor": "#ffffff",
    "edgeLabelBackground": "#ffffff"
  }
}}%%
graph TD
  A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
  B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
  C["Sensor Data&lt;br/&gt;Preprocessing"]
  D1["YOLOv8&lt;br/&gt;Object Detection"]
  D2["Voice Command&lt;br/&gt;Recognition"]
  E["Detected Object Info"]
  F["Intent or Goal Command"]
  G["Decision-Making Node"]
  H["ROS2 Navigation Stack"]
  I["Movement Commands&lt;br/&gt;via /cmd_vel"]
  J["GUI Update:&lt;br/&gt;Object and Nav Info"]
  K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
  L["User Feedback:&lt;br/&gt;GUI Visualization"]
  M["End"]

  %% Flow
  A --&gt; B
  B --&gt; C
  C --&gt; D1
  C --&gt; D2
  D1 --&gt; E
  D2 --&gt; F
  E --&gt; G
  F --&gt; G
  G --&gt; H
  H --&gt; I
  I --&gt; K
  K --&gt; M
  G --&gt; J
  J --&gt; L
  L --&gt; M

  %% Styling groups
  classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000
  classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000
  classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000
  classDef decision fill:#fce4ec,stroke:#ec407a,color:#000
  classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000
  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000

  %% Assign node classes
  class A,M startend
  class B sensing
  class C,D1,D2,E,F processing
  class G decision
  class H,I,K navctrl
  class J,L gui

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-discussions" class="md-nav__link">
    Project Discussions
  </a>
  
    <nav class="md-nav" aria-label="Project Discussions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-sensor-fusion-and-autonomous-decision-making" class="md-nav__link">
    1. Sensor Fusion and Autonomous Decision-Making
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-sensor-inputs" class="md-nav__link">
    2. Sensor Inputs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-perception-and-processing-nodes" class="md-nav__link">
    3. Perception and Processing Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#31-high-level-autonomy" class="md-nav__link">
    3.1 High-Level Autonomy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-low-level-motion-control" class="md-nav__link">
    3.2 Low-Level Motion Control
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-control-architecture-in-ros2" class="md-nav__link">
    3.3 Control Architecture in ROS2
  </a>
  
    <nav class="md-nav" aria-label="3.3 Control Architecture in ROS2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#4-ros2-and-gui" class="md-nav__link">
    4. ROS2 and GUI
  </a>
  
    <nav class="md-nav" aria-label="4. ROS2 and GUI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gui-integration" class="md-nav__link">
    GUI Integration
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#demonstration-videos" class="md-nav__link">
    Demonstration Videos
  </a>
  
    <nav class="md-nav" aria-label="Demonstration Videos">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-01-object-detection-gui-overlay" class="md-nav__link">
    Test 01: Object Detection + GUI Overlay
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-02-voice-command-navigation-coming-soon" class="md-nav__link">
    Test 02: Voice Command Navigation (Coming Soon)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-03-full-autonomy-simulation-planned" class="md-nav__link">
    Test 03: Full Autonomy Simulation (Planned)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sensor-integration" class="md-nav__link">
    Sensor Integration
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interaction-mechanism" class="md-nav__link">
    Interaction Mechanism
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#control-autonomy" class="md-nav__link">
    Control &amp; Autonomy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preparation-needs" class="md-nav__link">
    Preparation Needs
  </a>
  
    <nav class="md-nav" aria-label="Preparation Needs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#required-knowledge-topics-for-success" class="md-nav__link">
    Required Knowledge &amp; Topics for Success
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-demonstration-plan" class="md-nav__link">
    Final Demonstration Plan
  </a>
  
    <nav class="md-nav" aria-label="Final Demonstration Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-execution" class="md-nav__link">
    Setup &amp; Execution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handling-environmental-variability-future-scope" class="md-nav__link">
    Handling Environmental Variability (future scope)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing-evaluation-plan" class="md-nav__link">
    Testing &amp; Evaluation Plan
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    Impact
  </a>
  
    <nav class="md-nav" aria-label="Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#future-scope" class="md-nav__link">
    Future Scope:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-subject-to-change" class="md-nav__link">
    References (Subject to change):
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advising-resources" class="md-nav__link">
    Advising &amp; Resources
  </a>
  
    <nav class="md-nav" aria-label="Advising & Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-advisor" class="md-nav__link">
    Project Advisor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weekly-milestones-weeks-7-16" class="md-nav__link">
    Weekly Milestones (Weeks 7-16)
  </a>
  
    <nav class="md-nav" aria-label="Weekly Milestones (Weeks 7-16)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gantt-chart-representation" class="md-nav__link">
    Gantt Chart Representation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="charts/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Charts
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="esp-32-table/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sensors Table
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="obj_detect/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Detection Page
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="pic-table/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TurtleBot4 Hardware Specifications
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="second-page/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Speech Recognition Page
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="component-selection-example/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Component selection example
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="subfolder/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Subfolder
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#team-information" class="md-nav__link">
    Team Information
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-plan" class="md-nav__link">
    Project Plan
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-workflow" class="md-nav__link">
    Project Workflow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#init-theme-default-themevariables-fontsize-14px-primarycolor-ffffff-edgelabelbackground-ffffff-graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-flow-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-styling-groups-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-assign-node-classes-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui" class="md-nav__link">
    %%{ init: {
  "theme": "default",
  "themeVariables": {
    "fontSize": "14px",
    "primaryColor": "#ffffff",
    "edgeLabelBackground": "#ffffff"
  }
}}%%
graph TD
  A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
  B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
  C["Sensor Data&lt;br/&gt;Preprocessing"]
  D1["YOLOv8&lt;br/&gt;Object Detection"]
  D2["Voice Command&lt;br/&gt;Recognition"]
  E["Detected Object Info"]
  F["Intent or Goal Command"]
  G["Decision-Making Node"]
  H["ROS2 Navigation Stack"]
  I["Movement Commands&lt;br/&gt;via /cmd_vel"]
  J["GUI Update:&lt;br/&gt;Object and Nav Info"]
  K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
  L["User Feedback:&lt;br/&gt;GUI Visualization"]
  M["End"]

  %% Flow
  A --&gt; B
  B --&gt; C
  C --&gt; D1
  C --&gt; D2
  D1 --&gt; E
  D2 --&gt; F
  E --&gt; G
  F --&gt; G
  G --&gt; H
  H --&gt; I
  I --&gt; K
  K --&gt; M
  G --&gt; J
  J --&gt; L
  L --&gt; M

  %% Styling groups
  classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000
  classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000
  classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000
  classDef decision fill:#fce4ec,stroke:#ec407a,color:#000
  classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000
  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000

  %% Assign node classes
  class A,M startend
  class B sensing
  class C,D1,D2,E,F processing
  class G decision
  class H,I,K navctrl
  class J,L gui

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#project-discussions" class="md-nav__link">
    Project Discussions
  </a>
  
    <nav class="md-nav" aria-label="Project Discussions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-sensor-fusion-and-autonomous-decision-making" class="md-nav__link">
    1. Sensor Fusion and Autonomous Decision-Making
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-sensor-inputs" class="md-nav__link">
    2. Sensor Inputs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-perception-and-processing-nodes" class="md-nav__link">
    3. Perception and Processing Nodes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#31-high-level-autonomy" class="md-nav__link">
    3.1 High-Level Autonomy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-low-level-motion-control" class="md-nav__link">
    3.2 Low-Level Motion Control
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#33-control-architecture-in-ros2" class="md-nav__link">
    3.3 Control Architecture in ROS2
  </a>
  
    <nav class="md-nav" aria-label="3.3 Control Architecture in ROS2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#4-ros2-and-gui" class="md-nav__link">
    4. ROS2 and GUI
  </a>
  
    <nav class="md-nav" aria-label="4. ROS2 and GUI">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gui-integration" class="md-nav__link">
    GUI Integration
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#demonstration-videos" class="md-nav__link">
    Demonstration Videos
  </a>
  
    <nav class="md-nav" aria-label="Demonstration Videos">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#test-01-object-detection-gui-overlay" class="md-nav__link">
    Test 01: Object Detection + GUI Overlay
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-02-voice-command-navigation-coming-soon" class="md-nav__link">
    Test 02: Voice Command Navigation (Coming Soon)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-03-full-autonomy-simulation-planned" class="md-nav__link">
    Test 03: Full Autonomy Simulation (Planned)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#sensor-integration" class="md-nav__link">
    Sensor Integration
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#interaction-mechanism" class="md-nav__link">
    Interaction Mechanism
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#control-autonomy" class="md-nav__link">
    Control &amp; Autonomy
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#preparation-needs" class="md-nav__link">
    Preparation Needs
  </a>
  
    <nav class="md-nav" aria-label="Preparation Needs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#required-knowledge-topics-for-success" class="md-nav__link">
    Required Knowledge &amp; Topics for Success
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-demonstration-plan" class="md-nav__link">
    Final Demonstration Plan
  </a>
  
    <nav class="md-nav" aria-label="Final Demonstration Plan">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setup-execution" class="md-nav__link">
    Setup &amp; Execution
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#handling-environmental-variability-future-scope" class="md-nav__link">
    Handling Environmental Variability (future scope)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#testing-evaluation-plan" class="md-nav__link">
    Testing &amp; Evaluation Plan
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#impact" class="md-nav__link">
    Impact
  </a>
  
    <nav class="md-nav" aria-label="Impact">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#future-scope" class="md-nav__link">
    Future Scope:
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-subject-to-change" class="md-nav__link">
    References (Subject to change):
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advising-resources" class="md-nav__link">
    Advising &amp; Resources
  </a>
  
    <nav class="md-nav" aria-label="Advising & Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-advisor" class="md-nav__link">
    Project Advisor
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#weekly-milestones-weeks-7-16" class="md-nav__link">
    Weekly Milestones (Weeks 7-16)
  </a>
  
    <nav class="md-nav" aria-label="Weekly Milestones (Weeks 7-16)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gantt-chart-representation" class="md-nav__link">
    Gantt Chart Representation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io/edit/main/docs/index.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


<h2 id="team-information">Team Information</h2>
<ul>
<li><strong>Project Name:</strong> Intelligent TurtleBot: Deep Learning-Based Object Detection and Voice-Guided Navigation</li>
<li><strong>Team Number:</strong> 11</li>
<li><strong>Team Members:</strong> Anushka Gangadhar Satav, Adithya Konda, Sameerjeet Singh Chhabra</li>
<li><strong>Semester:</strong> Spring 2025</li>
<li><strong>University:</strong> Arizona State University</li>
<li><strong>Class:</strong> RAS 598 Experimentation and Deployment of Robots</li>
<li><strong>Professor:</strong> Dr. Dan Aukes</li>
<li><strong>Email:</strong> anushka.satav@asu.edu, akonda5@asu.edu, schhab18@asu.edu</li>
</ul>
<hr />
<h2 id="project-plan">Project Plan</h2>
<blockquote>
<p>Research Question: 
How can a mobile robot effectively combine vision, speech, and autonomous navigation to create a responsive and interactive system in real-world environments?</p>
</blockquote>
<p><strong>Concept:</strong> </p>
<p>This project explores how TurtleBot4 can intelligently interact with its surroundings through vision-based object detection and voice-guided navigation. Using TurtleBot 4 with Create 3 and Raspberry Pi, our aim is to integrate and demonstrate the following capabilities:</p>
<ul>
<li>Real-time object detection using YOLOv8, recognizing and categorizing environmental objects with live camera input.</li>
<li>Voice command interaction, allowing users to issue spoken instructions that influence robot behavior through a natural interface.</li>
<li>Autonomous navigation with obstacle avoidance using the ROS2 navigation stack, enabling safe and efficient mobility.</li>
<li>Decision-making based on perception, where voice commands and visual detections combine to guide task execution.</li>
<li>Fallback mechanisms to ensure robustness and handle unexpected failures in hardware or software modules.</li>
</ul>
<hr />
<p>This updated scope aligns directly with our project deliverables — voice-guided, vision-aware autonomous robot control — and leaves room for seamless extension into manipulation and complex task planning.</p>
<hr />
<h2 id="project-workflow">Project Workflow</h2>
<p>The system operates by capturing data from multiple sensors, interpreting user commands, and performing autonomous navigation and feedback. The diagram below outlines the overall data and control flow of the TurtleBot4 system.</p>
<h2 id="init-theme-default-themevariables-fontsize-14px-primarycolor-ffffff-edgelabelbackground-ffffff-graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-flow-a-b-b-c-c-d1-c-d2-d1-e-d2-f-e-g-f-g-g-h-h-i-i-k-k-m-g-j-j-l-l-m-styling-groups-classdef-startend-fillf6e3f3strokec27ba0color000-classdef-sensing-filld0f0efstroke5bbdbbcolor000-classdef-processing-fille8eaf6stroke7986cbcolor000-classdef-decision-fillfce4ecstrokeec407acolor000-classdef-navctrl-fille0f7fastroke00838fcolor000-classdef-gui-fillede7f6stroke7e57c2color000-assign-node-classes-class-am-startend-class-b-sensing-class-cd1d2ef-processing-class-g-decision-class-hik-navctrl-class-jl-gui"><pre class="mermaid"><code>%%{ init: {
  "theme": "default",
  "themeVariables": {
    "fontSize": "14px",
    "primaryColor": "#ffffff",
    "edgeLabelBackground": "#ffffff"
  }
}}%%
graph TD
  A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
  B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
  C["Sensor Data&lt;br/&gt;Preprocessing"]
  D1["YOLOv8&lt;br/&gt;Object Detection"]
  D2["Voice Command&lt;br/&gt;Recognition"]
  E["Detected Object Info"]
  F["Intent or Goal Command"]
  G["Decision-Making Node"]
  H["ROS2 Navigation Stack"]
  I["Movement Commands&lt;br/&gt;via /cmd_vel"]
  J["GUI Update:&lt;br/&gt;Object and Nav Info"]
  K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
  L["User Feedback:&lt;br/&gt;GUI Visualization"]
  M["End"]

  %% Flow
  A --&gt; B
  B --&gt; C
  C --&gt; D1
  C --&gt; D2
  D1 --&gt; E
  D2 --&gt; F
  E --&gt; G
  F --&gt; G
  G --&gt; H
  H --&gt; I
  I --&gt; K
  K --&gt; M
  G --&gt; J
  J --&gt; L
  L --&gt; M

  %% Styling groups
  classDef startend fill:#f6e3f3,stroke:#c27ba0,color:#000
  classDef sensing fill:#d0f0ef,stroke:#5bbdbb,color:#000
  classDef processing fill:#e8eaf6,stroke:#7986cb,color:#000
  classDef decision fill:#fce4ec,stroke:#ec407a,color:#000
  classDef navctrl fill:#e0f7fa,stroke:#00838f,color:#000
  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000

  %% Assign node classes
  class A,M startend
  class B sensing
  class C,D1,D2,E,F processing
  class G decision
  class H,I,K navctrl
  class J,L gui
</code></pre></h2>
<h2 id="project-discussions">Project Discussions</h2>
<h3 id="1-sensor-fusion-and-autonomous-decision-making">1. Sensor Fusion and Autonomous Decision-Making</h3>
<p>The TurtleBot4 system leverages multiple data sources — including vision, audio, and LiDAR — to perform real-time perception and intelligent control. 
These data streams are handled through modular ROS2 nodes that contribute to both high-level autonomy and low-level motion control.</p>
<h3 id="2-sensor-inputs">2. Sensor Inputs</h3>
<ul>
<li><strong>Oak-D Camera</strong> provides real-time RGB images for visual perception.</li>
<li><strong>Microphone</strong> captures user voice commands as audio input.</li>
<li><strong>LiDAR and IR sensors</strong> are used for obstacle detection and collision avoidance.</li>
</ul>
<p>These sensors operate concurrently and feed their outputs into dedicated ROS2 nodes.</p>
<h3 id="3-perception-and-processing-nodes">3. Perception and Processing Nodes</h3>
<ul>
<li><code>yolov8_processor</code>: Subscribes to image streams (<code>/rpi_13/oakd/preview/image_raw</code>) and outputs detected object data to <code>/yolov8_detections</code>.</li>
<li><code>voice_input_node</code>: Processes raw audio and converts it into structured voice data (<code>/voice_input</code>).</li>
<li><code>voice_command_parser</code>: Transforms audio into semantic commands (e.g., "move forward") and publishes them to <code>/voice_cmd</code>.</li>
</ul>
<p>This layered perception setup allows the robot to understand both its <strong>physical environment</strong> and <strong>human intent</strong> simultaneously.</p>
<h3 id="31-high-level-autonomy">3.1 High-Level Autonomy</h3>
<p>The core decision-making logic resides in the <code>decision_maker_node</code>, which:</p>
<ul>
<li>Consumes outputs from both <code>/yolov8_detections</code> and <code>/voice_cmd</code></li>
<li>Prioritizes user commands or detected objects to determine the appropriate action</li>
<li>Publishes resulting commands to <code>/action_cmd</code></li>
</ul>
<h3 id="32-low-level-motion-control">3.2 Low-Level Motion Control</h3>
<p>Low-level execution is handled by the following:</p>
<ul>
<li><code>collision_avoidance_node</code>: Merges <code>/action_cmd</code> with real-time sensor data (e.g., LiDAR) to determine safe trajectories</li>
<li><code>navigation_controller</code>: Translates the motion plan into velocity commands published to <code>/cmd_vel</code></li>
</ul>
<p>These nodes form a closed-loop controller that ensures both <strong>goal-directed behavior</strong> and <strong>safety</strong>.</p>
<hr />
<h2 id="33-control-architecture-in-ros2">3.3 Control Architecture in ROS2</h2>
<p>The following flowchart summarizes the control pipeline, organized by data type and decision layer:</p>
<pre class="mermaid"><code>graph TD

  subgraph "Sensor Inputs"
    CAM[Oak-D Camera]
    MIC[Microphone]
    LIDAR[IR / LiDAR Sensors]
  end

  subgraph "Perception Nodes"
    YOLO_NODE[Node: yolov8_processor]
    VOICE_NODE[Node: voice_input_node]
    DETECTIONS[/yolov8_detections/]
    VOICE_RAW[/voice_input/]
    CAM --&gt; YOLO_NODE --&gt; DETECTIONS
    MIC --&gt; VOICE_NODE --&gt; VOICE_RAW
  end

  subgraph "High-Level Autonomy"
    VOICE_PARSER[Node: voice_command_parser]
    VOICE_CMD[/voice_cmd/]
    DECISION[Node: decision_maker_node]
    ACT_CMD[/action_cmd/]
    VOICE_RAW --&gt; VOICE_PARSER --&gt; VOICE_CMD
    DETECTIONS --&gt; DECISION
    VOICE_CMD --&gt; DECISION --&gt; ACT_CMD
  end

  subgraph "Low-Level Control"
    COLLISION[Node: collision_avoidance_node]
    NAV[Node: navigation_controller]
    CMD[/cmd_vel/]
    LIDAR --&gt; COLLISION
    ACT_CMD --&gt; COLLISION --&gt; NAV --&gt; CMD
  end

  CMD --&gt; MOVE[TurtleBot Movement]</code></pre>
<h3 id="4-ros2-and-gui">4. ROS2 and GUI</h3>
<h4 id="gui-integration">GUI Integration</h4>
<p>The graphical user interface (GUI) plays a key role in closing the feedback loop between perception, autonomy, and user interaction. It provides real-time insights into the robot's internal state and sensory understanding.</p>
<p>The GUI displays:</p>
<ul>
<li>Real-time video feed with overlaid object detections from the onboard Oak-D camera</li>
<li>Voice command recognition status and interpreted instructions</li>
<li>Navigation and movement feedback, including direction, velocity, and planned path</li>
<li>Sensor data such as LiDAR scans, IMU readings, and obstacle detections</li>
</ul>
<p>The GUI is integrated with ROS2 and subscribes to the following key topics:</p>
<ul>
<li><code>/yolov8_detections</code> – Visual object detection results</li>
<li><code>/voice_cmd</code> – Parsed user voice commands</li>
<li><code>/cmd_vel</code> – Velocity commands issued to the robot</li>
<li><code>/rpi_13/hazard_detection</code> – Safety and obstacle data</li>
<li><code>/rpi_13/imu</code> – Orientation and motion readings</li>
</ul>
<p>This interface allows users to monitor system behavior in real-time, verify decision-making logic, and better understand the robot's perception and action in dynamic environments.</p>
<p><strong>1. Using Inkscape</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/069d103c-103e-4c88-a693-fd7bdd21b459" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/069d103c-103e-4c88-a693-fd7bdd21b459" /></a></p>
<p><strong>2. Proposed GUI For Control</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/641e4521-9a4e-4248-9bda-127ad392c9cd" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="team-assignment-03-gui" src="https://github.com/user-attachments/assets/641e4521-9a4e-4248-9bda-127ad392c9cd" /></a></p>
<p><strong>3. Proposed GUI For Navigation</strong>
<a class="glightbox" href="https://github.com/user-attachments/assets/0ae545f4-2bab-45d4-878f-b5a7143b01e1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="task-2" src="https://github.com/user-attachments/assets/0ae545f4-2bab-45d4-878f-b5a7143b01e1" /></a></p>
<hr />
<h2 id="demonstration-videos">Demonstration Videos</h2>
<h3 id="test-01-object-detection-gui-overlay">Test 01: Object Detection + GUI Overlay</h3>
<p><a href="https://youtube.com/shorts/hz7PwtZZgPg?si=9SIvASX0w476p8vp"><img alt="Watch on YouTube" src="https://img.youtube.com/vi/hz7PwtZZgPg/0.jpg" /></a></p>
<p><em>Real-time object detection using YOLOv8 with GUI overlay showing detected objects and confidence scores.</em></p>
<hr />
<h3 id="test-02-voice-command-navigation-coming-soon">Test 02: Voice Command Navigation <em>(Coming Soon)</em></h3>
<p><em>A short demo showcasing voice-command-based control of TurtleBot4 in a dynamic environment.</em></p>
<hr />
<h3 id="test-03-full-autonomy-simulation-planned">Test 03: Full Autonomy Simulation <em>(Planned)</em></h3>
<p><em>A future test combining voice input, object detection, and autonomous navigation on real hardware.</em></p>
<hr />
<h2 id="sensor-integration">Sensor Integration</h2>
<p><strong>Utilization of Sensor Data</strong></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/dfe928cf-b7cf-4cf4-82e4-b80c5853edfc" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="TurtleBotsFacing" src="https://github.com/user-attachments/assets/dfe928cf-b7cf-4cf4-82e4-b80c5853edfc" /></a></p>
<blockquote>
<p>Check <em>Sensors Table</em> for more Information</p>
</blockquote>
<ul>
<li><strong>Depth Camera (OAK-D/RealSense)</strong>: Object detection and distance estimation.</li>
<li><strong>LiDAR</strong>: SLAM-based navigation and obstacle detection.</li>
<li><strong>IMU</strong>: Enhancing motion stability and drift correction.</li>
<li><strong>Microphone</strong>: Capturing voice commands.</li>
<li><strong>Speaker</strong>: Responding with audio feedback.</li>
</ul>
<p><strong>Testing and Demonstration</strong></p>
<ul>
<li><strong>Unit Testing</strong>: Each sensor tested in isolation.</li>
<li><strong>Integration Testing</strong>: Validating sensor fusion for decision-making.</li>
<li><strong>Final Demonstration</strong>: Robot detects objects, responds to queries, follows voice commands, and navigates autonomously.</li>
</ul>
<hr />
<h2 id="interaction-mechanism">Interaction Mechanism</h2>
<p><strong>Behavioral Influence &amp; Interfaces</strong></p>
<ul>
<li><strong>Voice Command API</strong>: Users instruct the robot using predefined phrases.</li>
<li><strong>ROS2 RQT GUI/Web Dashboard</strong>: For remote monitoring.</li>
</ul>
<p><em>(Include a professional-looking UI sketch showing how users will interact with the system.)</em></p>
<h2 id="control-autonomy">Control &amp; Autonomy</h2>
<ul>
<li><strong>Sensor-Driven Control Loop</strong>: Robot makes decisions based on camera, LiDAR, and IMU inputs.</li>
<li><strong>ROS2 Navigation Stack</strong>: Used for path planning and movement.</li>
<li><strong>Hierarchical Decision Model</strong>: Combining high-level commands with low-level execution.</li>
</ul>
<hr />
<h2 id="preparation-needs">Preparation Needs</h2>
<h3 id="required-knowledge-topics-for-success">Required Knowledge &amp; Topics for Success</h3>
<ul>
<li><strong>Deep Learning for Object Detection</strong> (YOLOv8, OpenCV)</li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/b6a43a67-39f3-478a-89bd-19b4d52bfc7f" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="1_HctUSTC6_-OSWmCtTwmdeQ" src="https://github.com/user-attachments/assets/b6a43a67-39f3-478a-89bd-19b4d52bfc7f" /></a></p>
<ul>
<li><strong>ROS2 Navigation &amp; Collision Avoidance</strong></li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/688f6b41-9997-4dbe-bbdc-381155b9fe72" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="turtlebot4" src="https://github.com/user-attachments/assets/688f6b41-9997-4dbe-bbdc-381155b9fe72" /></a></p>
<ul>
<li><strong>Speech Recognition</strong></li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/2f24f9f2-9335-4004-8189-de530818126a" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="download" src="https://github.com/user-attachments/assets/2f24f9f2-9335-4004-8189-de530818126a" /></a></p>
<ul>
<li><strong>Hardware-Level Control for TurtleBot 4</strong></li>
</ul>
<p>Using communication between Host PC and TurtleBot4 to take Voice Commands as inputs and perform Pre-defined Actions</p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/6b89eaaf-8b66-48bb-af69-54fefb0fac88" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="images" src="https://github.com/user-attachments/assets/6b89eaaf-8b66-48bb-af69-54fefb0fac88" /></a></p>
<hr />
<h2 id="final-demonstration-plan">Final Demonstration Plan</h2>
<h3 id="setup-execution">Setup &amp; Execution</h3>
<ul>
<li><strong>Classroom Resources</strong>: Open space for navigation demo.</li>
<li><strong>Demonstration Steps</strong>: The robot will identify objects, respond to user queries, navigate obstacles, and execute spoken commands.</li>
</ul>
<h3 id="handling-environmental-variability-future-scope">Handling Environmental Variability (future scope)</h3>
<ul>
<li><strong>Adaptive Algorithms</strong>: Adjust object detection thresholds dynamically.</li>
<li><strong>Fallback Modes</strong>: If detection fails, switch to manual control or alternative recognition models.</li>
</ul>
<h3 id="testing-evaluation-plan">Testing &amp; Evaluation Plan</h3>
<ul>
<li><strong>Simulated Testing in Gazebo</strong> before real-world deployment.</li>
<li><strong>Comparison Metrics</strong>:</li>
<li>Object recognition accuracy.</li>
<li>Speech command response time.</li>
<li>Navigation success rate.</li>
</ul>
<hr />
<h2 id="impact">Impact</h2>
<p>This project will:</p>
<ul>
<li>Advance AI-driven robotics interactions for smart environments.</li>
<li>Develop speech-integrated autonomous systems.</li>
<li>Provide students hands-on experience with ROS2, AI, and embedded systems.</li>
<li>Potentially contribute to assistive robotics research.</li>
</ul>
<hr />
<hr />
<h3 id="future-scope">Future Scope:</h3>
<p>In future iterations, the system can be extended with:</p>
<ul>
<li>Robotic arm integration mounted on TurtleBot4 for executing pick-and-place actions based on detected objects.</li>
<li>Task-oriented planning using semantic understanding of scenes (e.g., pick red object and place it near the wall).</li>
</ul>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/320d1ff7-a982-4fe4-ac4c-eabfbe49d17d" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/320d1ff7-a982-4fe4-ac4c-eabfbe49d17d" /></a></p>
<p><a class="glightbox" href="https://github.com/user-attachments/assets/d01f286a-b695-4d40-ac33-74acaa0e303e" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="image" src="https://github.com/user-attachments/assets/d01f286a-b695-4d40-ac33-74acaa0e303e" /></a></p>
<p>Here is the generated image of a cobot mounted on a TurtleBot4 using CHATGPT 4o</p>
<h2 id="references-subject-to-change">References <em>(Subject to change)</em>:</h2>
<ol>
<li>Deep Learning model options: https://yolov8.com/</li>
<li>YOLOv8 example: https://rs-punia.medium.com/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956</li>
<li>Speech Recognition Libraries: https://pypi.org/project/SpeechRecognition/</li>
<li>Turtlebot4 Mapping Resource: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/generate_map.html</li>
<li>Mapping, Localizing, Path planning packages for Turtlebot4: https://turtlebot.github.io/turtlebot4-user-manual/tutorials/turtlebot4_navigator.html</li>
</ol>
<hr />
<h2 id="advising-resources">Advising &amp; Resources</h2>
<h3 id="project-advisor">Project Advisor</h3>
<ul>
<li><strong>Dr. Daniel Aukes</strong> </li>
<li><strong>Resource Needs</strong>: Hardware support, mentorship on TurtleBot4 Hardware integration with ROS2.</li>
</ul>
<hr />
<h1 id="weekly-milestones-weeks-7-16">Weekly Milestones (Weeks 7-16)</h1>
<table>
<thead>
<tr>
<th><strong>Week</strong></th>
<th><strong>Date</strong></th>
<th><strong>Milestone</strong></th>
<th><strong>Status</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Week 7</strong></td>
<td><strong>Feb 24, 2025</strong></td>
<td>Finalizing project scope, confirming sensor availability.</td>
<td>🔄 In Progress</td>
</tr>
<tr>
<td><strong>Week 8</strong></td>
<td><strong>Mar 3, 2025</strong></td>
<td>Setting up ROS2 communication between Raspberry Pi and host machine.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 9</strong></td>
<td><strong>Mar 10, 2025</strong></td>
<td>Implementing object detection pipeline.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 10</strong></td>
<td><strong>Mar 17, 2025</strong></td>
<td>Developing speech recognition module.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 11</strong></td>
<td><strong>Mar 24, 2025</strong></td>
<td>Testing navigation with LiDAR and obstacle avoidance.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 12</strong></td>
<td><strong>Mar 31, 2025</strong></td>
<td>Integrating all modules for unified control.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 13</strong></td>
<td><strong>Apr 7, 2025</strong></td>
<td>Conducting real-world tests and debugging issues.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 14</strong></td>
<td><strong>Apr 14, 2025</strong></td>
<td>Preparing final demonstration setup.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 15</strong></td>
<td><strong>Apr 21, 2025</strong></td>
<td>Final testing, documentation, and advisor review.</td>
<td>⏳ Pending</td>
</tr>
<tr>
<td><strong>Week 16</strong></td>
<td><strong>Apr 28, 2025</strong></td>
<td>🚀 <strong>Final Demonstration &amp; Project Submission</strong> 🎯</td>
<td>🚀 Upcoming</td>
</tr>
</tbody>
</table>
<hr />
<h2 id="gantt-chart-representation">Gantt Chart Representation</h2>
<pre class="mermaid"><code>gantt
    title Project Timeline (Weeks 7-16)
    dateFormat  YYYY-MM-DD
    section Planning
    Finalizing Scope :active, milestone1, 2025-02-24, 7d
    section Implementation
    ROS2 Setup :active, milestone2, 2025-03-03, 7d
    Object Detection :milestone3, 2025-03-10, 7d
    Speech Recognition :milestone4, 2025-03-17, 7d
    Navigation Testing :milestone5, 2025-03-24, 7d
    Integrate &amp; Debug :milestone6, 2025-03-31, 7d
    section Testing &amp; Deployment
    Real-World Testing :milestone7, 2025-04-07, 7d
    Final Prep :milestone8, 2025-04-14, 7d
    Document &amp; Review :milestone9, 2025-04-21, 7d
    Final Demonstration :milestone10, 2025-04-28, 6d</code></pre>





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
        
          
          <a href="charts/" class="md-footer__link md-footer__link--next" aria-label="Next: Charts" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Charts
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 team-name-here
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="/2023_fall" target="_blank" rel="noopener" title="2023 Site" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M64 464c-8.8 0-16-7.2-16-16V64c0-8.8 7.2-16 16-16h160v80c0 17.7 14.3 32 32 32h80v288c0 8.8-7.2 16-16 16H64zM64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V154.5c0-17-6.7-33.3-18.7-45.3l-90.6-90.5C262.7 6.7 246.5 0 229.5 0H64zm97 289c9.4-9.4 9.4-24.6 0-33.9s-24.6-9.4-33.9 0L79 303c-9.4 9.4-9.4 24.6 0 33.9l48 48c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-31-31 31-31zm96-34c-9.4-9.4-24.6-9.4-33.9 0s-9.4 24.6 0 33.9l31 31-31 31c-9.4 9.4-9.4 24.6 0 33.9s24.6 9.4 33.9 0l48-48c9.4-9.4 9.4-24.6 0-33.9l-48-48z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": ".", "features": ["navigation.tabs", "navigation.tabs.sticky", "toc.follow", "navigation.top", "navigation.path", "navigation.indexes", "navigation.prune", "content.action.edit", "navigation.footer"], "search": "assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
        <script src="javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>