
<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta name="robots" content="nofollow" />


      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://RAS598-2025-S-Team11.github.io/charts/">
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../esp-32-table/">
      
      
      <link rel="icon" href="../static/logo1.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.4.0">
            

        

    
      
        <title>Charts - Intelligent TurtleBot - Voice Guided Navigation and Object Detection</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.9f615399.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.649f08f9.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    


   <link href="../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style> <script src="../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#project-visual-overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" class="md-header__button md-logo" aria-label="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" data-md-component="logo">
      
  <img src="../static/logo1.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Intelligent TurtleBot - Voice Guided Navigation and Object Detection
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Charts
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Intelligent Mobile Manipulator

      </a>
    </li>
  

      
        
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
    
  
  Charts

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../esp-32-table/" class="md-tabs__link">
        
  
    
  
  Sensors Table

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../obj_detect/" class="md-tabs__link">
        
  
    
  
  Object Detection Page

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../pic-table/" class="md-tabs__link">
        
  
    
  
  TurtleBot4 Hardware Specifications

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../second-page/" class="md-tabs__link">
        
  
    
  
  Speech Recognition Page

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../source_code/" class="md-tabs__link">
        
  
    
  
  Source code

      </a>
    </li>
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../video_gallery/" class="md-tabs__link">
        
  
    
  
  Demonstration Gallery

      </a>
    </li>
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../component-selection-example/" class="md-tabs__link">
          
  
    
  
  Component selection example

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../subfolder/" class="md-tabs__link">
          
  
    
  
  Subfolder

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" class="md-nav__button md-logo" aria-label="Intelligent TurtleBot - Voice Guided Navigation and Object Detection" data-md-component="logo">
      
  <img src="../static/logo1.svg" alt="logo">

    </a>
    Intelligent TurtleBot - Voice Guided Navigation and Object Detection
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Intelligent Mobile Manipulator
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Charts
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Charts
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#project-visual-overview" class="md-nav__link">
    Project Visual Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gantt-chart-project-timeline-overview" class="md-nav__link">
    Gantt Chart – Project Timeline Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#main-project-pipeline" class="md-nav__link">
    Main Project Pipeline
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-c-c-d1-e-c-d2-f-e-g-f-g-g-h-i-k-m-g-j-l-m-classdef-sensors-filld0f0efstroke0097a7color000-classdef-perception-fille8eaf6stroke5c6bc0color000-classdef-decision-fillffe0e6stroked81b60color000-classdef-control-fillfff9c4strokefbc02dcolor000-classdef-gui-fillede7f6stroke7e57c2color000-classdef-startend-filleeeeeestroke757575color000-class-am-startend-class-b-sensors-class-cd1d2ef-perception-class-g-decision-class-hik-control-class-jl-gui" class="md-nav__link">
    graph TD
  A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
  B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
  C["Sensor Data&lt;br/&gt;Preprocessing"]
  D1["YOLOv8&lt;br/&gt;Object Detection"]
  D2["Voice Command&lt;br/&gt;Recognition"]
  E["Detected Object Info"]
  F["Intent or Goal Command"]
  G["Decision-Making Node"]
  H["ROS2 Navigation Stack"]
  I["Movement Commands&lt;br/&gt;via /cmd_vel"]
  J["GUI Update:&lt;br/&gt;Object and Nav Info"]
  K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
  L["User Feedback:&lt;br/&gt;GUI Visualization"]
  M["End"]

  A --&gt; B --&gt; C
  C --&gt; D1 --&gt; E
  C --&gt; D2 --&gt; F
  E --&gt; G
  F --&gt; G
  G --&gt; H --&gt; I --&gt; K --&gt; M
  G --&gt; J --&gt; L --&gt; M

  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000
  classDef perception fill:#e8eaf6,stroke:#5c6bc0,color:#000
  classDef decision fill:#ffe0e6,stroke:#d81b60,color:#000
  classDef control fill:#fff9c4,stroke:#fbc02d,color:#000
  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000
  classDef startend fill:#eeeeee,stroke:#757575,color:#000

  class A,M startend
  class B sensors
  class C,D1,D2,E,F perception
  class G decision
  class H,I,K control
  class J,L gui

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-control-and-autonomy-flow" class="md-nav__link">
    System Control and Autonomy Flow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-top-level-flow-aturtlebot4-booted-bsensor-data-collection-a-b-sensor-layer-subgraph-sensor_layer-ros2-sensor-layer-n1node-oakd_camera_node-t1rpi_13oakdpreviewimage_raw-n2node-voice_input_node-t2voice_input-end-processing-layer-subgraph-processing_layer-ros2-perception-command-parsing-n3node-yolov8_processor-t3yolov8_detections-n4node-voice_command_parser-t4voice_cmd-end-autonomy-layer-subgraph-autonomy_layer-ros2-autonomy-control-n5node-decision_maker_node-n6node-collision_avoidance_node-n7node-navigation_controller-t5action_cmd-t6cmd_vel-n5-t5-n5-n6-n7-t6-end-flow-between-subgraphs-b-sensor_layer-processing_layer-autonomy_layer-cturtlebot4-output-autonomy_layer-c" class="md-nav__link">
    graph TD

  %% Top-Level Flow
  A[TurtleBot4 Booted]
  B[Sensor Data Collection]

  A --&gt; B

  %% Sensor Layer
  subgraph Sensor_Layer [ROS2: Sensor Layer]
    N1[Node: oakd_camera_node] --&gt; T1[/rpi_13/oakd/preview/image_raw/]
    N2[Node: voice_input_node] --&gt; T2[/voice_input/]
  end

  %% Processing Layer
  subgraph Processing_Layer [ROS2: Perception &amp; Command Parsing]
    N3[Node: yolov8_processor] --&gt; T3[/yolov8_detections/]
    N4[Node: voice_command_parser] --&gt; T4[/voice_cmd/]
  end

  %% Autonomy Layer
  subgraph Autonomy_Layer [ROS2: Autonomy &amp; Control]
    N5[Node: decision_maker_node]
    N6[Node: collision_avoidance_node]
    N7[Node: navigation_controller]
    T5[/action_cmd/]
    T6[/cmd_vel/]
    N5 --&gt; T5
    N5 --&gt; N6 --&gt; N7 --&gt; T6
  end

  %% Flow between subgraphs
  B --&gt; Sensor_Layer --&gt; Processing_Layer --&gt; Autonomy_Layer
  C[Turtlebot4 Output]
  Autonomy_Layer --&gt; C

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hybrid-ros2-system-architecture" class="md-nav__link">
    Hybrid ROS2 System Architecture
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-sensors-subgraph-sensors-sensor-inputs-camoak-d-camera-micmicrophone-lidarir-lidar-sensors-end-perception-layer-subgraph-perception-perception-nodes-yolonode-yolov8_processor-voice_rawnode-voice_input_node-yolo_outyolov8_detections-voice_invoice_input-cam-yolo-yolo_out-mic-voice_raw-voice_in-end-high-level-autonomy-subgraph-high_auto-high-level-autonomy-voice_parsernode-voice_command_parser-voice_cmdvoice_cmd-decidenode-decision_maker_node-act_cmdaction_cmd-voice_in-voice_parser-voice_cmd-yolo_out-decide-voice_cmd-decide-act_cmd-end-low-level-autonomy-subgraph-low_auto-low-level-control-coll_avoidnode-collision_avoidance_node-nav_ctrlnode-navigation_controller-cmd_velcmd_vel-lidar-coll_avoid-act_cmd-coll_avoid-nav_ctrl-cmd_vel-end-actuation-cmd_vel-tbotturtlebot-movement-block-to-block-flow-sensors-perception-high_auto-low_auto-tbot" class="md-nav__link">
    graph TD

  %% Sensors
  subgraph SENSORS [Sensor Inputs]
    CAM[Oak-D Camera]
    MIC[Microphone]
    LIDAR[IR / LiDAR Sensors]
  end

  %% Perception Layer
  subgraph PERCEPTION [Perception Nodes]
    YOLO[Node: yolov8_processor]
    VOICE_RAW[Node: voice_input_node]
    YOLO_OUT[/yolov8_detections/]
    VOICE_IN[/voice_input/]
    CAM --&gt; YOLO --&gt; YOLO_OUT
    MIC --&gt; VOICE_RAW --&gt; VOICE_IN
  end

  %% High-Level Autonomy
  subgraph HIGH_AUTO [High-Level Autonomy]
    VOICE_PARSER[Node: voice_command_parser]
    VOICE_CMD[/voice_cmd/]
    DECIDE[Node: decision_maker_node]
    ACT_CMD[/action_cmd/]
    VOICE_IN --&gt; VOICE_PARSER --&gt; VOICE_CMD
    YOLO_OUT --&gt; DECIDE
    VOICE_CMD --&gt; DECIDE --&gt; ACT_CMD
  end

  %% Low-Level Autonomy
  subgraph LOW_AUTO [Low-Level Control]
    COLL_AVOID[Node: collision_avoidance_node]
    NAV_CTRL[Node: navigation_controller]
    CMD_VEL[/cmd_vel/]
    LIDAR --&gt; COLL_AVOID
    ACT_CMD --&gt; COLL_AVOID --&gt; NAV_CTRL --&gt; CMD_VEL
  end

  %% Actuation
  CMD_VEL --&gt; TBOT[TurtleBot Movement]

  %% Block-to-block flow
  SENSORS --&gt; PERCEPTION --&gt; HIGH_AUTO --&gt; LOW_AUTO --&gt; TBOT
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-work-concept-turtlebot4-with-mounted-cobot-arm" class="md-nav__link">
    Future Work Concept: TurtleBot4 with Mounted Cobot Arm
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../esp-32-table/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Sensors Table
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../obj_detect/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Object Detection Page
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../pic-table/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    TurtleBot4 Hardware Specifications
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../second-page/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Speech Recognition Page
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../source_code/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Source code
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    <li class="md-nav__item">
      <a href="../video_gallery/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demonstration Gallery
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../component-selection-example/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Component selection example
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
    
    
      
        
          
        
      
        
      
    
    
      
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
    <a href="../subfolder/" class="md-nav__link">
      
  
  <span class="md-ellipsis">
    Subfolder
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#project-visual-overview" class="md-nav__link">
    Project Visual Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#gantt-chart-project-timeline-overview" class="md-nav__link">
    Gantt Chart – Project Timeline Overview
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#main-project-pipeline" class="md-nav__link">
    Main Project Pipeline
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-c-c-d1-e-c-d2-f-e-g-f-g-g-h-i-k-m-g-j-l-m-classdef-sensors-filld0f0efstroke0097a7color000-classdef-perception-fille8eaf6stroke5c6bc0color000-classdef-decision-fillffe0e6stroked81b60color000-classdef-control-fillfff9c4strokefbc02dcolor000-classdef-gui-fillede7f6stroke7e57c2color000-classdef-startend-filleeeeeestroke757575color000-class-am-startend-class-b-sensors-class-cd1d2ef-perception-class-g-decision-class-hik-control-class-jl-gui" class="md-nav__link">
    graph TD
  A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
  B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
  C["Sensor Data&lt;br/&gt;Preprocessing"]
  D1["YOLOv8&lt;br/&gt;Object Detection"]
  D2["Voice Command&lt;br/&gt;Recognition"]
  E["Detected Object Info"]
  F["Intent or Goal Command"]
  G["Decision-Making Node"]
  H["ROS2 Navigation Stack"]
  I["Movement Commands&lt;br/&gt;via /cmd_vel"]
  J["GUI Update:&lt;br/&gt;Object and Nav Info"]
  K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
  L["User Feedback:&lt;br/&gt;GUI Visualization"]
  M["End"]

  A --&gt; B --&gt; C
  C --&gt; D1 --&gt; E
  C --&gt; D2 --&gt; F
  E --&gt; G
  F --&gt; G
  G --&gt; H --&gt; I --&gt; K --&gt; M
  G --&gt; J --&gt; L --&gt; M

  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000
  classDef perception fill:#e8eaf6,stroke:#5c6bc0,color:#000
  classDef decision fill:#ffe0e6,stroke:#d81b60,color:#000
  classDef control fill:#fff9c4,stroke:#fbc02d,color:#000
  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000
  classDef startend fill:#eeeeee,stroke:#757575,color:#000

  class A,M startend
  class B sensors
  class C,D1,D2,E,F perception
  class G decision
  class H,I,K control
  class J,L gui

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#system-control-and-autonomy-flow" class="md-nav__link">
    System Control and Autonomy Flow
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-top-level-flow-aturtlebot4-booted-bsensor-data-collection-a-b-sensor-layer-subgraph-sensor_layer-ros2-sensor-layer-n1node-oakd_camera_node-t1rpi_13oakdpreviewimage_raw-n2node-voice_input_node-t2voice_input-end-processing-layer-subgraph-processing_layer-ros2-perception-command-parsing-n3node-yolov8_processor-t3yolov8_detections-n4node-voice_command_parser-t4voice_cmd-end-autonomy-layer-subgraph-autonomy_layer-ros2-autonomy-control-n5node-decision_maker_node-n6node-collision_avoidance_node-n7node-navigation_controller-t5action_cmd-t6cmd_vel-n5-t5-n5-n6-n7-t6-end-flow-between-subgraphs-b-sensor_layer-processing_layer-autonomy_layer-cturtlebot4-output-autonomy_layer-c" class="md-nav__link">
    graph TD

  %% Top-Level Flow
  A[TurtleBot4 Booted]
  B[Sensor Data Collection]

  A --&gt; B

  %% Sensor Layer
  subgraph Sensor_Layer [ROS2: Sensor Layer]
    N1[Node: oakd_camera_node] --&gt; T1[/rpi_13/oakd/preview/image_raw/]
    N2[Node: voice_input_node] --&gt; T2[/voice_input/]
  end

  %% Processing Layer
  subgraph Processing_Layer [ROS2: Perception &amp; Command Parsing]
    N3[Node: yolov8_processor] --&gt; T3[/yolov8_detections/]
    N4[Node: voice_command_parser] --&gt; T4[/voice_cmd/]
  end

  %% Autonomy Layer
  subgraph Autonomy_Layer [ROS2: Autonomy &amp; Control]
    N5[Node: decision_maker_node]
    N6[Node: collision_avoidance_node]
    N7[Node: navigation_controller]
    T5[/action_cmd/]
    T6[/cmd_vel/]
    N5 --&gt; T5
    N5 --&gt; N6 --&gt; N7 --&gt; T6
  end

  %% Flow between subgraphs
  B --&gt; Sensor_Layer --&gt; Processing_Layer --&gt; Autonomy_Layer
  C[Turtlebot4 Output]
  Autonomy_Layer --&gt; C

  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hybrid-ros2-system-architecture" class="md-nav__link">
    Hybrid ROS2 System Architecture
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#graph-td-sensors-subgraph-sensors-sensor-inputs-camoak-d-camera-micmicrophone-lidarir-lidar-sensors-end-perception-layer-subgraph-perception-perception-nodes-yolonode-yolov8_processor-voice_rawnode-voice_input_node-yolo_outyolov8_detections-voice_invoice_input-cam-yolo-yolo_out-mic-voice_raw-voice_in-end-high-level-autonomy-subgraph-high_auto-high-level-autonomy-voice_parsernode-voice_command_parser-voice_cmdvoice_cmd-decidenode-decision_maker_node-act_cmdaction_cmd-voice_in-voice_parser-voice_cmd-yolo_out-decide-voice_cmd-decide-act_cmd-end-low-level-autonomy-subgraph-low_auto-low-level-control-coll_avoidnode-collision_avoidance_node-nav_ctrlnode-navigation_controller-cmd_velcmd_vel-lidar-coll_avoid-act_cmd-coll_avoid-nav_ctrl-cmd_vel-end-actuation-cmd_vel-tbotturtlebot-movement-block-to-block-flow-sensors-perception-high_auto-low_auto-tbot" class="md-nav__link">
    graph TD

  %% Sensors
  subgraph SENSORS [Sensor Inputs]
    CAM[Oak-D Camera]
    MIC[Microphone]
    LIDAR[IR / LiDAR Sensors]
  end

  %% Perception Layer
  subgraph PERCEPTION [Perception Nodes]
    YOLO[Node: yolov8_processor]
    VOICE_RAW[Node: voice_input_node]
    YOLO_OUT[/yolov8_detections/]
    VOICE_IN[/voice_input/]
    CAM --&gt; YOLO --&gt; YOLO_OUT
    MIC --&gt; VOICE_RAW --&gt; VOICE_IN
  end

  %% High-Level Autonomy
  subgraph HIGH_AUTO [High-Level Autonomy]
    VOICE_PARSER[Node: voice_command_parser]
    VOICE_CMD[/voice_cmd/]
    DECIDE[Node: decision_maker_node]
    ACT_CMD[/action_cmd/]
    VOICE_IN --&gt; VOICE_PARSER --&gt; VOICE_CMD
    YOLO_OUT --&gt; DECIDE
    VOICE_CMD --&gt; DECIDE --&gt; ACT_CMD
  end

  %% Low-Level Autonomy
  subgraph LOW_AUTO [Low-Level Control]
    COLL_AVOID[Node: collision_avoidance_node]
    NAV_CTRL[Node: navigation_controller]
    CMD_VEL[/cmd_vel/]
    LIDAR --&gt; COLL_AVOID
    ACT_CMD --&gt; COLL_AVOID --&gt; NAV_CTRL --&gt; CMD_VEL
  end

  %% Actuation
  CMD_VEL --&gt; TBOT[TurtleBot Movement]

  %% Block-to-block flow
  SENSORS --&gt; PERCEPTION --&gt; HIGH_AUTO --&gt; LOW_AUTO --&gt; TBOT
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-work-concept-turtlebot4-with-mounted-cobot-arm" class="md-nav__link">
    Future Work Concept: TurtleBot4 with Mounted Cobot Arm
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
    <a href="https://github.com/RAS598-2025-S-Team11/RAS598-2025-S-Team11.github.io/edit/main/docs/charts.md" title="Edit this page" class="md-content__button md-icon">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4v-2m10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1 2.1 2.1Z"/></svg>
    </a>
  
  


  <h1>Charts</h1>

<h2 id="project-visual-overview">Project Visual Overview</h2>
<table>
<thead>
<tr>
<th>Section</th>
<th>Chart Title</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Timeline</strong></td>
<td>Gantt Chart</td>
<td>Shows weekly project phases from planning to deployment.</td>
</tr>
<tr>
<td><strong>Workflow</strong></td>
<td>Project Workflow Chart</td>
<td>Displays the full robot pipeline from sensor data to GUI feedback.</td>
</tr>
<tr>
<td><strong>System Control</strong></td>
<td>System Architecture + ROS2 Nodes &amp; Topics</td>
<td>Visualizes ROS2 nodes and topics, including collision avoidance logic.</td>
</tr>
<tr>
<td><strong>Autonomy Layers</strong></td>
<td>High-Level vs Low-Level Control Flow</td>
<td>Separates voice-guided task selection from low-level obstacle avoidance with aligned ROS2 nodes and data flow.</td>
</tr>
<tr>
<td><strong>Future Work</strong></td>
<td>Cobot Arm Integration Flow</td>
<td>Outlines the proposed pick-and-place functionality using a mounted arm.</td>
</tr>
</tbody>
</table>
<h2 id="gantt-chart-project-timeline-overview">Gantt Chart – Project Timeline Overview</h2>
<pre class="mermaid"><code>gantt
    title Project Timeline (Weeks 7–16)
    dateFormat  YYYY-MM-DD
    axisFormat  %b %d

    section Planning
    Finalize Scope            :active,milestone1, 2025-02-24, 7d

    section Implementation
    ROS2 Setup                :active,milestone2, 2025-03-03, 7d
    GUI Development           :active,milestone3, 2025-03-10, 7d
    YOLOv8 + Oak-D            :active,milestone4, 2025-03-17, 7d
    Voice Command             :active,milestone5, 2025-03-24, 7d
    ROS2 Integration          :active,milestone6, 2025-03-31, 10d

    section Testing &amp; Deployment
    TurtleBot Testing         :active,milestone7, 2025-04-10, 7d
    Final Debug               :active,milestone8, 2025-04-17, 5d
    Docs &amp; Video              :active,milestone9, 2025-04-22, 5d
    Final Demo                :active,milestone10, 2025-04-28, 6d</code></pre>
<h2 id="main-project-pipeline">Main Project Pipeline</h2>
<ul>
<li>The flowchart below represents the overall working of our Intelligent TurtleBot4 system. </li>
<li>It starts with sensor data collection from the Oak-D camera, IMU, LiDAR, and microphone. </li>
<li>
<p>The data is then preprocessed and sent to two parallel modules: YOLOv8 for object detection and voice recognition for interpreting commands.</p>
</li>
<li>
<p>Outputs from both modules are fed into a decision-making node that determines the robot's next action. </p>
</li>
<li>The chosen action is executed via the ROS2 navigation stack and published as movement commands. </li>
<li>Simultaneously, the GUI updates with relevant feedback, allowing users to visualize object info and robot behavior in real time.</li>
</ul>
<h2 id="graph-td-astartbrturtlebot4-powered-on-bsensor-layerbroak-d-camera-imu-lidar-mic-csensor-databrpreprocessing-d1yolov8brobject-detection-d2voice-commandbrrecognition-edetected-object-info-fintent-or-goal-command-gdecision-making-node-hros2-navigation-stack-imovement-commandsbrvia-cmd_vel-jgui-updatebrobject-and-nav-info-kactuator-responsebrturtlebot-moves-luser-feedbackbrgui-visualization-mend-a-b-c-c-d1-e-c-d2-f-e-g-f-g-g-h-i-k-m-g-j-l-m-classdef-sensors-filld0f0efstroke0097a7color000-classdef-perception-fille8eaf6stroke5c6bc0color000-classdef-decision-fillffe0e6stroked81b60color000-classdef-control-fillfff9c4strokefbc02dcolor000-classdef-gui-fillede7f6stroke7e57c2color000-classdef-startend-filleeeeeestroke757575color000-class-am-startend-class-b-sensors-class-cd1d2ef-perception-class-g-decision-class-hik-control-class-jl-gui"><pre class="mermaid"><code>graph TD
  A["Start:&lt;br/&gt;TurtleBot4 Powered On"]
  B["Sensor Layer:&lt;br/&gt;Oak-D Camera, IMU, LiDAR, Mic"]
  C["Sensor Data&lt;br/&gt;Preprocessing"]
  D1["YOLOv8&lt;br/&gt;Object Detection"]
  D2["Voice Command&lt;br/&gt;Recognition"]
  E["Detected Object Info"]
  F["Intent or Goal Command"]
  G["Decision-Making Node"]
  H["ROS2 Navigation Stack"]
  I["Movement Commands&lt;br/&gt;via /cmd_vel"]
  J["GUI Update:&lt;br/&gt;Object and Nav Info"]
  K["Actuator Response:&lt;br/&gt;TurtleBot Moves"]
  L["User Feedback:&lt;br/&gt;GUI Visualization"]
  M["End"]

  A --&gt; B --&gt; C
  C --&gt; D1 --&gt; E
  C --&gt; D2 --&gt; F
  E --&gt; G
  F --&gt; G
  G --&gt; H --&gt; I --&gt; K --&gt; M
  G --&gt; J --&gt; L --&gt; M

  classDef sensors fill:#d0f0ef,stroke:#0097a7,color:#000
  classDef perception fill:#e8eaf6,stroke:#5c6bc0,color:#000
  classDef decision fill:#ffe0e6,stroke:#d81b60,color:#000
  classDef control fill:#fff9c4,stroke:#fbc02d,color:#000
  classDef gui fill:#ede7f6,stroke:#7e57c2,color:#000
  classDef startend fill:#eeeeee,stroke:#757575,color:#000

  class A,M startend
  class B sensors
  class C,D1,D2,E,F perception
  class G decision
  class H,I,K control
  class J,L gui
</code></pre></h2>
<h2 id="system-control-and-autonomy-flow">System Control and Autonomy Flow</h2>
<ul>
<li>The diagram below presents the complete decision and control loop of our TurtleBot4 system.</li>
<li>From user voice inputs and real-time camera feeds, data is collected, processed, and passed through an autonomy layer to issue movement commands.</li>
<li>Each subsystem (sensor, processing, autonomy) is driven by dedicated ROS2 nodes communicating through standard topics.</li>
</ul>
<h2 id="graph-td-top-level-flow-aturtlebot4-booted-bsensor-data-collection-a-b-sensor-layer-subgraph-sensor_layer-ros2-sensor-layer-n1node-oakd_camera_node-t1rpi_13oakdpreviewimage_raw-n2node-voice_input_node-t2voice_input-end-processing-layer-subgraph-processing_layer-ros2-perception-command-parsing-n3node-yolov8_processor-t3yolov8_detections-n4node-voice_command_parser-t4voice_cmd-end-autonomy-layer-subgraph-autonomy_layer-ros2-autonomy-control-n5node-decision_maker_node-n6node-collision_avoidance_node-n7node-navigation_controller-t5action_cmd-t6cmd_vel-n5-t5-n5-n6-n7-t6-end-flow-between-subgraphs-b-sensor_layer-processing_layer-autonomy_layer-cturtlebot4-output-autonomy_layer-c"><pre class="mermaid"><code>graph TD

  %% Top-Level Flow
  A[TurtleBot4 Booted]
  B[Sensor Data Collection]

  A --&gt; B

  %% Sensor Layer
  subgraph Sensor_Layer [ROS2: Sensor Layer]
    N1[Node: oakd_camera_node] --&gt; T1[/rpi_13/oakd/preview/image_raw/]
    N2[Node: voice_input_node] --&gt; T2[/voice_input/]
  end

  %% Processing Layer
  subgraph Processing_Layer [ROS2: Perception &amp; Command Parsing]
    N3[Node: yolov8_processor] --&gt; T3[/yolov8_detections/]
    N4[Node: voice_command_parser] --&gt; T4[/voice_cmd/]
  end

  %% Autonomy Layer
  subgraph Autonomy_Layer [ROS2: Autonomy &amp; Control]
    N5[Node: decision_maker_node]
    N6[Node: collision_avoidance_node]
    N7[Node: navigation_controller]
    T5[/action_cmd/]
    T6[/cmd_vel/]
    N5 --&gt; T5
    N5 --&gt; N6 --&gt; N7 --&gt; T6
  end

  %% Flow between subgraphs
  B --&gt; Sensor_Layer --&gt; Processing_Layer --&gt; Autonomy_Layer
  C[Turtlebot4 Output]
  Autonomy_Layer --&gt; C
</code></pre></h2>
<h2 id="hybrid-ros2-system-architecture">Hybrid ROS2 System Architecture</h2>
<ul>
<li>This diagram separates high-level autonomy (top row) from low-level motion control (bottom row), with clearly labeled ROS2 nodes and topic communication.</li>
<li>It maintains modular blocks for perception, decision-making, and actuation while improving visual alignment and clarity.</li>
</ul>
<h2 id="graph-td-sensors-subgraph-sensors-sensor-inputs-camoak-d-camera-micmicrophone-lidarir-lidar-sensors-end-perception-layer-subgraph-perception-perception-nodes-yolonode-yolov8_processor-voice_rawnode-voice_input_node-yolo_outyolov8_detections-voice_invoice_input-cam-yolo-yolo_out-mic-voice_raw-voice_in-end-high-level-autonomy-subgraph-high_auto-high-level-autonomy-voice_parsernode-voice_command_parser-voice_cmdvoice_cmd-decidenode-decision_maker_node-act_cmdaction_cmd-voice_in-voice_parser-voice_cmd-yolo_out-decide-voice_cmd-decide-act_cmd-end-low-level-autonomy-subgraph-low_auto-low-level-control-coll_avoidnode-collision_avoidance_node-nav_ctrlnode-navigation_controller-cmd_velcmd_vel-lidar-coll_avoid-act_cmd-coll_avoid-nav_ctrl-cmd_vel-end-actuation-cmd_vel-tbotturtlebot-movement-block-to-block-flow-sensors-perception-high_auto-low_auto-tbot"><pre class="mermaid"><code>graph TD

  %% Sensors
  subgraph SENSORS [Sensor Inputs]
    CAM[Oak-D Camera]
    MIC[Microphone]
    LIDAR[IR / LiDAR Sensors]
  end

  %% Perception Layer
  subgraph PERCEPTION [Perception Nodes]
    YOLO[Node: yolov8_processor]
    VOICE_RAW[Node: voice_input_node]
    YOLO_OUT[/yolov8_detections/]
    VOICE_IN[/voice_input/]
    CAM --&gt; YOLO --&gt; YOLO_OUT
    MIC --&gt; VOICE_RAW --&gt; VOICE_IN
  end

  %% High-Level Autonomy
  subgraph HIGH_AUTO [High-Level Autonomy]
    VOICE_PARSER[Node: voice_command_parser]
    VOICE_CMD[/voice_cmd/]
    DECIDE[Node: decision_maker_node]
    ACT_CMD[/action_cmd/]
    VOICE_IN --&gt; VOICE_PARSER --&gt; VOICE_CMD
    YOLO_OUT --&gt; DECIDE
    VOICE_CMD --&gt; DECIDE --&gt; ACT_CMD
  end

  %% Low-Level Autonomy
  subgraph LOW_AUTO [Low-Level Control]
    COLL_AVOID[Node: collision_avoidance_node]
    NAV_CTRL[Node: navigation_controller]
    CMD_VEL[/cmd_vel/]
    LIDAR --&gt; COLL_AVOID
    ACT_CMD --&gt; COLL_AVOID --&gt; NAV_CTRL --&gt; CMD_VEL
  end

  %% Actuation
  CMD_VEL --&gt; TBOT[TurtleBot Movement]

  %% Block-to-block flow
  SENSORS --&gt; PERCEPTION --&gt; HIGH_AUTO --&gt; LOW_AUTO --&gt; TBOT</code></pre></h2>
<h2 id="future-work-concept-turtlebot4-with-mounted-cobot-arm">Future Work Concept: TurtleBot4 with Mounted Cobot Arm</h2>
<p>This future work visual outlines the integration of a robotic arm on TurtleBot4. 
The system uses object detection and coordinates from the perception pipeline to compute inverse kinematics and execute pick-and-place actions via a dedicated ROS2 control node.</p>
<p>Goals to Capture Visually:
- Addition of a robotic arm mounted on TurtleBot4
- Use of ROS2 for communication with the arm
- Performing pick-and-place tasks
- Integration with existing perception (e.g., YOLOv8 object detection for picking targets)</p>
<pre class="mermaid"><code>graph TD

  %% Object Detection &amp; Localization
  V1["Object Detection&lt;br/&gt;using YOLOv8"]
  V2["Target Object&lt;br/&gt;Coordinates"]

  %% Motion Planning
  M1["Inverse Kinematics&lt;br/&gt;&amp; Arm Planning"]

  %% Platform Base
  P2["TurtleBot4 Base Platform"]
  P1["Mounted Cobot Arm&lt;br/&gt;on Platform"]

  %% ROS2 Control
  N1["ROS2 Arm&lt;br/&gt;Control Node"]

  %% Execution
  E1["Pick and Place&lt;br/&gt;Execution"]
  E2["Object&lt;br/&gt;Grasped and Placed"]

  %% Connections
  V1 --&gt; V2 --&gt; M1 --&gt; N1
  P1 --&gt; P2 --&gt; N1
  N1 --&gt; E1 --&gt; E2</code></pre>
<hr />





                
              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href=".." class="md-footer__link md-footer__link--prev" aria-label="Previous: Intelligent Mobile Manipulator" rel="prev">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Intelligent Mobile Manipulator
              </div>
            </div>
          </a>
        
        
          
          <a href="../esp-32-table/" class="md-footer__link md-footer__link--next" aria-label="Next: Sensors Table" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Sensors Table
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2024 team-name-here
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="/2023_fall" target="_blank" rel="noopener" title="2023 Site" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.4.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M64 464c-8.8 0-16-7.2-16-16V64c0-8.8 7.2-16 16-16h160v80c0 17.7 14.3 32 32 32h80v288c0 8.8-7.2 16-16 16H64zM64 0C28.7 0 0 28.7 0 64v384c0 35.3 28.7 64 64 64h256c35.3 0 64-28.7 64-64V154.5c0-17-6.7-33.3-18.7-45.3l-90.6-90.5C262.7 6.7 246.5 0 229.5 0H64zm97 289c9.4-9.4 9.4-24.6 0-33.9s-24.6-9.4-33.9 0L79 303c-9.4 9.4-9.4 24.6 0 33.9l48 48c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-31-31 31-31zm96-34c-9.4-9.4-24.6-9.4-33.9 0s-9.4 24.6 0 33.9l31 31-31 31c-9.4 9.4-9.4 24.6 0 33.9s24.6 9.4 33.9 0l48-48c9.4-9.4 9.4-24.6 0-33.9l-48-48z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.tabs.sticky", "toc.follow", "navigation.top", "navigation.path", "navigation.indexes", "navigation.prune", "content.action.edit", "navigation.footer"], "search": "../assets/javascripts/workers/search.a264c092.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.4e0fa4ba.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  <script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body>
</html>